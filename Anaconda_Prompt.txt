https://pypi.org/project/torch/
https://pytorch.org/get-started/previous-versions/     # torch cuda -> try pip code for your gpu cuda (nvidia-smi) version
https://download.pytorch.org/whl  # lots of wheel are avaialable torch+cuda

# Update Conda, setuptools ________________________________________________________________________________________
conda update -n base -c defaults conda

# If you want to see the current lib path
conda config --show envs_dirs

# If you want to add a new folder with lib path
conda config --add envs_dirs C:\conda\envs

# create new environment on a specific directory
conda create -p C:\conda\envs\isaac python=3.11 -y



python -m pip install --upgrade pip

pip install -U pip setuptools wheel
or
python.exe -m pip install -U pip setuptools wheel

pip install -U pip
pip install opencv-python
pip install ion-python==1.8.10


Option A (recommended): use Python 3.12 _______________________________________________________________

# 0) Remove/Delete Old env on Python 3.13 (One environment at a time)
conda env remove --name RF_Lunar_Land

# 1) New env on Python 3.12
conda create -n RF_Lunar_Land python=3.12 -y
conda activate RF_Lunar_Land

get python version:
python --version

change python version:
conda install python=3.12

# 2a) CPU-only PyTorch
conda install pytorch torchvision torchaudio cpuonly -c pytorch

# 2b) Or, GPU build (NVIDIA):
#    pick the CUDA version that matches your driver/toolkit (e.g., 12.1)
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y

# 3) JupyterLab (optional)
conda install -c conda-forge jupyterlab

# 4) Spyder (optional)
conda install -c conda-forge spyder

# 5) Matplotlib (optional)
conda install -c conda-forge matplotlib

# 6) pandas (optional)
conda install -c conda-forge pandas

# 7) numba (optional)
conda install -c conda-forge numba cudatoolkit

A) Numba (CPU only)
conda install -c conda-forge numba
B) Numba with CUDA 12 support (recommended with your cu124 PyTorch)
conda install -c conda-forge numba "cuda-version=12.4" cuda-nvrtc cuda-nvcc -y

#Quick check after install
from numba import cuda
print("Numba CUDA available:", cuda.is_available())
print("Driver version:", cuda.runtime.get_driver_version())

8) install git
conda install git -y

9) Verify CUDA availability on terminal:
python -c "import torch; print('torch', torch.__version__); print('torch.version.cuda', torch.version.cuda); print('cuda available', torch.cuda.is_available()); print('device count', torch.cuda.device_count()); print('device 0', torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)"







Option B: keep your current env but drop the 3.13 pin (downgrade Python) _______________________________________________________________

# See if you have pins set
conda config --show pinned_packages

# Remove the Python pin (if present)
conda config --remove pinned_packages "python=3.13"

# Try installing again; the solver will likely choose Python 3.12
conda install pytorch torchvision torchaudio cpuonly -c pytorch -y
# or for GPU:
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y

install important libraries:_________________________________________________________


pip install "gymnasium[classic-control]"
conda install -c conda-forge swig box2d-py


pip install "gymnasium[box2d]"
pip install gymnasium[mujoco]

pip install flappy-bird-gymnasium

pip install rich

pip uninstall -y numba llvmlite
pip install numba
# llvmlite comes in automatically with numba



*******************************************************************************
# Clone Environment Copy Enviroment__________________________________________
# from the working env
conda activate rf
conda env export --no-builds > rf.yml

# Same PC
conda create --name new_env --clone old_env

# on the new machine/env
conda env create -f rf.yml -n rf_NEW
#----Good----




# Just Get the list --> Not good for clone ---
# Need to test ----
# show them all =======
pip list > requirements.txt
pip list --format=columns

# Save requirements file
pip freeze > requirements.txt
# Install from requirements file
pip install -r requirements.txt
## Verbose
pip install -r requirements.txt -v
## Isolate
pip install --no-cache-dir -r requirements.txt


# Remove torch from requirements ****
pip install -r requirements.txt --no-deps

# Or generate a filtered file:*****

## linux
grep -v torch requirements.txt > requirements_notorch.txt
## windows
findstr /v /i torch requirements.txt > requirements_notorch.txt
findstr /OFFLINE /v /i torch requirements.txt > requirements_notorch.txt
pip install --no-cache-dir -r requirements_notorch.txt


pip install -r requirements_notorch.txt







python code resources: ____________________________________________________________________________________________

https://www.youtube.com/watch?v=SMZfgeHFFcA&list=PL9jZRDZXEdTHPOiUDM7SZB4cDdcJfQa8M&index=12
https://pylessons.com/LunarLander-v2-PPO/
https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b
https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/RL_brain.py

https://github.com/philtabor
https://github.com/philtabor/Actor-Critic-Methods-Paper-To-Code/tree/master/ActorCritic
https://github.com/philtabor/Actor-Critic-Methods-Paper-To-Code/tree/master/Reinforce


Install _______________________________________________________________________________________
https://visualstudio.microsoft.com/visual-cpp-build-tools/


# Spyder Noebook 
pip uninstall ruamel.yaml
pip uninstall spyder-notebook
pip install ruamel.yaml==0.17.21
pip install spyder-notebook==0.4.0






#************************************************************************************************************
#************************************************************************************************************
#************************************************************************************************************
---
title: "Hydrocat: GPU Environment & YOLOv8 Setup"
output:
  github_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
# This file is meant as documentation for GitHub.
# By default, chunks won't execute when knitting.
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

#**************************************************************
conda create --name Hydrocat python=3.12 -y
conda activate Hydrocat
conda install -c conda-forge numba "cuda-version=12.4" cuda-nvrtc cuda-nvcc -y
pip install ultralytics matplotlib pandas jupyterlab jupyter notebook spyder
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124





# Yolo
pip install ultralytics -q

# kagglehub is a small helper library for downloading datasets/models directly from Kaggle inside Python.
pip install kagglehub

# downloading datasets and running notebooks
pip install kaggle


# GPU ************************
# Nvidia Driver: https://www.nvidia.com/en-us/drivers/
nvidia-smi
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121


# if error *************************
conda install --yes --force-reinstall python expat pip

python -c "import xml.parsers.expat; print('expat OK')"
python -m pip --version



# Verify CUDA in PyTorch
import torch
print(torch.cuda.is_available())   # Should return True
print(torch.cuda.get_device_name(0))  # Shows your GPU name


import torch
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))



# Use token for Colab ********************************************************
jupyter server \
  --ServerApp.allow_origin='https://colab.research.google.com' \
  --port=8888 \
  --ServerApp.port_retries=0


jupyter server --ServerApp.allow_origin='https://colab.research.google.com' --port=8888  --ServerApp.port_retries=0

#_______________________________________________________


#_______________________________________________________________________________

# Install ARGoS *********************************************
https://releases.ubuntu.com/focal/

https://www.argos-sim.info/core.php

Binary Packages for Ubuntu 22.04 LTS 64bit
sudo chmod +x argos3_simulator-3.0.0-x86_64-beta59.deb
sudo apt install ./argos3_simulator-3.0.0-x86_64-beta59.deb 


# Install app on Linux *********************************************
ls
sudo chmod +x filename.sh
sudo apt install filename.sh











# Jetson ________________________________________________
https://github.com/dusty-nv/jetson-containers
https://forums.developer.nvidia.com/t/looking-for-a-pre-built-wheel-for-pytorch-for-jetson-nano/324816/3


# UR10________________________________________
https://pypi.org/project/ur-rtde/
pip install ur-rtde
https://ieeexplore.ieee.org/document/10871000




# Clone Environment __________________________________________
# from the working env
conda activate IR
conda env export --no-builds > IR.yml

# on the new machine/env
conda env create -f IR.yml -n IR_NEW


# Force install wheel______________________________________________________________
unzip - l Toolbox-1.0.0-py3-none-any.whl
pip install Toolbox-1.0.0-py3-none-any.whl --no-deps --force-reinstall --ignore-requires-python





# Troubleshoot Error  ****************************************************************

1. ImportError: DLL load failed while importing _C: The operating system cannot run %1.________________________________________________________
That Windows error means PyTorch’s native C/C++ extension (torch._C) couldn’t load—usually a mismatched/corrupt 
install or missing DLLs. Here’s the fastest way to fix it in your div2k conda env.

Quickest fix paths
(suppose your envisronment is div2k)....
Option A — CPU-only (simplest, no CUDA needed)_____________
conda activate div2k
pip uninstall -y torch torchvision torchaudio
conda remove -y pytorch torchvision torchaudio pytorch-cuda cudatoolkit
conda clean -y --all

conda install -y pytorch torchvision torchaudio cpuonly -c pytorch
python -c "import torch; print('OK torch', torch.__version__, 'CUDA?', torch.cuda.is_available())"


Option B — GPU (CUDA 12.1 recommended for Windows)__________

Prereqs: recent NVIDIA driver (≥ 531), don’t mix pip & conda for PyTorch/CUDA.
conda activate div2k
pip uninstall -y torch torchvision torchaudio
conda remove -y pytorch torchvision torchaudio pytorch-cuda cudatoolkit
conda clean -y --all

conda install -y pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
python -c "import torch; print('OK torch', torch.__version__, 'CUDA?', torch.cuda.is_available(), 'device count', torch.cuda.device_count())"

if Conda fails, try pip _________________
conda activate div2k
pip install --upgrade --force-reinstall --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

#_______________________________________________________________________________


# LLaVA  ****************************************************************
https://llava-vl.github.io/
https://www.jetson-ai-lab.com/tutorial_live-llava.html



# Troubleshoot Error  ****************************************************************
# InternalTorchDynamoError: RuntimeError: Compiler: cl.exe is not found.

Solution 1: 
model_compile = False
if model_compile:
    try:
        model = torch.compile(model, backend="eager")  # no Triton required
    except Exception:
        pass

Solution 2:
Install Microsoft Visual C++ distribution (see C++Distribution.png)
#_______________________________________________________________________________




# matplotlib restarting kernel error troubleshoot ****************************************************************
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"   # ⚠️ temporary, evaluation-only
# After matplotlib.plot
os.environ["KMP_DUPLICATE_LIB_OK"] = "FALSE"
#_______________________________________________________________________________



# Check function parameters ****************************************************************
print(create_model)                       # where is it from?
import inspect
print(inspect.signature(create_model))    # what params does it accept?
#_______________________________________________________________________________


# Use multiple GPUs ****************************************************************
import torch
import torch.nn as nn

device = "cuda" if torch.cuda.is_available() else "cpu"

model = MyModel(...)  # your model
if torch.cuda.is_available():
    n = torch.cuda.device_count()
    print("GPUs visible:", n)
    if n > 1:
        model = nn.DataParallel(model)  # uses all visible GPUs
model = model.to(device)
# train as usual

# Limit GPUs inside code (device_ids) ___________________
gpu_device_ids = [0, 1]  # max 2 GPUs

    # ---- decide device + optional DataParallel ----
    if torch.cuda.is_available() and device != "cpu":
        # If user didn't specify, default to "all visible GPUs"
        if gpu_device_ids is None:
            gpu_device_ids = list(range(torch.cuda.device_count()))

        # Primary device is the first id
        device = torch.device(f"cuda:{gpu_device_ids[0]}")
        model = model.to(device)

        # Only wrap if using 2+ GPUs
        if len(gpu_device_ids) > 1:
            model = nn.DataParallel(model, device_ids=gpu_device_ids)
    else:
        device = torch.device("cpu")
        model = model.to(device)



# Function ***** --------***************************
import os
import torch
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP

def is_ddp_run(mode: str) -> bool:
    world = int(os.environ.get("WORLD_SIZE", "1"))
    return (mode == "ddp") and (world > 1)

def ddp_cleanup():
    if dist.is_available() and dist.is_initialized():
        # Optional: barrier can hang if a rank crashed. Use with care.
        # dist.barrier()
        dist.destroy_process_group()

def setup_device_and_model(model, mode: str):
    """
    mode: "cpu" | "cuda" | "ddp"
      - "dp" is handled OUTSIDE this function
      - "ddp" requires: torchrun --nproc_per_node=N script.py

    Returns: device_t, model, is_ddp, rank, world

    run_mode = "cpu"   # force CPU
    run_mode = "cuda"  # single GPU (or CPU fallback)
    run_mode = "dp"    # DataParallel (handled outside setup_device_and_model)
    run_mode = "ddp"   # DistributedDataParallel (torchrun required)

    """
    world_env = int(os.environ.get("WORLD_SIZE", "1"))
    is_ddp = (mode == "ddp") and (world_env > 1)

    if is_ddp:
        want_cuda = torch.cuda.is_available()
        backend = "nccl" if want_cuda else "gloo"

        if not dist.is_initialized():
            dist.init_process_group(backend=backend)

        rank = dist.get_rank()
        world = dist.get_world_size()

        if want_cuda:
            local_rank = int(os.environ.get("LOCAL_RANK", "0"))
            torch.cuda.set_device(local_rank)
            device_t = torch.device(f"cuda:{local_rank}")
            model = model.to(device_t)
            model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=False)
        else:
            device_t = torch.device("cpu")
            model = model.to(device_t)
            model = DDP(model)

        return device_t, model, True, rank, world

    # Single-process CPU/CUDA (and also DP wrapper happens outside)
    rank, world = 0, 1
    if mode == "cpu":
        device_t = torch.device("cpu")
    else:  # "cuda" or anything else -> use cuda if available
        device_t = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    model = model.to(device_t)
    return device_t, model, False, rank, world



# <<<--------
# -----------------------------
# Decide runtime run_mode & wrap BEFORE loaders (DDP needs sampler)
# -----------------------------
if run_mode == "dp":
    if not torch.cuda.is_available():
        raise RuntimeError("run_mode='dp' but CUDA is not available.")
    if gpu_device_ids is None:
        gpu_device_ids = list(range(torch.cuda.device_count()))
    if len(gpu_device_ids) == 0:
        raise RuntimeError("run_mode='dp' but no CUDA devices found.")

    device_t = torch.device(f"cuda:{gpu_device_ids[0]}")
    model = model.to(device_t)

    if len(gpu_device_ids) > 1:
        model = nn.DataParallel(model, device_ids=gpu_device_ids, output_device=gpu_device_ids[0])

    rank, world, is_ddp = 0, 1, False

else:
    # cpu / cuda / ddp
    device_t, model, is_ddp, rank, world = setup_device_and_model(model, run_mode)

# -------->>>




# <<<<<<<<<<<<<-------- DP Fucntion <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
import torch
import torch.nn as nn
from typing import Iterable, Tuple


def setup_dp_model(
    model: nn.Module,
    gpu_device_ids: Iterable[int] | None = None,
) -> Tuple[nn.Module, torch.device, int, int, bool]:
    """
    Configure a model for single-process DataParallel (DP).

    Behavior:
    - Verifies CUDA availability
    - Auto-selects all visible GPUs if gpu_device_ids is None
    - Moves model to primary GPU
    - Wraps model with nn.DataParallel if multiple GPUs are provided

    Returns
    -------
    model : nn.Module
        Possibly DataParallel-wrapped model
    device_t : torch.device
        Primary device (cuda:<first_gpu>)
    rank : int
        Always 0 for DP
    world : int
        Always 1 for DP
    is_ddp : bool
        Always False for DP

    Example: ******************************
    model, device_t, rank, world, is_ddp = setup_dp_model( model, gpu_device_ids=[0,],)
    """

    if not torch.cuda.is_available():
        raise RuntimeError("run_mode='dp' but CUDA is not available.")

    if gpu_device_ids is None:
        gpu_device_ids = list(range(torch.cuda.device_count()))

    gpu_device_ids = list(gpu_device_ids)
    if len(gpu_device_ids) == 0:
        raise RuntimeError("run_mode='dp' but no CUDA devices found.")

    primary_gpu = int(gpu_device_ids[0])
    device_t = torch.device(f"cuda:{primary_gpu}")

    model = model.to(device_t)

    if len(gpu_device_ids) > 1:
        model = nn.DataParallel(
            model,
            device_ids=gpu_device_ids,
            output_device=primary_gpu,
        )

    rank = 0
    world = 1
    is_ddp = False

    return model, device_t, rank, world, is_ddp

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# <<<<<<<<<<<<<-------- DDP Fucntion <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP


def is_ddp_run(mode: str) -> bool:
    world = int(os.environ.get("WORLD_SIZE", "1"))
    return (mode == "ddp") and (world > 1)


def ddp_cleanup():
    if dist.is_available() and dist.is_initialized():
        dist.destroy_process_group()


def setup_device_and_model(model, mode: str):
    """
    mode: "cpu" | "cuda" | "ddp"
      - "dp" is handled OUTSIDE this function
      - "ddp" requires: torchrun --nproc_per_node=N script.py

    Returns:
      model, device_t, rank, world, is_ddp

    Example: ************************************
      model, device_t, rank, world, is_ddp = setup_device_and_model(model, mode=run_mode)
    """
    world_env = int(os.environ.get("WORLD_SIZE", "1"))
    is_ddp = (mode == "ddp") and (world_env > 1)

    if is_ddp:
        want_cuda = torch.cuda.is_available()
        backend = "nccl" if want_cuda else "gloo"

        if not dist.is_initialized():
            dist.init_process_group(backend=backend)

        rank = dist.get_rank()
        world = dist.get_world_size()

        if want_cuda:
            local_rank = int(os.environ.get("LOCAL_RANK", "0"))
            torch.cuda.set_device(local_rank)
            device_t = torch.device(f"cuda:{local_rank}")

            model = model.to(device_t)
            model = DDP(
                model,
                device_ids=[local_rank],
                output_device=local_rank,
                find_unused_parameters=False,
            )
        else:
            device_t = torch.device("cpu")
            model = model.to(device_t)
            model = DDP(model)

        return model, device_t, rank, world, True

    # Single-process CPU/CUDA (DP wrapping happens outside)
    rank, world = 0, 1

    if mode == "cpu":
        device_t = torch.device("cpu")
    else:  # "cuda" (or fallback)
        device_t = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    model = model.to(device_t)
    return model, device_t, rank, world, False

#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

## Model to device
if run_mode == "dp":
    model, device_t, rank, world, is_ddp = setup_dp_model(model, gpu_device_ids)
else:
    model, device_t, rank, world, is_ddp = setup_device_and_model(model, run_mode)


#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# Kernel Command
torchrun \
  --nnodes=1 \
  --nproc_per_node=10 \
  train.py \
  --epochs 100 \
  --n-trials 50 \
  --n-trials-main-model 100 \
  --pbar-interval 100 \
  --train-epochs 500 \
  --run-mode ddp \
  --compile-train \
  --train-time-pkl train_time_ddp

#_______________________________________________________________________________




# Save dictionary as json preserving the order ****************************************************************
import json
print(json.dumps(final_metrics, indent=2, sort_keys=False))
with open("final_metrics.json", "w") as f:
    json.dump(final_metrics, f, sort_keys=False, indent=2)

#_______________________________________________________________________________




# Troubleshoot solve numba issues ========================
conda remove -y numpy numba llvmlite
conda install -y numpy=2.3 numba llvmlite
#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<




# Windows Ubuntu Speak ****************************************************************
# pip install pywin32

# sudo apt update
# sudo apt install espeak-ng
# espeak "Hello world"

import sys
import threading
import subprocess

_tts_lock = threading.Lock()

if sys.platform.startswith("win"):
    import win32com.client as wincl
    _speaker = wincl.Dispatch("SAPI.SpVoice")
    _speaker.Rate = 0       # -10 .. +10
    _speaker.Volume = 100   # 0 .. 100

    def speak_text(text: str):
        if not text:
            return
        with _tts_lock:
            print("Speaking:", text, flush=True)
            _speaker.Speak(text)

elif sys.platform.startswith("linux"):
    def speak_text(text: str, rate=150, volume=100):
        if not text:
            return
        with _tts_lock:
            print("Speaking:", text, flush=True)
            subprocess.run([
                "espeak",
                f"-s{rate}",
                f"-a{volume}",
                text
            ])

else:
    def speak_text(text: str):
        print("TTS not supported on this OS:", text)
#_______________________________________________________________________________




# argparse example ****************************************************************

import argparse

def parse_args():
    parser = argparse.ArgumentParser(
        description="BlindHelp camera client"
    )

    parser.add_argument(
        "--source",
        choices=["esp32", "webcam"],
        default="esp32",
        help="Video source: esp32 (MJPEG over WiFi) or webcam (OpenCV)"
    )

    # Important for Spyder / IPython: ignore unknown args
    args, _ = parser.parse_known_args()
    return args
args = parse_args()

Usage: python train.py --source esp32


######################
import argparse
def parse_args():
    parser = argparse.ArgumentParser(description="RL training launcher")

    parser.add_argument(
        "--run_training_now",
        action="store_true",
        help="Run training immediately (default: False)",
    )

    # Spyder / IPython safe
    args, _ = parser.parse_known_args()
    return args
args = parse_args()

usage: python train.py --run_training_now
#_______________________________________________________________________________

# Diskpart make usb readonly ============================================
select disk 1/2/3
attribute disk clear/set readonly
#_______________________________________________________________________________




# Run Linux command on Windows ============================================
1. Open powershell as Admin
2. run: wsl --install -d Ubuntu
3. Reboot if prompted
4. Open normal PowerShell
5. run: wsl
6. uname -a
7. Update Ubuntu packages: sudo apt update && sudo apt upgrade -y
8. Install basic dependencies: sudo apt install -y git wget curl build-essential
9. Install Miniconda (Linux): wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
10. Run installer:: bash Miniconda3-latest-Linux-x86_64.sh
11. Reload shell:: source ~/.bashrc
12. Verify: conda --version

## Create a Conda environment for Isaac Lab =======================
conda create -n isaaclab python=3.10 -y
conda activate isaaclab

cd ~
git clone https://github.com/NVIDIA-Omniverse/IsaacLab.git

cd IsaacLab
./isaaclab.sh --install
Verify installation: ./isaaclab.sh --check
CUDA check: python -c "import torch; print(torch.cuda.is_available())"

Example Run training:
~/IsaacLab/isaaclab.sh -p scripts/train.py \
    --algorithm PPO \
    --num-envs 128 \
    --headless

#_______________________________________________________________________________




#================================================================================================
# Install / Use Isaac Sim + Isaac Lab on Windows ================================================
#================================================================================================
# Date: Jan 19, 2026
# Source:
https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/pip_installation.html

# Note (Python version must match Isaac Sim):====
# Isaac Sim 5.X -> Python 3.11
# Isaac Sim 4.X -> Python 3.10

# Enable Windows Long Paths (recommended)=====
Regedit -> HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem
LongPathsEnabled (REG_DWORD) = 1
(Reboot)

# Anaconda Prompt=====
# Prefer short paths to avoid Windows path-length issues
mkdir C:\conda
mkdir C:\conda\envs

conda create -p C:\conda\envs\isaaclab python=3.11 -y
conda activate C:\conda\envs\isaaclab

python -m pip install --upgrade pip

# Install PyTorch CUDA build (keep versions aligned)
pip install -U torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 ^
  --index-url https://download.pytorch.org/whl/cu128

# Install Isaac Sim (pip)
pip install "isaacsim[all,extscache]==5.1.0" --extra-index-url https://pypi.nvidia.com

# Clone Isaac Lab (use a short folder; avoid OneDrive)
cd C:\conda
git clone https://github.com/isaac-sim/IsaacLab.git

# Install Isaac Lab components
C:\conda\IsaacLab\isaaclab.bat --install rl_games
# or full:
C:\conda\IsaacLab\isaaclab.bat --install

# Verify
C:\conda\IsaacLab\isaaclab.bat -p scripts\tutorials\00_sim\create_empty.py


# Usage =============================
IsaacLab\isaaclab.bat -p "C:\conda\scripts\train2.py" ^
    --algorithm PPO ^
    --num-envs 128 ^
    --headless
or
IsaacLab\isaaclab.bat -p "C:\conda\WarehouseBenchmark\scripts\train.py" --algorithm PPO --num-envs 128 --headless
#___________________________________________________________________________________
#_______________________________________________________________________________


# Show / Hide Warning ==========================================
import warnings
warnings.simplefilter("default")
def _warn_to_print(message, category, filename, lineno, file=None, line=None):
    print(f"[WARNING] {category.__name__}: {message}  ({filename}:{lineno})")
warnings.showwarning = _warn_to_print



import warnings
warnings.filterwarnings("ignore")
#___________________________________________________________________________________




#==============================================================
#==============================================================
#==============================================================
# python script to install all conda pip from a saved txt file
#==============================================================


# run_installs.py ->>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# python run_installs.py install_steps.txt
import subprocess
import sys
from pathlib import Path

def run(cmd: str):
    print(f"\n>>> {cmd}")
    subprocess.check_call(cmd, shell=True)

def main():
    steps_file = Path(sys.argv[1]) if len(sys.argv) > 1 else Path("install_steps.txt")
    if not steps_file.exists():
        raise FileNotFoundError(f"Missing: {steps_file.resolve()}")

    for raw in steps_file.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue

        # Ensure pip uses the current interpreter (important in conda envs)
        if line.startswith("pip "):
            line = f"{sys.executable} -m {line}"

        run(line)

    print("\nAll install steps completed.")

if __name__ == "__main__":
    main()

#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<




# install_steps.txt ->>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

pip install --no-cache-dir --force-reinstall tqdm ipywidgets jupyterlab_widgets
pip install --no-cache-dir --force-reinstall optuna
pip install --no-cache-dir --force-reinstall opencv-python


conda install -c conda-forge jupyterlab jupyter notebook spyder pandas matplotlib pillow -y

pip install --force-reinstall --no-cache-dir  torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0  --index-url https://download.pytorch.org/whl/cu124

pip install --no-cache-dir --force-reinstall  tensorboard
pip install --no-cache-dir  --force-reinstall pycocotools
pip install papermill

jupyter notebook

#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<



# Anaconda prompt command ->>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

conda create --name mask_rcnn python=3.12 -y
conda activate mask_rcnn
python run_installs.py install_steps.txt

#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
#==============================================================
#==============================================================
#==============================================================







