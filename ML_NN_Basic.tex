\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=1.2em}
\usepackage{bm}
\usepackage{tabularx}  % Add this in your preamble



\usepackage{array}
\usepackage{booktabs}
\usepackage{makecell} % better cell wrapping
\usepackage{multirow}
\usepackage{setspace}
\usepackage{xcolor} % for more color names like olive
\definecolor{salmon}{RGB}{250,128,114}

\usepackage{makeidx}   % For creating index
\makeindex             % Initialize index



\usepackage{hyperref}  % clickable ToC/LoF/LoT/index
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}



\begin{document}

\title{Summary of Activation Functions, Loss Functions, and Optimizers}
\author{Md Shahriar Forhad (https://github.com/Shahriar88)}
\date{}
\maketitle


% ---- Front matter (at beginning) ----
\pagenumbering{roman}
\tableofcontents
\listoffigures
\listoftables
%\printindex
\cleardoublepage
\pagenumbering{arabic}
% -------------------------------------



\section{Activation Functions}\index{Activation Functions}

\subsection{Sigmoid}\index{Activation Functions!Sigmoid}
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
\textbf{Applications:} Binary classification outputs, logistic regression, simple neural networks.  
\textbf{Advantages:}
\begin{itemize}
\item Smooth, differentiable.
\item Maps input to $(0,1)$ range for probability interpretation.
\end{itemize}
\textbf{Disadvantages:}
\begin{itemize}
\item Vanishing gradients for large $|x|$.
\item Non-zero-centered outputs slow convergence.
\end{itemize}

\subsection{Tanh}\index{Activation Functions!Tanh}
\[
\tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
\]
\textbf{Applications:} Hidden layers of RNNs, MLPs.  
\textbf{Advantages:}
\begin{itemize}
\item Zero-centered outputs.
\item Stronger gradients than Sigmoid.
\end{itemize}
\textbf{Disadvantages:}
\begin{itemize}
\item Still suffers from vanishing gradients.
\item Costlier than ReLU.
\end{itemize}

\subsection{ReLU}\index{Activation Functions!ReLU}
\[
\mathrm{ReLU}(x) = \max(0, x)
\]
\textbf{Applications:} CNNs, fully connected layers in deep networks.  
\textbf{Advantages:}
\begin{itemize}
\item Simple and efficient.
\item Sparse activations reduce computation.
\item Mitigates vanishing gradient problem.
\end{itemize}
\textbf{Disadvantages:}
\begin{itemize}
\item ``Dying ReLU'' problem for negative inputs.
\item Unbounded positive outputs.
\end{itemize}

\subsection{Leaky ReLU}\index{Activation Functions!Leaky ReLU}
\[
f(x) = \begin{cases} x, & x > 0 \\ \alpha x, & x \le 0 \end{cases}
\]
\textbf{Applications:} CNNs where ReLU causes dead neurons.  
\textbf{Advantages:} Avoids dying ReLU; small slope for negatives.  
\textbf{Disadvantages:} Requires $\alpha$ tuning.

\subsection{Softmax}\index{Activation Functions!Softmax}
\[
\mathrm{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
\]
\textbf{Applications:} Multiclass classification output layers.  
\textbf{Advantages:} Converts logits to probabilities.  
\textbf{Disadvantages:} Sensitive to large logits; saturation slows learning.

\subsection{GELU}\index{Activation Functions!GELU}
\[
\mathrm{GELU}(x) = x \Phi(x)
\]
\textbf{Applications:} Transformers, BERT, Vision Transformers.  
\textbf{Advantages:} Smooth and probabilistic activation.  
\textbf{Disadvantages:} More computation-heavy.

% ------------------------------------------------------

\section{Loss Functions}\index{Loss Functions}

\subsection{Mean Squared Error (MSE)}\index{Loss Functions!MSE}
\[
\mathrm{MSE} = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2
\]
\textbf{Applications:} Regression problems, autoencoders.  
\textbf{Advantages:} Penalizes large errors heavily; convex.  
\textbf{Disadvantages:} Sensitive to outliers.

\subsection{Mean Absolute Error (MAE)}\index{Loss Functions!MAE}
\[
\mathrm{MAE} = \frac{1}{N}\sum_{i=1}^{N} |y_i - \hat{y}_i|
\]
\textbf{Applications:} Robust regression.  
\textbf{Advantages:} Robust to outliers.  
\textbf{Disadvantages:} Non-differentiable at 0.

\subsection{Huber Loss}\index{Loss Functions!Huber Loss}
\[
L_\delta(e) =
\begin{cases}
\frac{1}{2}e^2, & |e| \le \delta \\
\delta(|e| - \frac{1}{2}\delta), & |e| > \delta
\end{cases}
\]
\textbf{Applications:} Regression with moderate outliers.  
\textbf{Advantages:} Combines smoothness (MSE) and robustness (MAE).  
\textbf{Disadvantages:} Requires tuning $\delta$.

\subsection{Cross Entropy Loss}\index{Loss Functions!Cross Entropy}
\[
L = -\frac{1}{N}\sum_{i=1}^N \sum_{k=1}^K y_{ik}\log p_{ik}
\]
\textbf{Applications:} Classification (binary, multiclass, multi-label).  
\textbf{Advantages:} Probabilistic; aligns with maximum likelihood.  
\textbf{Disadvantages:} Sensitive to noisy labels; overconfident predictions.

\subsection{Focal Loss}\index{Loss Functions!Focal Loss}
\[
L = -\alpha (1 - p_t)^\gamma \log(p_t)
\]
\textbf{Applications:} Object detection (Mask R-CNN, RetinaNet).  
\textbf{Advantages:} Handles class imbalance.  
\textbf{Disadvantages:} Hyperparameters $\alpha$, $\gamma$ need tuning.

\subsection{Dice / IoU Loss}\index{Loss Functions!Dice Loss}\index{Loss Functions!IoU Loss}
\[
\mathrm{Dice} = \frac{2|A \cap B|}{|A| + |B|}, \qquad
L = 1 - \mathrm{Dice}
\]
\textbf{Applications:} Image segmentation (Mask R-CNN, U-Net).  
\textbf{Advantages:} Works well with imbalanced masks.  
\textbf{Disadvantages:} Non-linear, harder optimization.

\subsection{KL Divergence}\index{Loss Functions!KL Divergence}
\[
D_{KL}(P\Vert Q) = \sum_i P(i)\log\frac{P(i)}{Q(i)}
\]
\textbf{Applications:} Variational Autoencoders, distillation.  
\textbf{Advantages:} Measures information difference.  
\textbf{Disadvantages:} Asymmetric; can be unstable.

\setlength{\tabcolsep}{4pt} % tighter columns

\subsection*{Loss Functions}
\renewcommand{\arraystretch}{1.25} % row spacing
\begin{tabular}{|p{2.8cm}|p{2.8cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Loss} & \textbf{Typical Use} & \textbf{Advantages} & \textbf{Disadvantages} \\ \hline

MSE ($\frac{1}{N}\sum (y-\hat y)^2$) & Regression &
Convex, smooth, strong penalty on large errors &
Sensitive to outliers \\ \hline

MAE ($\frac{1}{N}\sum |y-\hat y|$) & Robust regression &
Robust to outliers &
Non-differentiable at 0; slower convergence \\ \hline

Huber / Smooth-$L_1$ & Regression, box regression &
Combines MSE and MAE; robust and smooth &
Requires tuning of $\delta$ \\ \hline

Binary CE & Binary classification &
Probabilistic; aligns with MLE &
Sensitive to noisy labels \\ \hline

Multiclass CE & Multiclass classification &
Standard loss with softmax; probabilistic interpretation &
Overconfident predictions are heavily penalized \\ \hline

BCEWithLogits & Multi-label classification &
Numerically stable (sigmoid + CE combined) &
Ignores label dependencies \\ \hline

Focal Loss & Imbalanced classification/detection &
Focuses learning on hard examples &
Requires tuning of $\alpha$ and $\gamma$ \\ \hline

Dice Loss & Image segmentation (masks) &
Handles class imbalance; measures overlap directly &
Unstable for very small targets \\ \hline

IoU (Jaccard) Loss & Segmentation / bounding boxes &
Directly optimizes IoU metric &
Non-smooth; slower early convergence \\ \hline

Smooth-$L_1$ (boxes) & Object detection (R-CNNs) &
Robust to outliers; standard for box regression &
Scale-sensitive \\ \hline

KL Divergence & VAEs, knowledge distillation &
Measures divergence between distributions &
Asymmetric; unstable when $Q\approx0$ \\ \hline

Triplet Loss & Metric learning &
Learns discriminative embedding spaces &
Needs triplet mining; margin tuning \\ \hline

Contrastive Loss & Siamese networks / similarity tasks &
Learns pairwise distance relationships &
Requires balanced positive/negative pairs \\ \hline

Cosine Embedding & Text/vision embeddings &
Scale-invariant and simple &
Loses magnitude information \\ \hline

Perceptual Loss & Super-resolution / style transfer &
Encourages perceptual similarity using deep features &
Computationally heavy; needs pretrained $\phi$ \\ \hline

Total Variation & Image smoothing / denoising &
Removes noise, encourages smoothness &
Can over-smooth fine details \\ \hline
\end{tabular}

% ------------------------------------------------------

\section{Optimizers}\index{Optimizers}

\subsection{Stochastic Gradient Descent (SGD)}\index{Optimizers!SGD}
\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta_t)
\]
\textbf{Applications:} Classical ML, CNNs.  
\textbf{Advantages:} Simple, effective.  
\textbf{Disadvantages:} Sensitive to learning rate; oscillates.

\subsection{Momentum}\index{Optimizers!Momentum}
\[
v_t = \beta v_{t-1} + (1 - \beta)\nabla_\theta J(\theta_t), \quad
\theta_{t+1} = \theta_t - \eta v_t
\]
\textbf{Applications:} Deep CNNs.  
\textbf{Advantages:} Faster convergence; less noise.  
\textbf{Disadvantages:} Needs tuning of $\beta$.

\subsection{Adagrad}\index{Optimizers!Adagrad}
\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot \nabla_\theta J(\theta_t)
\]
\textbf{Applications:} Sparse features (e.g. NLP embeddings).  
\textbf{Advantages:} Adaptive learning rate.  
\textbf{Disadvantages:} Learning rate decays too fast.

\subsection{RMSProp}\index{Optimizers!RMSProp}
\[
E[g^2]_t = \beta E[g^2]_{t-1} + (1 - \beta)g_t^2, \quad
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t
\]
\textbf{Applications:} RNNs, time series, non-stationary data.  
\textbf{Advantages:} Stable, adaptive.  
\textbf{Disadvantages:} Sensitive to $\beta$.

\subsection{Adam}\index{Optimizers!Adam}
\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t,\quad
v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t^2
\]
\[
\hat{m}_t = \frac{m_t}{1 - \beta_1^t},\quad
\hat{v}_t = \frac{v_t}{1 - \beta_2^t},\quad
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}\hat{m}_t
\]
\textbf{Applications:} Deep learning, NLP, Transformers.  
\textbf{Advantages:} Combines momentum + adaptive rate.  
\textbf{Disadvantages:} May generalize poorly.

\subsection{AdamW}\index{Optimizers!AdamW}
\[
\theta_{t+1} = \theta_t - \eta\left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t\right)
\]
\textbf{Applications:} Transformers, BERT, ViTs.  
\textbf{Advantages:} Decoupled weight decay â†’ better generalization.  
\textbf{Disadvantages:} Slightly more computation.

\subsection{LAMB}\index{Optimizers!LAMB}
\[
r_t = \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}, \quad
\theta_{t+1} = \theta_t - \eta \frac{\|\theta_t\|}{\|r_t\|} r_t
\]
\textbf{Applications:} Large-batch Transformer training.  
\textbf{Advantages:} Enables distributed training.  
\textbf{Disadvantages:} Complex; more tuning.

% ------------------------------------------------------

\section{Summary Tables}

\subsection*{Activation Functions}
\begin{tabular}{|l|l|l|}
\hline
Function & Pros & Cons \\ \hline
Sigmoid & Probabilistic, smooth & Vanishing gradient \\
Tanh & Zero-centered & Saturation at extremes \\
ReLU & Sparse, fast & Dead neurons \\
Leaky ReLU & Fixes dead ReLU & Hyperparam $\alpha$ \\
Softmax & Probabilities & Saturation \\
GELU & Smooth & Slower compute \\ \hline
\end{tabular}

\subsection*{Optimizers}
\begin{tabular}{|l|l|l|}
\hline
Optimizer & Pros & Cons \\ \hline
SGD & Simple, reliable & Slow, oscillates \\
Momentum & Smooth convergence & Needs tuning \\
RMSProp & Stable & Sensitive $\beta$ \\
Adam & Fast, adaptive & May overfit \\
AdamW & Best generalization & Slightly slower \\ \hline
\end{tabular}

\newpage







\section{Forward Pass}\index{Forward Pass}

\noindent
The forward pass computes activations layer by layer using learned weights and biases. Each layer applies linear transformations followed by nonlinear activation functions (e.g., sigmoid, ReLU, etc.).

\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{4pt}

\begin{table}[h!]
\centering
\caption{Forward Pass Computations and Data Structures}
\begin{tabular}{|p{3.5cm}|p{5cm}|p{4cm}|p{3cm}|}
\hline
\textbf{Step} & \textbf{Calculation} & \textbf{Data Structure} & \textbf{Multiplication Type} \\ \hline

\multicolumn{4}{|l|}{\textbf{Forward Pass}} \\ \hline

Input to 1st hidden layer (HI\_1) &
$HI_1 = W_1 \cdot X + B_1$ &
$HI_1$: Matrix, $W_1$: Matrix, $X$: Matrix, $B_1$: Vector &
\textbf{Dot Product} (between $W_1$ and $X$) \\ \hline

Output of 1st hidden layer (HO\_1) &
$HO_1 = \sigma(HI_1)$ &
$HO_1$: Vector &
\textbf{Element-wise} (Sigmoid applied element-wise) \\ \hline

Input to 2nd hidden layer (HI\_2) &
$HI_2 = W_2 \cdot HO_1 + B_2$ &
$HI_2$: Vector, $W_2$: Matrix, $HO_1$: Vector, $B_2$: Vector &
\textbf{Dot Product} (between $W_2$ and $HO_1$) \\ \hline

Output of 2nd hidden layer (HO\_2) &
$HO_2 = \sigma(HI_2)$ &
$HO_2$: Vector &
\textbf{Element-wise} (Sigmoid applied element-wise) \\ \hline

Input to output layer (HO\_final) &
$HO_{final} = W_3 \cdot HO_2 + B_3$ &
$HO_{final}$: Vector, $W_3$: Matrix, $HO_2$: Vector, $B_3$: Vector &
\textbf{Dot Product} (between $W_3$ and $HO_2$) \\ \hline

Final output $\hat{Y}$ &
$\hat{Y} = \sigma(HO_{final})$ &
$\hat{Y}$: Vector &
\textbf{Element-wise} (Sigmoid applied element-wise) \\ \hline

Error Calculation &
$E = Y - \hat{Y}$ &
$E$: Vector, $Y$: Vector, $\hat{Y}$: Vector &
\textbf{Element-wise} (Subtraction) \\ \hline
\end{tabular}
\end{table}






\section{Backpropagation}\index{Backpropagation}
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{4pt}

\begin{table}[h!]
\centering
\caption{Comprehensive Backpropagation Steps}
\begin{tabular}{|p{3.8cm}|p{5.2cm}|p{4.2cm}|p{3cm}|}
\hline
\textbf{Step} & \textbf{Calculation} & \textbf{Data Structure} & \textbf{Multiplication Type} \\ \hline

\multicolumn{4}{|l|}{\textbf{Backpropagation}} \\ \hline

Error at output layer &
$err_{HO_{final}} = E \cdot \sigma'(HO_{final})$ &
$err_{HO_{final}}$: Vector, $E$: Vector, $\sigma'(HO_{final})$: Vector &
\textbf{Element-wise} (Multiplication) \\ \hline

Error in 2nd hidden layer (err\_HO\_2) &
$err_{HO_2} = err_{HO_{final}} \cdot W_3^{T} \cdot \sigma'(HI_2)$ &
$err_{HO_2}$: Vector, $W_3^{T}$: Matrix (transpose), $\sigma'(HI_2)$: Vector &
\textbf{Dot Product} (between $err_{HO_{final}}$ and $W_3^{T}$) followed by \textbf{Element-wise} multiplication with $\sigma'(HI_2)$ \\ \hline

Error in 1st hidden layer (err\_HO\_1) &
$err_{HO_1} = err_{HO_2} \cdot W_2^{T} \cdot \sigma'(HI_1)$ &
$err_{HO_1}$: Vector, $W_2^{T}$: Matrix (transpose), $\sigma'(HI_1)$: Vector &
\textbf{Dot Product} (between $err_{HO_2}$ and $W_2^{T}$) followed by \textbf{Element-wise} multiplication with $\sigma'(HI_1)$ \\ \hline

Error Calculation &
$E = Y - \hat{Y}$ &
$E$: Vector, $Y$: Vector, $\hat{Y}$: Vector &
\textbf{Element-wise} (Subtraction) \\ \hline
\end{tabular}
\end{table}


\begin{center}
\fbox{%
\parbox{0.92\linewidth}{%
\[
\textcolor{blue}{Error\_Layer\_N} = 
\textcolor{olive}{\sigma'_d(Input\_N)} 
\Bigl(
\textcolor{salmon}{Error\_Layer\_{N+1}} 
(\textcolor{orange}{W\_{N+1}})^{T}
\Bigr),
\qquad
\textcolor{green}{Input\_N = W X + B.}
\]
}}
\end{center}





\section{Weight and Bias Updates}\index{Weight and Bias Updates}
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{4pt}

\begin{table}[h!]
\centering
\caption{Weight and Bias Update Steps}
\begin{tabular}{|p{3.8cm}|p{5.2cm}|p{4.2cm}|p{3cm}|}
\hline
\textbf{Step} & \textbf{Calculation} & \textbf{Data Structure} & \textbf{Multiplication Type} \\ \hline

\multicolumn{4}{|l|}{\textbf{Weight and Bias Updates}} \\ \hline

Update $W_1$ and $B_1$ &
$W_1 = W_1 + lr \cdot X^{T} \cdot err_{HO_1}, \quad B_1 = B_1 + lr \cdot err_{HO_1}$ &
$W_1$: Matrix, $X^{T}$: Matrix (transpose), $err_{HO_1}$: Vector &
\textbf{Dot Product} (between $X^{T}$ and $err_{HO_1}$) \\ \hline

Update $W_2$ and $B_2$ &
$W_2 = W_2 + lr \cdot HO_1^{T} \cdot err_{HO_2}, \quad B_2 = B_2 + lr \cdot err_{HO_2}$ &
$W_2$: Matrix, $HO_1^{T}$: Matrix (transpose), $err_{HO_2}$: Vector &
\textbf{Dot Product} (between $HO_1^{T}$ and $err_{HO_2}$) \\ \hline

Update $W_3$ and $B_3$ &
$W_3 = W_3 + lr \cdot HO_2^{T} \cdot err_{HO_{final}}, \quad B_3 = B_3 + lr \cdot err_{HO_{final}}$ &
$W_3$: Matrix, $HO_2^{T}$: Matrix (transpose), $err_{HO_{final}}$: Vector &
\textbf{Dot Product} (between $HO_2^{T}$ and $err_{HO_{final}}$) \\ \hline
\end{tabular}
\end{table}

\begin{center}
\fbox{%
\parbox{0.92\linewidth}{%
\[
\textcolor{blue}{W_{new}} = 
\textcolor{green!60!black}{W_{old}} + 
\textcolor{olive}{lr} \cdot 
\textcolor{purple}{Own\_Error} \cdot 
\textcolor{orange}{Own\_Input^{T}}
\]

\[
\textcolor{orange}{Own\_Input} = \textcolor{purple}{X}, \quad
\textcolor{orange}{Own\_Input\_N} = \textcolor{purple}{HO_{N-1}} = 
\textcolor{purple}{\sigma(HI_{N-1})}
\]
}}
\end{center}






\section{General Structure of Forward Pass and Backpropagation}
The general structure of the forward pass and backpropagation is the same for all basic neural networks, regardless of how many hidden layers or neurons are used. The following principles hold true for most feedforward neural networks (also known as \textbf{multilayer perceptrons (MLPs)}).

\subsection{Forward Pass}
\subsubsection*{Basic Structure}
\begin{itemize}
    \item The network consists of an input layer, one or more hidden layers, and an output layer.
    \item Each layer computes a weighted sum of its inputs (or the outputs of the previous layer), adds a bias, and applies an activation function (such as sigmoid, ReLU, or tanh) to produce its output.
\end{itemize}

\subsubsection*{Computation Flow}
\begin{itemize}
    \item Data flows forward through the network from the input layer to the output layer.
    \item The output of one layer becomes the input to the next layer.
\end{itemize}

\subsection{Backpropagation}
\subsubsection*{Error Propagation}
\begin{itemize}
    \item After computing the final output during the forward pass, the network compares the predicted output with the expected (target) output.
    \item The error is calculated using a loss function (for example, mean squared error or cross-entropy).
    \item This error is propagated backward from the output layer to the hidden layers, adjusting the neuron weights to minimize the error.
\end{itemize}

\subsubsection*{Weight Update}
\begin{itemize}
    \item The gradients (rate of change of error with respect to weights) are computed using the chain rule of calculus.
    \item The weights and biases are updated using gradient descent or its variants (e.g., stochastic gradient descent).
\end{itemize}

\subsubsection*{General Steps}
\begin{enumerate}
    \item Compute the error at the output layer.
    \item Backpropagate the error through each preceding layer.
    \item Update the weights and biases to reduce the overall error.
\end{enumerate}

\subsection{Universality of the Process}
\noindent The process of forward pass and backpropagation described above holds for all basic neural networks, regardless of:
\begin{itemize}
    \item The number of hidden layers.
    \item The number of neurons per layer.
    \item The activation function used.
\end{itemize}

\subsection{Variations}
While the core steps remain the same, there are variations among different network types and techniques:
\begin{itemize}
    \item \textbf{Activation Functions:} Modern networks often use ReLU (Rectified Linear Unit) or tanh instead of sigmoid in hidden layers.
    \item \textbf{Loss Functions:} Classification problems typically use cross-entropy loss, while regression problems often use mean squared error (MSE).
    \item \textbf{Learning Algorithms:} Standard gradient descent can be replaced by advanced optimizers like Adam, RMSProp, or Adagrad, which adaptively adjust the learning rate.
\end{itemize}

\subsection{Summary}
\begin{itemize}
    \item The concept of forward pass and backpropagation is \textbf{universal} to most feedforward neural networks.
    \item Differences arise mainly in the architecture (number of layers or neurons) and the type of activation, loss function, and optimization algorithm used.
    \item As networks become deeper and more complex, the same principles apply, but across more layers.
\end{itemize}

\subsubsection*{Examples}
\begin{itemize}
    \item \textbf{Deep Neural Networks (DNNs):} Contain more hidden layers, but the core forward and backward propagation remain identical.
    \item \textbf{Convolutional Neural Networks (CNNs)} and \textbf{Recurrent Neural Networks (RNNs):} Both employ forward and backward propagation with specialized modifications.
\end{itemize}

\noindent In essence, for any basic feedforward neural network, the process of forward pass and backpropagation remains fundamentally the same.



% ============================================================
% Forward + Backprop Example Tables (copy/paste into your paper)
% ============================================================

% ---------------------------
% Forward propagation (single neuron)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Forward propagation for a single-neuron network}
\label{tab:forward_single}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{c l l c}
\toprule
\textbf{Step} & \textbf{Quantity} & \textbf{Expression} & \textbf{Numerical value} \\
\midrule
1 & Input         & $x$                                & $2$ \\
2 & Weight        & $w$                                & $3$ \\
3 & Bias          & $b$                                & $1$ \\
4 & Linear output & $z = wx + b$                       & $7$ \\
5 & Activation    & $\hat{y} = \sigma(z)$              & $0.999088$ \\
6 & Target        & $y$                                & $1$ \\
7 & Loss          & $\mathcal{L}=\frac12(y-\hat{y})^2$ & $4.16\times10^{-7}$ \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Backpropagation (output layer / layer 2)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Backpropagation for the output layer (layer 2)}
\label{tab:backprop_output}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c l l c p{6.2cm}}
\toprule
\textbf{Step} & \textbf{Gradient} & \textbf{Formula} & \textbf{Value} & \textbf{Value comes from} \\
\midrule
7  & $\frac{\partial \mathcal{L}}{\partial \hat{y}}$
   & $\hat{y}-y$
   & $\textminus 0.119587$
   & Forward step 5 ($\hat{y}$), forward step 6 ($y$) \\

8  & $\frac{\partial \hat{y}}{\partial z_2}$
   & $\hat{y}(1-\hat{y})$
   & $0.105233$
   & Forward step 5 ($\hat{y}$) \\

9  & $\frac{\partial \mathcal{L}}{\partial z_2}$
   & Step 7 $\times$ Step 8
   & $\textminus 0.012580$
   & Backprop steps 7 and 8 \\

10 & $\frac{\partial z_2}{\partial w_2}$
   & $a_1$
   & $0.999088$
   & Forward step 3 ($a_1$) \\

11 & $\frac{\partial \mathcal{L}}{\partial w_2}$
   & Step 9 $\times$ Step 10
   & $\textminus 0.012569$
   & Backprop step 9, forward step 3 \\

12 & $\frac{\partial z_2}{\partial b_2}$
   & $1$
   & $1$
   & Affine layer definition \\

13 & $\frac{\partial \mathcal{L}}{\partial b_2}$
   & Step 9 $\times$ Step 12
   & $\textminus 0.012580$
   & Backprop step 9 \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Backpropagation (hidden layer / layer 1)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Backpropagation for the hidden layer (layer 1)}
\label{tab:backprop_hidden}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c l l c p{6.2cm}}
\toprule
\textbf{Step} & \textbf{Gradient} & \textbf{Formula} & \textbf{Value} & \textbf{Value comes from} \\
\midrule
14 & $\frac{\partial z_2}{\partial a_1}$
   & $w_2$
   & $4$
   & Forward definition of layer 2 \\

15 & $\frac{\partial \mathcal{L}}{\partial a_1}$
   & Step 9 $\times$ Step 14
   & $\textminus 0.050319$
   & Backprop step 9, forward step 4 \\

16 & $\frac{\partial a_1}{\partial z_1}$
   & $a_1(1-a_1)$
   & $0.000911$
   & Forward step 3 ($a_1$) \\

17 & $\frac{\partial \mathcal{L}}{\partial z_1}$
   & Step 15 $\times$ Step 16
   & $\textminus 4.584\times10^{-5}$
   & Backprop steps 15 and 16 \\

18 & $\frac{\partial z_1}{\partial w_1}$
   & $x$
   & $2$
   & Forward step 1 ($x$) \\

19 & $\frac{\partial \mathcal{L}}{\partial w_1}$
   & Step 17 $\times$ Step 18
   & $\textminus 9.168\times10^{-5}$
   & Backprop step 17, forward step 1 \\

20 & $\frac{\partial z_1}{\partial b_1}$
   & $1$
   & $1$
   & Affine layer definition \\

21 & $\frac{\partial \mathcal{L}}{\partial b_1}$
   & Step 17 $\times$ Step 20
   & $\textminus 4.584\times10^{-5}$
   & Backprop step 17 \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Note under tables
% ---------------------------
\noindent\textbf{Note.}
Each gradient is computed using the chain rule and depends exclusively on
quantities obtained during the forward pass or gradients propagated from
subsequent layers. This explicit dependency structure mirrors the computation
graph used by automatic differentiation frameworks.





%===============================

% ============================================================
% Three-layer (2 hidden + 1 output) Example with 3 Activations
% Activations: ReLU -> tanh -> sigmoid
% ============================================================

\subsection{Three-Layer Forward and Backprop Example}\index{Forward Pass!Three-layer example}\index{Backpropagation!Three-layer example}

\noindent\textbf{Network definition (scalar).}\index{Activation Functions!ReLU}\index{Activation Functions!tanh}\index{Activation Functions!Sigmoid}
\[
z_1 = w_1 x + b_1,\quad a_1 = \mathrm{ReLU}(z_1),
\]
\[
z_2 = w_2 a_1 + b_2,\quad a_2 = \tanh(z_2),
\]
\[
z_3 = w_3 a_2 + b_3,\quad \hat{y} = \sigma(z_3),
\qquad
\mathcal{L}=\tfrac12(y-\hat{y})^2.
\]

\noindent\textbf{Activation derivatives.}\index{Derivative!ReLU}\index{Derivative!tanh}\index{Derivative!Sigmoid}
\[
\frac{d}{dz}\mathrm{ReLU}(z)=
\begin{cases}
1,& z>0\\
0,& z\le 0
\end{cases},
\qquad
\frac{d}{dz}\tanh(z)=1-\tanh^2(z),
\qquad
\frac{d}{dz}\sigma(z)=\sigma(z)\bigl(1-\sigma(z)\bigr).
\]

\noindent\textbf{Numerical values used.}\index{Example!Three-layer backprop}
\[
x=2,\;
(w_1,b_1)=(1.5,-1),\;
(w_2,b_2)=(0.5,0.1),\;
(w_3,b_3)=(2,-0.3),\;
y=1.
\]

% ---------------------------
% Forward propagation (three-layer network)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Forward propagation for a three-layer network with three different activation functions}
\label{tab:forward_three_layer}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{c l l c}
\toprule
\textbf{Step} & \textbf{Quantity} & \textbf{Expression} & \textbf{Numerical value} \\
\midrule
1  & Input & $x$ & $2$ \\

2  & Layer 1 weight & $w_1$ & $1.5$ \\
3  & Layer 1 bias & $b_1$ & $\textminus 1$ \\
4  & Pre-activation (L1) & $z_1 = w_1x + b_1$ & $2.0$ \\
5  & Activation (L1, ReLU) & $a_1 = \mathrm{ReLU}(z_1)$ & $2.0$ \\

6  & Layer 2 weight & $w_2$ & $0.5$ \\
7  & Layer 2 bias & $b_2$ & $0.1$ \\
8  & Pre-activation (L2) & $z_2 = w_2a_1 + b_2$ & $1.1$ \\
9  & Activation (L2, tanh) & $a_2 = \tanh(z_2)$ & $0.800499$ \\

10 & Layer 3 weight & $w_3$ & $2$ \\
11 & Layer 3 bias & $b_3$ & $\textminus 0.3$ \\
12 & Pre-activation (L3) & $z_3 = w_3a_2 + b_3$ & $1.300998$ \\
13 & Output activation (sigmoid) & $\hat{y}=\sigma(z_3)$ & $0.786003$ \\

14 & Target & $y$ & $1$ \\
15 & Loss & $\mathcal{L}=\tfrac12(y-\hat{y})^2$ & $0.022897$ \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Backpropagation (Layer 3: sigmoid output)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Backpropagation through the output layer (layer 3, sigmoid)}
\label{tab:backprop_layer3}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c l l c p{6.4cm}}
\toprule
\textbf{Step} & \textbf{Gradient} & \textbf{Formula} & \textbf{Value} & \textbf{Value comes from} \\
\midrule
16 & $\frac{\partial \mathcal{L}}{\partial \hat{y}}$
   & $\hat{y}-y$
   & $\textminus 0.213997$
   & Forward step 13 ($\hat{y}$), step 14 ($y$) \\

17 & $\frac{\partial \hat{y}}{\partial z_3}$
   & $\hat{y}(1-\hat{y})$
   & $0.168202$
   & Forward step 13 ($\hat{y}$) \\

18 & $\frac{\partial \mathcal{L}}{\partial z_3}$
   & Step 16 $\times$ Step 17
   & $\textminus 0.035995$
   & Backprop steps 16 and 17 \\

19 & $\frac{\partial z_3}{\partial w_3}$
   & $a_2$
   & $0.800499$
   & Forward step 9 ($a_2$) \\

20 & $\frac{\partial \mathcal{L}}{\partial w_3}$
   & Step 18 $\times$ Step 19
   & $\textminus 0.028814$
   & Backprop step 18, forward step 9 \\

21 & $\frac{\partial z_3}{\partial b_3}$
   & $1$
   & $1$
   & Affine layer definition \\

22 & $\frac{\partial \mathcal{L}}{\partial b_3}$
   & Step 18 $\times$ Step 21
   & $\textminus 0.035995$
   & Backprop step 18 \\

23 & $\frac{\partial z_3}{\partial a_2}$
   & $w_3$
   & $2$
   & Forward step 10 ($w_3$) \\

24 & $\frac{\partial \mathcal{L}}{\partial a_2}$
   & Step 18 $\times$ Step 23
   & $\textminus 0.071990$
   & Backprop step 18, forward step 10 \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Backpropagation (Layer 2: tanh hidden)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Backpropagation through hidden layer 2 (layer 2, tanh)}
\label{tab:backprop_layer2}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c l l c p{6.4cm}}
\toprule
\textbf{Step} & \textbf{Gradient} & \textbf{Formula} & \textbf{Value} & \textbf{Value comes from} \\
\midrule
25 & $\frac{\partial a_2}{\partial z_2}$
   & $1-a_2^2$
   & $0.359201$
   & Forward step 9 ($a_2$) \\

26 & $\frac{\partial \mathcal{L}}{\partial z_2}$
   & Step 24 $\times$ Step 25
   & $\textminus 0.025859$
   & Backprop step 24, backprop step 25 \\

27 & $\frac{\partial z_2}{\partial w_2}$
   & $a_1$
   & $2.0$
   & Forward step 5 ($a_1$) \\

28 & $\frac{\partial \mathcal{L}}{\partial w_2}$
   & Step 26 $\times$ Step 27
   & $\textminus 0.051718$
   & Backprop step 26, forward step 5 \\

29 & $\frac{\partial z_2}{\partial b_2}$
   & $1$
   & $1$
   & Affine layer definition \\

30 & $\frac{\partial \mathcal{L}}{\partial b_2}$
   & Step 26 $\times$ Step 29
   & $\textminus 0.025859$
   & Backprop step 26 \\

31 & $\frac{\partial z_2}{\partial a_1}$
   & $w_2$
   & $0.5$
   & Forward step 6 ($w_2$) \\

32 & $\frac{\partial \mathcal{L}}{\partial a_1}$
   & Step 26 $\times$ Step 31
   & $\textminus 0.012929$
   & Backprop step 26, forward step 6 \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Backpropagation (Layer 1: ReLU hidden)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Backpropagation through hidden layer 1 (layer 1, ReLU)}
\label{tab:backprop_layer1}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c l l c p{6.4cm}}
\toprule
\textbf{Step} & \textbf{Gradient} & \textbf{Formula} & \textbf{Value} & \textbf{Value comes from} \\
\midrule
33 & $\frac{\partial a_1}{\partial z_1}$
   & $\mathbb{1}[z_1>0]$
   & $1$
   & Forward step 4 ($z_1=2.0>0$) \\

34 & $\frac{\partial \mathcal{L}}{\partial z_1}$
   & Step 32 $\times$ Step 33
   & $\textminus 0.012929$
   & Backprop step 32, backprop step 33 \\

35 & $\frac{\partial z_1}{\partial w_1}$
   & $x$
   & $2$
   & Forward step 1 ($x$) \\

36 & $\frac{\partial \mathcal{L}}{\partial w_1}$
   & Step 34 $\times$ Step 35
   & $\textminus 0.025859$
   & Backprop step 34, forward step 1 \\

37 & $\frac{\partial z_1}{\partial b_1}$
   & $1$
   & $1$
   & Affine layer definition \\

38 & $\frac{\partial \mathcal{L}}{\partial b_1}$
   & Step 34 $\times$ Step 37
   & $\textminus 0.012929$
   & Backprop step 34 \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Note under tables
% ---------------------------
\noindent\textbf{Note.}\index{Chain rule}\index{Automatic differentiation}
Each gradient is computed using the chain rule and depends exclusively on
quantities obtained during the forward pass or gradients propagated from
subsequent layers. The three activation derivatives appear explicitly:
sigmoid uses $\hat{y}(1-\hat{y})$, tanh uses $1-a_2^2$, and ReLU uses the gate
$\mathbb{1}[z_1>0]$.





\newpage





%================================

% ============================================================
% Three-layer (2 hidden + 1 output) Example with 3 Activations
% Activations: ReLU -> tanh -> sigmoid
% ============================================================

\subsection{Three-Layer Forward and Backprop Example}\index{Forward Pass!Three-layer example}\index{Backpropagation!Three-layer example}

\noindent\textbf{Network definition (scalar).}\index{Activation Functions!ReLU}\index{Activation Functions!tanh}\index{Activation Functions!Sigmoid}
\[
z_1 = w_1 x + b_1,\quad a_1 = \mathrm{ReLU}(z_1),
\]
\[
z_2 = w_2 a_1 + b_2,\quad a_2 = \tanh(z_2),
\]
\[
z_3 = w_3 a_2 + b_3,\quad \hat{y} = \sigma(z_3),
\qquad
\mathcal{L}=\tfrac12(y-\hat{y})^2.
\]

\noindent\textbf{Activation derivatives.}\index{Derivative!ReLU}\index{Derivative!tanh}\index{Derivative!Sigmoid}
\[
\frac{d}{dz}\mathrm{ReLU}(z)=
\begin{cases}
1,& z>0\\
0,& z\le 0
\end{cases},
\qquad
\frac{d}{dz}\tanh(z)=1-\tanh^2(z),
\qquad
\frac{d}{dz}\sigma(z)=\sigma(z)\bigl(1-\sigma(z)\bigr).
\]

\noindent\textbf{Numerical values used.}\index{Example!Three-layer backprop}
\[
x=2,\;
(w_1,b_1)=(1.5,-1),\;
(w_2,b_2)=(0.5,0.1),\;
(w_3,b_3)=(2,-0.3),\;
y=1.
\]

% ---------------------------
% Forward propagation (three-layer network)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Forward propagation for a three-layer network with three different activation functions}
\label{tab:forward_three_layer}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{c l l c}
\toprule
\textbf{Step} & \textbf{Quantity} & \textbf{Expression} & \textbf{Numerical value} \\
\midrule
1  & Input & $x$ & $2$ \\

2  & Layer 1 weight & $w_1$ & $1.5$ \\
3  & Layer 1 bias & $b_1$ & $\textminus 1$ \\
4  & Pre-activation (L1) & $z_1 = w_1x + b_1$ & $2.0$ \\
5  & Activation (L1, ReLU) & $a_1 = \mathrm{ReLU}(z_1)$ & $2.0$ \\

6  & Layer 2 weight & $w_2$ & $0.5$ \\
7  & Layer 2 bias & $b_2$ & $0.1$ \\
8  & Pre-activation (L2) & $z_2 = w_2a_1 + b_2$ & $1.1$ \\
9  & Activation (L2, tanh) & $a_2 = \tanh(z_2)$ & $0.800499$ \\

10 & Layer 3 weight & $w_3$ & $2$ \\
11 & Layer 3 bias & $b_3$ & $\textminus 0.3$ \\
12 & Pre-activation (L3) & $z_3 = w_3a_2 + b_3$ & $1.300998$ \\
13 & Output activation (sigmoid) & $\hat{y}=\sigma(z_3)$ & $0.786003$ \\

14 & Target & $y$ & $1$ \\
15 & Loss & $\mathcal{L}=\tfrac12(y-\hat{y})^2$ & $0.022897$ \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Backpropagation (Layer 3: sigmoid output)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Backpropagation through the output layer (layer 3, sigmoid)}
\label{tab:backprop_layer3}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c l l c p{6.4cm}}
\toprule
\textbf{Step} & \textbf{Gradient} & \textbf{Formula} & \textbf{Value} & \textbf{Value comes from} \\
\midrule
16 & $\frac{\partial \mathcal{L}}{\partial \hat{y}}$
   & $\hat{y}-y$
   & $\textminus 0.213997$
   & Forward step 13 ($\hat{y}$), step 14 ($y$) \\

17 & $\frac{\partial \hat{y}}{\partial z_3}$
   & $\hat{y}(1-\hat{y})$
   & $0.168202$
   & Forward step 13 ($\hat{y}$) \\

18 & $\frac{\partial \mathcal{L}}{\partial z_3}$
   & Step 16 $\times$ Step 17
   & $\textminus 0.035995$
   & Backprop steps 16 and 17 \\

19 & $\frac{\partial z_3}{\partial w_3}$
   & $a_2$
   & $0.800499$
   & Forward step 9 ($a_2$) \\

20 & $\frac{\partial \mathcal{L}}{\partial w_3}$
   & Step 18 $\times$ Step 19
   & $\textminus 0.028814$
   & Backprop step 18, forward step 9 \\

21 & $\frac{\partial z_3}{\partial b_3}$
   & $1$
   & $1$
   & Affine layer definition \\

22 & $\frac{\partial \mathcal{L}}{\partial b_3}$
   & Step 18 $\times$ Step 21
   & $\textminus 0.035995$
   & Backprop step 18 \\

23 & $\frac{\partial z_3}{\partial a_2}$
   & $w_3$
   & $2$
   & Forward step 10 ($w_3$) \\

24 & $\frac{\partial \mathcal{L}}{\partial a_2}$
   & Step 18 $\times$ Step 23
   & $\textminus 0.071990$
   & Backprop step 18, forward step 10 \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Backpropagation (Layer 2: tanh hidden)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Backpropagation through hidden layer 2 (layer 2, tanh)}
\label{tab:backprop_layer2}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c l l c p{6.4cm}}
\toprule
\textbf{Step} & \textbf{Gradient} & \textbf{Formula} & \textbf{Value} & \textbf{Value comes from} \\
\midrule
25 & $\frac{\partial a_2}{\partial z_2}$
   & $1-a_2^2$
   & $0.359201$
   & Forward step 9 ($a_2$) \\

26 & $\frac{\partial \mathcal{L}}{\partial z_2}$
   & Step 24 $\times$ Step 25
   & $\textminus 0.025859$
   & Backprop step 24, backprop step 25 \\

27 & $\frac{\partial z_2}{\partial w_2}$
   & $a_1$
   & $2.0$
   & Forward step 5 ($a_1$) \\

28 & $\frac{\partial \mathcal{L}}{\partial w_2}$
   & Step 26 $\times$ Step 27
   & $\textminus 0.051718$
   & Backprop step 26, forward step 5 \\

29 & $\frac{\partial z_2}{\partial b_2}$
   & $1$
   & $1$
   & Affine layer definition \\

30 & $\frac{\partial \mathcal{L}}{\partial b_2}$
   & Step 26 $\times$ Step 29
   & $\textminus 0.025859$
   & Backprop step 26 \\

31 & $\frac{\partial z_2}{\partial a_1}$
   & $w_2$
   & $0.5$
   & Forward step 6 ($w_2$) \\

32 & $\frac{\partial \mathcal{L}}{\partial a_1}$
   & Step 26 $\times$ Step 31
   & $\textminus 0.012929$
   & Backprop step 26, forward step 6 \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Backpropagation (Layer 1: ReLU hidden)
% ---------------------------
\begin{table}[htbp]
\centering
\caption{Backpropagation through hidden layer 1 (layer 1, ReLU)}
\label{tab:backprop_layer1}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{c l l c p{6.4cm}}
\toprule
\textbf{Step} & \textbf{Gradient} & \textbf{Formula} & \textbf{Value} & \textbf{Value comes from} \\
\midrule
33 & $\frac{\partial a_1}{\partial z_1}$
   & $\mathbb{1}[z_1>0]$
   & $1$
   & Forward step 4 ($z_1=2.0>0$) \\

34 & $\frac{\partial \mathcal{L}}{\partial z_1}$
   & Step 32 $\times$ Step 33
   & $\textminus 0.012929$
   & Backprop step 32, backprop step 33 \\

35 & $\frac{\partial z_1}{\partial w_1}$
   & $x$
   & $2$
   & Forward step 1 ($x$) \\

36 & $\frac{\partial \mathcal{L}}{\partial w_1}$
   & Step 34 $\times$ Step 35
   & $\textminus 0.025859$
   & Backprop step 34, forward step 1 \\

37 & $\frac{\partial z_1}{\partial b_1}$
   & $1$
   & $1$
   & Affine layer definition \\

38 & $\frac{\partial \mathcal{L}}{\partial b_1}$
   & Step 34 $\times$ Step 37
   & $\textminus 0.012929$
   & Backprop step 34 \\
\bottomrule
\end{tabular}
\end{table}


% ---------------------------
% Note under tables
% ---------------------------
\noindent\textbf{Note.}\index{Chain rule}\index{Automatic differentiation}
Each gradient is computed using the chain rule and depends exclusively on
quantities obtained during the forward pass or gradients propagated from
subsequent layers. The three activation derivatives appear explicitly:
sigmoid uses $\hat{y}(1-\hat{y})$, tanh uses $1-a_2^2$, and ReLU uses the gate
$\mathbb{1}[z_1>0]$.

\newpage
% ============================================================
% Small Chain-Rule Example: Gradient Accumulation
% ============================================================

\subsection{Small Chain-Rule Example and Gradient Accumulation}
\index{Chain rule}
\index{Backpropagation!gradient accumulation}
\index{Automatic differentiation!reverse mode}

\paragraph{Computation graph.}
Consider the following scalar computation:
\[
\begin{aligned}
v_3 &= x^2, \\
v_5 &= 2v_3, \\
v_6 &= v_3 + 1, \\
f &= v_5\,v_6 .
\end{aligned}
\]
Here, the intermediate variable $v_3$ feeds into two downstream variables,
$v_5$ and $v_6$.

\paragraph{Forward pass (numerical example).}
Let $x=3$. Then:
\[
\begin{aligned}
v_3 &= 9, \\
v_5 &= 18, \\
v_6 &= 10, \\
f &= 180.
\end{aligned}
\]

\paragraph{Backward pass: chain rule with accumulation.}
To compute the gradient with respect to $v_3$, note that $f$ depends on $v_3$
through two distinct paths. The multivariable chain rule gives:
\[
\boxed{
\frac{\partial f}{\partial v_3}
=
\frac{\partial f}{\partial v_6}
\frac{\partial v_6}{\partial v_3}
+
\frac{\partial f}{\partial v_5}
\frac{\partial v_5}{\partial v_3}
}
\]

\paragraph{Evaluate each term.}
\[
\frac{\partial f}{\partial v_5} = v_6 = 10,
\qquad
\frac{\partial f}{\partial v_6} = v_5 = 18,
\]
\[
\frac{\partial v_5}{\partial v_3} = 2,
\qquad
\frac{\partial v_6}{\partial v_3} = 1.
\]

\paragraph{Gradient accumulation.}
Substituting:
\[
\frac{\partial f}{\partial v_3}
=
(18)(1) + (10)(2)
=
18 + 20
=
\boxed{38}.
\]

\paragraph{Interpretation.}
The summation in the chain rule corresponds directly to gradient accumulation
in reverse-mode automatic differentiation. When a node feeds multiple children,
each child contributes a partial gradient, and the total gradient is obtained
by summing all contributions:
\[
\frac{\partial f}{\partial v_3}
\;\longleftrightarrow\;
\texttt{grad[v3] += contribution from each child}.
\]

\noindent\textbf{Key point.}
The ``$+$'' operator in the chain rule corresponds to the ``\texttt{+=}''
operation in backpropagation implementations: gradients are accumulated
whenever a variable influences the output through multiple paths.





\newpage
% ============================================================
% Unified Section: Matrix-Form Backpropagation, Product Types,
% and Forward-Pass Memory
% Requires: amsmath, amssymb, booktabs, makeidx
% ============================================================

\subsection{Matrix-Form Backpropagation, Product Types, and Forward-Pass Memory}
\index{Backpropagation!matrix form}
\index{Matrix calculus}
\index{Chain rule}
\index{Forward Pass!memory}
\index{Automatic differentiation}

\noindent\textbf{Goal.}
This section presents forward and backward propagation for a three-layer
feedforward neural network using vector and matrix notation.
The formulation explicitly identifies the type of each algebraic operation
(matrix--vector product, Hadamard product, outer product) and clarifies which
intermediate quantities must be stored during the forward pass to enable
backpropagation.
The symbol $\odot$ denotes the \emph{Hadamard (element-wise) product} and is
\emph{not} the Khatri--Rao product.
\index{Hadamard product}
\index{Khatri--Rao product}
\index{Outer product}

% ------------------------------------------------------------
% Notation and dimensions
% ------------------------------------------------------------
\paragraph{Notation and dimensions.}\index{Notation!dimensions}
Let the input be $\mathbf{x}\in\mathbb{R}^{d_0}$ and define $\mathbf{a}_0:=\mathbf{x}$.
For layers $k\in\{1,2,3\}$:
\[
\mathbf{W}_k\in\mathbb{R}^{d_k\times d_{k-1}},\qquad
\mathbf{b}_k\in\mathbb{R}^{d_k},\qquad
\mathbf{z}_k\in\mathbb{R}^{d_k},\qquad
\mathbf{a}_k\in\mathbb{R}^{d_k}.
\]
Each affine transformation uses a \textbf{matrix--vector product}:
\[
\mathbf{z}_k = \mathbf{W}_k\mathbf{a}_{k-1} + \mathbf{b}_k.
\]

% ------------------------------------------------------------
% Network and activations
% ------------------------------------------------------------
\paragraph{Three-layer network with heterogeneous activations.}
\index{Activation Functions}
\index{Activation Functions!ReLU}
\index{Activation Functions!tanh}
\index{Activation Functions!Sigmoid}
The network employs different activation functions at each layer:
\[
\mathbf{a}_1 = \mathrm{ReLU}(\mathbf{z}_1),\qquad
\mathbf{a}_2 = \tanh(\mathbf{z}_2),\qquad
\hat{\mathbf{y}}=\mathbf{a}_3=\sigma(\mathbf{z}_3).
\]
For a single training sample, the loss is
\[
\mathcal{L}=\tfrac12\lVert \hat{\mathbf{y}}-\mathbf{y}\rVert^2.
\index{Loss Functions!MSE}
\]

% ------------------------------------------------------------
% Activation derivatives
% ------------------------------------------------------------
\paragraph{Activation derivatives (element-wise).}
\index{Derivative!activation function}
All activation derivatives are applied element-wise:
\[
\frac{d}{dz}\mathrm{ReLU}(z)=
\begin{cases}
1,& z>0\\
0,& z\le 0
\end{cases},
\qquad
\frac{d}{dz}\tanh(z)=1-\tanh^2(z),
\qquad
\frac{d}{dz}\sigma(z)=\sigma(z)\bigl(1-\sigma(z)\bigr).
\]
For vectors, we use the Hadamard product $\odot$ and element-wise powers such as
$\mathbf{a}^{\odot 2}=\mathbf{a}\odot\mathbf{a}$.

% ------------------------------------------------------------
% Forward propagation
% ------------------------------------------------------------
\paragraph{Forward propagation (matrix form).}
\index{Forward Pass!matrix form}
\begin{align}
\mathbf{z}_1 &= \mathbf{W}_1\mathbf{x} + \mathbf{b}_1,
& \mathbf{a}_1 &= \mathrm{ReLU}(\mathbf{z}_1), \\
\mathbf{z}_2 &= \mathbf{W}_2\mathbf{a}_1 + \mathbf{b}_2,
& \mathbf{a}_2 &= \tanh(\mathbf{z}_2), \\
\mathbf{z}_3 &= \mathbf{W}_3\mathbf{a}_2 + \mathbf{b}_3,
& \hat{\mathbf{y}} &= \sigma(\mathbf{z}_3).
\end{align}

% ------------------------------------------------------------
% Backpropagation
% ------------------------------------------------------------
\paragraph{Backpropagation (matrix form).}
\index{Backpropagation!matrix form}
Define error signals (``deltas''):
\[
\boldsymbol{\delta}_k := \frac{\partial \mathcal{L}}{\partial \mathbf{z}_k}
\in\mathbb{R}^{d_k}.
\]

\noindent\textbf{Layer 3 (sigmoid output).}
\index{Sigmoid!derivative}
\[
\boldsymbol{\delta}_3
=
(\hat{\mathbf{y}}-\mathbf{y})
\;\odot\;
\bigl(\hat{\mathbf{y}}\odot(1-\hat{\mathbf{y}})\bigr),
\]
where $\odot$ is a \textbf{Hadamard (element-wise) product}.

\noindent\textbf{Layer 2 (tanh).}
\index{tanh!derivative}
\[
\boldsymbol{\delta}_2
=
(\mathbf{W}_3^{\top}\boldsymbol{\delta}_3)
\;\odot\;
\bigl(1-\mathbf{a}_2^{\odot 2}\bigr).
\]
Here, $\mathbf{W}_3^{\top}\boldsymbol{\delta}_3$ is a
\textbf{matrix--vector product}.

\noindent\textbf{Layer 1 (ReLU).}
\index{ReLU!derivative}
\[
\boldsymbol{\delta}_1
=
(\mathbf{W}_2^{\top}\boldsymbol{\delta}_2)
\;\odot\;
\mathbb{1}[\mathbf{z}_1>0],
\]
where $\mathbb{1}[\mathbf{z}_1>0]$ is an element-wise binary mask.

% ------------------------------------------------------------
% Parameter gradients
% ------------------------------------------------------------
\paragraph{Parameter gradients.}
\index{Gradient!weights}
\index{Gradient!biases}
For each layer $k\in\{1,2,3\}$:
\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}_k}
=
\boldsymbol{\delta}_k\,\mathbf{a}_{k-1}^{\top},
\qquad
\frac{\partial \mathcal{L}}{\partial \mathbf{b}_k}
=
\boldsymbol{\delta}_k.
\]
The product $\boldsymbol{\delta}_k\,\mathbf{a}_{k-1}^{\top}$ is an
\textbf{outer product}, yielding a matrix in
$\mathbb{R}^{d_k\times d_{k-1}}$.
It is \textbf{not} a Hadamard, Kronecker, or Khatri--Rao product.
\index{Kronecker product}
\index{Khatri--Rao product}

% ------------------------------------------------------------
% Forward-pass memory
% ------------------------------------------------------------
\paragraph{Forward-pass memory (saved tensors).}
\index{Forward Pass!memory}
\index{Backpropagation!saved tensors}
During the forward pass, certain intermediate quantities must be retained in
memory because they are required to compute gradients during backpropagation.
Automatic differentiation frameworks store these values as part of the
computational graph.

\begin{table}[htbp]
\centering
\caption{Quantities stored during forward propagation and their role in backpropagation}
\label{tab:forward_memory}
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{l l l}
\toprule
\textbf{Stored quantity} & \textbf{Saved during forward pass} & \textbf{Used in backpropagation} \\
\midrule
$\mathbf{x}=\mathbf{a}_0$ 
& Input layer 
& $\boldsymbol{\delta}_1\,\mathbf{x}^{\top}$ \\

$\mathbf{z}_1$ 
& Pre-activation (layer 1) 
& ReLU mask $\mathbb{1}[\mathbf{z}_1>0]$ \\

$\mathbf{a}_1$ 
& Activation (layer 1) 
& $\boldsymbol{\delta}_2\,\mathbf{a}_1^{\top}$ \\

$\mathbf{z}_2$ 
& Pre-activation (layer 2) 
& $1-\mathbf{a}_2^{\odot 2}$ \\

$\mathbf{a}_2$ 
& Activation (layer 2) 
& $\boldsymbol{\delta}_3\,\mathbf{a}_2^{\top}$ \\

$\mathbf{z}_3$ 
& Pre-activation (output layer) 
& $\hat{\mathbf{y}}\odot(1-\hat{\mathbf{y}})$ \\

$\hat{\mathbf{y}}$ 
& Network output 
& $\hat{\mathbf{y}}-\mathbf{y}$ \\
\bottomrule
\end{tabular}
\end{table}

% ------------------------------------------------------------
% Product-type summary
% ------------------------------------------------------------
\paragraph{Summary of product types.}
\index{Product types}
\begin{table}[htbp]
\centering
\caption{Product types appearing in forward and backward propagation}
\label{tab:product_types_backprop}
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{l l l}
\toprule
\textbf{Symbol / form} & \textbf{Name} & \textbf{Where it appears} \\
\midrule
$\mathbf{W}\mathbf{a}$, $\mathbf{W}^{\top}\boldsymbol{\delta}$ 
& Matrix--vector product 
& Forward affine map; backward error propagation \\

$\odot$ 
& Hadamard (element-wise) product 
& Activation derivatives and masks \\

$\boldsymbol{\delta}\,\mathbf{a}^{\top}$ 
& Outer product 
& Weight gradients \\

\bottomrule
\end{tabular}
\end{table}

\noindent\textbf{Remark.}
Standard backpropagation for feedforward neural networks requires only
matrix--vector products, Hadamard products, and outer products.
Higher-order tensor products such as the Kronecker or Khatri--Rao products do
not appear.



















% ============================================================
% Hessian--Vector Products and the Role of Nabla
% ============================================================
\newpage
\subsection{Hessian--Vector Products and the Role of the Nabla Operator}
\index{Hessian-vector product}
\index{Nabla operator}
\index{Second-order derivatives}

Let $f(\mathbf{w})$ denote a scalar-valued objective function, where
$\mathbf{w}\in\mathbb{R}^{W}$ is the vector of model parameters
(e.g., weights and biases of a neural network).
The \emph{gradient} of $f$ with respect to $\mathbf{w}$ is denoted by
$\nabla f(\mathbf{w})\in\mathbb{R}^{W}$, and the \emph{Hessian} is the
matrix of second-order partial derivatives
\[
\mathbf{H} = \nabla^{2} f(\mathbf{w}) \in \mathbb{R}^{W\times W},
\qquad
H_{ij} = \frac{\partial^{2} f}{\partial w_i\,\partial w_j}.
\]

Forming the full Hessian matrix explicitly is typically computationally
prohibitive for large-scale models. However, many second-order methods
require only the \emph{Hessian--vector product}, defined as
\[
\mathbf{H}\mathbf{u} = \nabla^{2} f(\mathbf{w})\,\mathbf{u},
\]
where $\mathbf{u}\in\mathbb{R}^{W}$ is an arbitrary vector in parameter
space. The result $\mathbf{H}\mathbf{u}$ is itself a vector in
$\mathbb{R}^{W}$ and represents the action of the curvature of $f$ along
the direction $\mathbf{u}$.

A key identity enables efficient computation of this product without
explicitly constructing the Hessian:
\[
\mathbf{H}\mathbf{u}
=
\nabla_{\mathbf{w}}\!\left( \nabla f(\mathbf{w})^{\top}\mathbf{u} \right).
\]
This identity can be interpreted as follows:
\begin{enumerate}
  \item Compute the gradient $\nabla f(\mathbf{w})$ with respect to the
        parameters.
  \item Form a scalar by taking its inner product with $\mathbf{u}$.
  \item Differentiate this scalar once more with respect to
        $\mathbf{w}$.
\end{enumerate}

The nabla operator $\nabla_{\mathbf{w}}$ therefore always denotes partial
derivatives with respect to the parameter vector $\mathbf{w}$. In this
context, the first application of $\nabla_{\mathbf{w}}$ produces the
gradient, while the second application produces second-order curvature
information. Automatic differentiation systems can evaluate the above
expression efficiently, yielding Hessian--vector products in time
comparable to a small constant multiple of a gradient computation.

This capability is fundamental to Hessian-free optimization, conjugate
gradient methods, and other second-orderâ€“inspired algorithms used in
large-scale machine learning.


% ============================================================
% Worked Examples: Understanding the Hessian--Vector Product
% ============================================================

\subsubsection{Worked Examples and Interpretation}
\index{Hessian-vector product!example}
\index{Second-order derivatives!example}

To clarify the meaning of the Hessian--vector product
$\mathbf{H}\mathbf{u}=\nabla^2 f(\mathbf{w})\,\mathbf{u}$ and the role of
the nabla operator, we present concrete examples.

\paragraph{Example 1: Simple quadratic function.}
Consider the scalar function
\[
f(w_1,w_2)=w_1^2+3w_1w_2+2w_2^2.
\]

The gradient of $f$ with respect to
$\mathbf{w}=(w_1,w_2)^\top$ is
\[
\nabla f(\mathbf{w})
=
\begin{pmatrix}
2w_1+3w_2\\
3w_1+4w_2
\end{pmatrix}.
\]

Applying the nabla operator once more yields the Hessian
\[
\nabla^2 f(\mathbf{w})
=
\begin{pmatrix}
2 & 3\\
3 & 4
\end{pmatrix}.
\]

Let $\mathbf{u}=(1,-1)^\top$ be an arbitrary direction in parameter
space. The Hessian--vector product is then
\[
\mathbf{H}\mathbf{u}
=
\begin{pmatrix}
2 & 3\\
3 & 4
\end{pmatrix}
\begin{pmatrix}
1\\
-1
\end{pmatrix}
=
\begin{pmatrix}
-1\\
-1
\end{pmatrix}.
\]

This result represents how the gradient of $f$ changes when the
parameters are perturbed in the direction $\mathbf{u}$.

\paragraph{Verification via the nabla identity.}
Using the identity
\[
\mathbf{H}\mathbf{u}
=
\nabla_{\mathbf{w}}\!\left(\nabla f(\mathbf{w})^\top\mathbf{u}\right),
\]
we first compute the scalar
\[
\nabla f(\mathbf{w})^\top\mathbf{u}
=
(2w_1+3w_2,\,3w_1+4w_2)
\begin{pmatrix}
1\\
-1
\end{pmatrix}
=
-(w_1+w_2).
\]
Differentiating this scalar with respect to $\mathbf{w}$ gives
\[
\nabla_{\mathbf{w}}(-(w_1+w_2))
=
\begin{pmatrix}
-1\\
-1
\end{pmatrix},
\]
which matches the Hessian--vector product obtained above.

\paragraph{Example 2: Interpretation in neural networks.}
In neural networks, the parameter vector
$\mathbf{w}$ collects all weights and biases, and
$f(\mathbf{w})=\mathcal{L}$ denotes the loss.
The gradient $\nabla f(\mathbf{w})$ is computed via backpropagation.
For any chosen direction $\mathbf{u}$ in parameter space,
the Hessian--vector product $\mathbf{H}\mathbf{u}$ measures how this
gradient changes along $\mathbf{u}$.

Crucially, the identity
\[
\mathbf{H}\mathbf{u}
=
\nabla_{\mathbf{w}}\!\left(\nabla f(\mathbf{w})^\top\mathbf{u}\right)
\]
allows the Hessian--vector product to be computed using automatic
differentiation without explicitly forming the Hessian matrix.
This enables curvature-aware optimization methods to scale to large
models.

%======================================

% ------------------------------------------------------------
% Practical Example: Hessian--Vector Product via Forward and Backpropagation
% ------------------------------------------------------------

\subsubsection{Practical Example: Hessian--Vector Product via Forward and Backpropagation}
\index{Hessian-vector product!backpropagation example}

We now present a practical example demonstrating how a Hessian--vector
product is computed using standard forward and backward propagation in a
neural network, without explicitly forming the Hessian matrix.

\paragraph{Model and loss.}
Consider a single-neuron model with one input:
\[
z = wx, \qquad \hat{y} = z, \qquad \mathcal{L} = \tfrac12(\hat{y}-y)^2,
\]
where $w$ is the trainable parameter, $x$ is the input, and $y$ is the
target value. The parameter vector is $\mathbf{w}=(w)$.

\paragraph{Forward pass.}
The forward pass computes:
\[
z = wx, \qquad
\hat{y} = wx, \qquad
\mathcal{L} = \tfrac12(wx - y)^2.
\]

\paragraph{First backward pass (gradient computation).}
Backpropagation computes the gradient of the loss with respect to $w$:
\[
\nabla f(w) = \frac{\partial \mathcal{L}}{\partial w}
= (wx - y)x.
\]

This is the standard gradient used in first-order optimization methods
such as gradient descent.

\paragraph{Forming the scalar for the Hessian--vector product.}
Let $u$ be a chosen direction in parameter space (a scalar in this
example). We form the scalar
\[
s(w) = \nabla f(w)^\top u = (wx - y)x\,u.
\]

\paragraph{Second backward pass (Hessian--vector product).}
We now differentiate $s(w)$ with respect to $w$:
\[
\nabla_w s(w)
= \frac{d}{dw}\bigl((wx - y)x\,u\bigr)
= x^2 u.
\]

Since the second derivative of the loss is
\[
\nabla^2 f(w) = x^2,
\]
the above result is exactly the Hessian--vector product:
\[
\nabla_w s(w) = \nabla^2 f(w)\,u.
\]

\paragraph{Interpretation in terms of backpropagation.}
This computation uses:
\begin{itemize}
  \item one forward pass to compute $\mathcal{L}$,
  \item one backward pass to compute $\nabla f(w)$,
  \item a second backward pass to compute $\nabla^2 f(w)\,u$.
\end{itemize}

At no point is the Hessian matrix explicitly constructed. Instead, the
Hessian--vector product is obtained by reusing the standard
backpropagation machinery on a scalar quantity. This is precisely how
modern deep learning frameworks compute second-order information in
practice.

\paragraph{Extension to deep networks.}
In multilayer neural networks, the same procedure applies with
$\mathbf{w}$ representing all weights and biases. The two backward
passes correspond to backpropagation through the original loss and
backpropagation through a scalar derived from the gradient. This enables
Hessian-free optimization and curvature-aware methods to scale to
large models.












%======================================
% ------------------------------------------------------------
% Practical Example: Hessian--Vector Product in a Three-Layer Network
% (with Weights and Biases)
% ------------------------------------------------------------

\subsubsection{Practical Example: Hessian--Vector Product in a Three-Layer Network}
\index{Hessian-vector product!three-layer network}
\index{Backpropagation!second-order example}

We now present a practical example demonstrating how Hessian--vector
products are computed in a three-layer neural network using standard
forward and backward propagation, including both weights and biases.

\paragraph{Model and loss.}
Consider a scalar three-layer network
\[
z_1 = w_1 x + b_1, \qquad a_1 = z_1,
\]
\[
z_2 = w_2 a_1 + b_2, \qquad a_2 = z_2,
\]
\[
z_3 = w_3 a_2 + b_3, \qquad \hat{y} = z_3,
\qquad
\mathcal{L} = \tfrac12(\hat{y}-y)^2,
\]
where $x$ is the input, $y$ is the target, and the parameter vector is
\[
\boldsymbol{\theta} = (w_1,b_1,w_2,b_2,w_3,b_3)^\top.
\]
Identity activations are used to isolate the propagation mechanism.

---

\paragraph{Forward pass.}
The forward pass computes
\[
\hat{y}
=
w_3\bigl(w_2(w_1x+b_1)+b_2\bigr)+b_3,
\]
\[
\mathcal{L}
=
\tfrac12\bigl(\hat{y}-y\bigr)^2.
\]

---

\paragraph{First backward pass (gradient computation).}
Backpropagation yields the gradient of the loss with respect to all
parameters:
\[
\nabla f(\boldsymbol{\theta})
=
\begin{pmatrix}
\frac{\partial \mathcal{L}}{\partial w_1}\\
\frac{\partial \mathcal{L}}{\partial b_1}\\
\frac{\partial \mathcal{L}}{\partial w_2}\\
\frac{\partial \mathcal{L}}{\partial b_2}\\
\frac{\partial \mathcal{L}}{\partial w_3}\\
\frac{\partial \mathcal{L}}{\partial b_3}
\end{pmatrix}
=
(\hat{y}-y)
\begin{pmatrix}
w_3 w_2 x\\
w_3 w_2\\
w_3 a_1\\
w_3\\
a_2\\
1
\end{pmatrix}.
\]

This is the standard gradient computed during ordinary backpropagation.

---

\paragraph{Concrete three-layer network usage.}
To make the role of the Hessian--vector product explicit in a deep
learning setting, consider the three-layer scalar network
\[
z_1 = w_1 x + b_1, \qquad a_1 = z_1,
\]
\[
z_2 = w_2 a_1 + b_2, \qquad a_2 = z_2,
\]
\[
z_3 = w_3 a_2 + b_3, \qquad \hat{y} = z_3,
\qquad
\mathcal{L} = \tfrac12(\hat{y}-y)^2,
\]
with parameter vector
\[
\boldsymbol{\theta}
=
(w_1,b_1,w_2,b_2,w_3,b_3)^\top.
\]

\paragraph{First backward pass (gradient).}
Applying standard backpropagation yields
\[
\nabla f(\boldsymbol{\theta})
=
(\hat{y}-y)
\begin{pmatrix}
w_3 w_2 x\\
w_3 w_2\\
w_3 a_1\\
w_3\\
a_2\\
1
\end{pmatrix}.
\]
This is the gradient used by first-order optimizers such as SGD or Adam.

\paragraph{Scalar construction for curvature probing.}
Given a direction vector
\[
\mathbf{u}
=
(u_{w_1},u_{b_1},u_{w_2},u_{b_2},u_{w_3},u_{b_3})^\top,
\]
we form the scalar
\[
s(\boldsymbol{\theta})
=
\nabla f(\boldsymbol{\theta})^\top \mathbf{u}
=
(\hat{y}-y)
\Bigl(
u_{w_1} w_3 w_2 x
+
u_{b_1} w_3 w_2
+
u_{w_2} w_3 a_1
+
u_{b_2} w_3
+
u_{w_3} a_2
+
u_{b_3}
\Bigr).
\]
This scalar measures how the gradient of the loss changes when the
parameters are perturbed along the direction $\mathbf{u}$.

\paragraph{Second backward pass (Hessian--vector product).}
Differentiating $s(\boldsymbol{\theta})$ with respect to the parameters
gives
\[
\nabla_{\boldsymbol{\theta}} s(\boldsymbol{\theta})
=
\nabla^2 f(\boldsymbol{\theta})\,\mathbf{u}
=
\mathbf{H}\mathbf{u}.
\]
Each component of this vector captures the second-order sensitivity of
the loss with respect to the corresponding parameter along the chosen
direction $\mathbf{u}$.

\paragraph{How this fits into forward and backward propagation.}
Operationally, the computation can be summarized as:
\begin{itemize}
  \item \textbf{Forward pass:} compute activations
        $(a_1,a_2,\hat{y})$ and the loss $\mathcal{L}$.
  \item \textbf{First backward pass:} compute the gradient
        $\nabla f(\boldsymbol{\theta})$ using standard backpropagation.
  \item \textbf{Scalar projection:} form
        $s(\boldsymbol{\theta})=\nabla f(\boldsymbol{\theta})^\top\mathbf{u}$.
  \item \textbf{Second backward pass:} backpropagate through
        $s(\boldsymbol{\theta})$ to obtain $\mathbf{H}\mathbf{u}$.
\end{itemize}

\paragraph{Interpretation in deep learning terms.}
\begin{itemize}
  \item The first backward pass computes \emph{slopes} of the loss.
  \item The second backward pass computes how those slopes \emph{change}
        along a specific direction in parameter space.
  \item The vector $\mathbf{u}$ selects which direction of curvature is
        being examined.
  \item No Hessian matrix is formed; curvature is accessed implicitly
        through backpropagation.
\end{itemize}

\paragraph{Practical meaning.}
In large neural networks, the same mechanism applies when
$\boldsymbol{\theta}$ contains millions of parameters. Hessian--vector
products computed in this manner are used inside iterative solvers such
as conjugate gradient to obtain curvature-aware update directions,
analyze sharpness of minima, and enable Hessian-free optimization---all
while preserving the standard forward and backward propagation
structure used in deep learning frameworks.


%======================================

\paragraph{Role of the second backward pass: clarification.}
The purpose of the second backward pass depends on how the
Hessian--vector product is used. The same mathematical operation serves
two distinct roles in deep learning.

\begin{itemize}
  \item \textbf{Standard first-order training (SGD, Adam).}
  \begin{itemize}
    \item Training consists of one forward pass and one backward pass
          per batch.
    \item The backward pass computes the gradient
          $\nabla f(\boldsymbol{\theta})$.
    \item Parameter updates depend only on first-order information.
    \item No Hessian--vector products are required.
    \item In this setting, a second backward pass is unnecessary.
  \end{itemize}

  \item \textbf{Second backward pass used for analysis or diagnostics.}
  \begin{itemize}
    \item The second backward pass computes
          $\mathbf{H}\mathbf{u}=\nabla^2 f(\boldsymbol{\theta})\,\mathbf{u}$.
    \item The direction $\mathbf{u}$ is chosen externally (e.g., the
          gradient direction or a random vector).
    \item Quantities such as $\mathbf{u}^\top\mathbf{H}\mathbf{u}$ are
          used to assess curvature, sharpness, or stability.
    \item Training updates are not modified.
    \item The second pass is used only to monitor or analyze convergence
          behavior.
  \end{itemize}

  \item \textbf{Second backward pass used for optimization.}
  \begin{itemize}
    \item In second-order or Hessian-free methods, the second backward
          pass directly influences parameter updates.
    \item Optimization algorithms such as conjugate gradient solve
          linear systems of the form
          \[
          \mathbf{H}\mathbf{p} = -\nabla f(\boldsymbol{\theta})
          \]
          using repeated Hessian--vector products.
    \item Each Hessian--vector product requires a second backward pass.
    \item The resulting direction $\mathbf{p}$ determines how the model
          parameters are updated.
    \item In this case, the second backward pass is part of the training
          algorithm itself.
  \end{itemize}

  \item \textbf{Why the second backward pass is not always used.}
  \begin{itemize}
    \item Hessian--vector products are computationally more expensive
          than gradients.
    \item Stochastic noise in mini-batch training complicates curvature
          estimation.
    \item First-order methods often provide sufficient performance at
          lower computational cost.
    \item As a result, curvature-based methods are applied selectively.
  \end{itemize}
\end{itemize}

\paragraph{Summary.}
\begin{itemize}
  \item One backward pass computes the slope of the loss.
  \item A second backward pass computes how that slope changes.
  \item The second backward pass may be used for analysis only or as an
        integral part of optimization.
  \item Whether it affects training depends on how the Hessian--vector
        product is used.
\end{itemize}



\paragraph{When Hessian--vector products are used in practice.}
The use of Hessian--vector products depends on the choice of optimization
strategy.

\begin{itemize}
  \item \textbf{Typical deep learning training.}
  \begin{itemize}
    \item Most deep learning models are trained using first-order
          optimizers such as SGD, Adam, or RMSProp.
    \item These methods require only one forward pass and one backward
          pass per batch.
    \item Curvature information is not explicitly computed.
    \item Hessian--vector products are therefore \emph{not} used during
          standard training.
  \end{itemize}

  \item \textbf{Second-order and curvature-aware optimization.}
  \begin{itemize}
    \item When second-order information is desired, Hessian--vector
          products become essential.
    \item Methods such as Hessian-free optimization, Newton--CG, and
          other curvature-aware algorithms rely on repeated evaluations
          of $\mathbf{H}\mathbf{u}$.
    \item The second backward pass is used to compute how gradients
          change along selected directions.
    \item This additional information is used to construct improved
          update directions for the model parameters.
  \end{itemize}

  \item \textbf{Practical implication.}
  \begin{itemize}
    \item In most applications, first-order methods are preferred due to
          their simplicity and computational efficiency.
    \item Hessian--vector products are used selectively, either for
          advanced optimization or for analyzing the geometry of the
          loss landscape.
    \item When a second-order optimizer is employed, the additional
          backward pass becomes a necessary and integral part of the
          training procedure.
  \end{itemize}
\end{itemize}

\paragraph{Key takeaway.}
\begin{itemize}
  \item Standard training does not require Hessian--vector products.
  \item Hessian--vector products become useful and necessary when
        second-order optimization methods are employed.
\end{itemize}



\paragraph{Role and selection of the direction vector in second-order optimization.}
Second-order optimization methods do not operate on the Hessian matrix
$\mathbf{H}$ directly. Instead, they rely on Hessian--vector products of
the form $\mathbf{H}\mathbf{u}$, which require the specification of a
direction vector $\mathbf{u}$ in parameter space.

\begin{itemize}
  \item \textbf{Why a direction vector is required.}
  \begin{itemize}
    \item The Hessian $\mathbf{H}\in\mathbb{R}^{W\times W}$ is too large to
          form or store explicitly in deep learning models.
    \item Second-order methods therefore interact with the Hessian only
          through its action on vectors.
    \item The vector $\mathbf{u}$ specifies the direction along which
          curvature information is extracted.
  \end{itemize}

  \item \textbf{Direction vectors are not arbitrary parameters.}
  \begin{itemize}
    \item The vector $\mathbf{u}$ is not learned and is not part of the
          model.
    \item It is generated algorithmically by the optimization method.
    \item Its purpose is to probe curvature or construct update
          directions.
  \end{itemize}

  \item \textbf{Typical choices of the direction vector $\mathbf{u}$.}
  \begin{itemize}
    \item \emph{Gradient direction:}
          \[
          \mathbf{u} = \nabla f(\boldsymbol{\theta}),
          \]
          used to measure curvature along the descent direction and to
          assess sharpness or stability.
    \item \emph{Conjugate gradient search directions:}
          \[
          \mathbf{u} = \mathbf{p}_k,
          \]
          where $\mathbf{p}_k$ is the current search direction generated
          by the conjugate gradient algorithm when solving
          \[
          \mathbf{H}\mathbf{p} = -\nabla f(\boldsymbol{\theta}).
          \]
    \item \emph{Random probe directions:}
          used to estimate average curvature or spectral properties of
          the Hessian.
  \end{itemize}

  \item \textbf{How the direction vector affects optimization.}
  \begin{itemize}
    \item The quality of the second-order update depends on the choice of
          $\mathbf{u}$.
    \item In Newton--CG and Hessian-free optimization, the direction
          vectors are chosen adaptively to approximate the Newton step.
    \item Poorly chosen directions yield poor curvature estimates and
          ineffective updates.
  \end{itemize}

  \item \textbf{Connection to the second backward pass.}
  \begin{itemize}
    \item Each chosen direction $\mathbf{u}$ triggers a Hessian--vector
          product $\mathbf{H}\mathbf{u}$.
    \item Computing $\mathbf{H}\mathbf{u}$ requires a second backward
          pass through the scalar
          $\nabla f(\boldsymbol{\theta})^\top \mathbf{u}$.
    \item Multiple directions may be evaluated during a single
          optimization step.
  \end{itemize}
\end{itemize}

\paragraph{Key takeaway.}
\begin{itemize}
  \item Second-order optimization requires both gradient information and
        carefully chosen direction vectors.
  \item Hessian--vector products provide curvature information only along
        those directions.
  \item The effectiveness of second-order methods depends critically on
        how the direction vectors are defined and updated.
\end{itemize}


\paragraph{Where Newton--CG is used in practice.}
Newton--Conjugate Gradient (Newton--CG) methods appear in deep learning
and scientific computing primarily in settings where curvature
information is valuable and the optimization problem is sufficiently
structured.

\begin{itemize}
  \item \textbf{Hessian-free neural network training.}
  \begin{itemize}
    \item Newton--CG is the core optimization engine in Hessian-free
          training methods.
    \item The Newton update direction $\mathbf{p}$ is obtained by
          approximately solving
          \[
          \mathbf{H}\mathbf{p} = -\nabla f(\boldsymbol{\theta})
          \]
          using conjugate gradient iterations.
    \item Each CG iteration requires Hessian--vector products
          $\mathbf{H}\mathbf{u}$ rather than the full Hessian.
    \item \textbf{Example applications:}
      \begin{itemize}
        \item training deep autoencoders for dimensionality reduction,
        \item optimization of recurrent neural networks (RNNs) and LSTMs
              suffering from vanishing or exploding gradients,
        \item training very deep feedforward networks where SGD converges
              slowly due to ill-conditioned curvature.
      \end{itemize}
  \end{itemize}

  \item \textbf{Large-scale scientific and engineering optimization.}
  \begin{itemize}
    \item Newton--CG is widely used in physics-based optimization
          problems involving smooth, differentiable objective functions.
    \item \textbf{Example applications:}
      \begin{itemize}
        \item inverse problems in imaging (e.g., tomography and
              deconvolution),
        \item parameter estimation in PDE-constrained optimization,
        \item aerodynamic shape optimization and structural mechanics,
        \item fluid dynamics and climate modeling simulations.
      \end{itemize}
    \item In these problems, curvature information dramatically reduces
          the number of required optimization iterations.
  \end{itemize}

  \item \textbf{Classical machine learning models.}
  \begin{itemize}
    \item Newton--CG is commonly used when the loss function is convex or
          nearly convex.
    \item \textbf{Example applications:}
      \begin{itemize}
        \item logistic regression with large feature sets,
        \item generalized linear models (GLMs),
        \item kernel-based learning methods with smooth regularization.
      \end{itemize}
    \item In these settings, Newton--CG often converges in far fewer
          iterations than gradient descent.
  \end{itemize}

  \item \textbf{Gauss--Newton and Fisher-based methods in deep learning.}
  \begin{itemize}
    \item Newton--CG is frequently applied to Gauss--Newton or Fisher
          information matrix approximations of the Hessian.
    \item These approximations are positive semi-definite, making CG
          numerically stable.
    \item \textbf{Example applications:}
      \begin{itemize}
        \item training neural networks with squared-error or likelihood-
              based losses,
        \item natural gradient methods and trust-region approaches,
        \item curvature-aware fine-tuning of pretrained deep networks.
      \end{itemize}
  \end{itemize}

  \item \textbf{Why Newton--CG is not the default in deep learning.}
  \begin{itemize}
    \item Each Newton step requires multiple Hessian--vector products,
          increasing computational cost.
    \item Mini-batch stochasticity introduces noise into curvature
          estimates.
    \item First-order methods (SGD, Adam) often achieve competitive
          performance with significantly lower overhead.
    \item As a result, Newton--CG is used selectively rather than as a
          default optimizer.
  \end{itemize}
\end{itemize}

\paragraph{Summary.}
\begin{itemize}
  \item Newton--CG is most useful when curvature information significantly
        improves convergence.
  \item It is widely used in Hessian-free neural network training and
        large-scale scientific and engineering optimization.
  \item In deep learning, it is typically applied with Gauss--Newton or
        Fisher approximations rather than the exact Hessian.
  \item Its limited use is due to computational cost and stochastic
        noise, not lack of effectiveness.
\end{itemize}




%======================================



\begin{table}[htbp]
\centering
\caption{Comparison of PyTorch behavior during training and inference}
\label{tab:pytorch_training_vs_inference}
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{p{4.2cm} p{5.4cm} p{5.4cm}}
\toprule
\textbf{Aspect} & \textbf{Training mode (autograd enabled)} & \textbf{Inference mode (autograd disabled)} \\
\midrule
Forward pass behavior
& Records the full computational graph of tensor operations
& Performs forward computation only \\

Intermediate activations
& Stored in memory for backpropagation
& Not stored \\

Gradient tracking
& Tracks tensors with \texttt{requires\_grad=True}
& No gradient tracking \\

Operation metadata
& Saved to enable chain rule application during backward pass
& Not saved \\

Backward pass
& Required to compute gradients of the loss
& Not performed \\

Gradient storage
& Gradients accumulated in \texttt{.grad} fields of parameters
& No gradient buffers allocated \\

Higher-order derivatives
& Supported when explicitly requested (e.g., Hessian--vector products)
& Not available \\

Memory usage
& Higher due to graph and activation storage
& Significantly reduced \\

Runtime performance
& Slower due to bookkeeping overhead
& Faster and more efficient \\

Typical use case
& Model training and optimization
& Evaluation, validation, and deployment \\

How mode is enabled
& Default behavior when gradients are required
& Using \texttt{torch.no\_grad()} and \texttt{model.eval()} \\

\bottomrule
\end{tabular}
\end{table}




\noindent\textbf{Key takeaway.}
During training, PyTorch retains intermediate computations and constructs
a computational graph to enable automatic differentiation. During
inference, this bookkeeping is disabled to reduce memory usage and
improve execution speed, while preserving the same forward computation.




\paragraph{Clarification: meaning of the computational graph.}
In this context, the term \emph{graph} refers to PyTorchâ€™s dynamic
computational graph constructed during the forward pass when automatic
differentiation is enabled. This graph is a directed acyclic graph whose
nodes correspond to tensor operations and whose edges represent data
dependencies between tensors.

Each node in the graph stores the local derivative information required
to apply the chain rule during backpropagation, along with references to
its parent tensors. During the backward pass, PyTorch traverses this
graph in reverse to compute gradients of the loss with respect to model
parameters.

During inference, the computational graph is not constructed. Tensor
operations are executed without storing dependency information or
intermediate activations, which reduces memory usage and improves
runtime performance.


%======================================
%======================================
%==============End==================


\printindex
\end{document}
