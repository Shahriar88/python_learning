{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e6481-134f-49bc-81cb-897ea7fd0367",
   "metadata": {
    "id": "d52e6481-134f-49bc-81cb-897ea7fd0367"
   },
   "outputs": [],
   "source": [
    "# RuntimeError: use_libuv was requested but PyTorch was build without libuv support\n",
    "# https://docs.libuv.org/en/v1.x/\n",
    "# conda install -c conda-forge libuv -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899a34a-9e37-41fa-9db5-5a2b978069d9",
   "metadata": {
    "id": "5899a34a-9e37-41fa-9db5-5a2b978069d9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1bf99-5ec1-4e99-a13c-3293808aceb3",
   "metadata": {
    "id": "90c1bf99-5ec1-4e99-a13c-3293808aceb3"
   },
   "outputs": [],
   "source": [
    "# Need to test\n",
    "# pip install --no-cache-dir torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "# pip install --force-reinstall --no-cache-dir torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "#torchrun --standalone --nproc_per_node=1 Mask_RCNN_CUDA_DDP_v1.py --run-mode ddp --batch-size 4 --num-workers 4 --train-epochs 5 --train-ckpt-folder train_v0\n",
    "\n",
    "#set MASTER_ADDR=127.0.0.1\n",
    "#set MASTER_PORT=29501\n",
    "#torchrun --nproc_per_node=1 Mask_RCNN_CUDA_DDP_v1.py --run-mode ddp --batch-size 4 --num-workers 4 --train-epochs 5 --train-ckpt-folder train_v0\n",
    "\n",
    "#pip install --force-reinstall --no-cache-dir \"numpy<2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffbb44-e085-458a-9b4d-34524febf4a7",
   "metadata": {
    "id": "8cffbb44-e085-458a-9b4d-34524febf4a7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e9a7a-99bd-4310-be89-0e5ddc30701b",
   "metadata": {
    "id": "0e8e9a7a-99bd-4310-be89-0e5ddc30701b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff23ae5-7f3a-41f2-a9ca-4322f4629585",
   "metadata": {
    "id": "3ff23ae5-7f3a-41f2-a9ca-4322f4629585"
   },
   "source": [
    "## |||||||||||||||||||||||||||||||||||||||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fca00a-b759-4a03-9817-7e2db9419dd0",
   "metadata": {
    "id": "40fca00a-b759-4a03-9817-7e2db9419dd0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b149f2d0-2596-456d-8b22-09f9974a6f54",
   "metadata": {
    "id": "b149f2d0-2596-456d-8b22-09f9974a6f54"
   },
   "source": [
    "# Test GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a11edd6b-f069-4b84-acd1-9226ca3d97d8",
   "metadata": {
    "id": "a11edd6b-f069-4b84-acd1-9226ca3d97d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocating ~10 MB on GPU...\n",
      "Allocation successful.\n",
      "Check nvidia-smi now. Holding memory for 30 seconds...\n",
      "VRAM released.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# --- sanity check ---\n",
    "assert torch.cuda.is_available(), \"CUDA is not available\"\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16   # 2 bytes per element\n",
    "\n",
    "# Target allocation: ~300 MB\n",
    "target_mb = 10\n",
    "bytes_per_elem = torch.tensor([], dtype=dtype).element_size()\n",
    "num_elements = (target_mb * 1024 * 1024) // bytes_per_elem\n",
    "\n",
    "print(f\"Allocating ~{target_mb} MB on GPU...\")\n",
    "\n",
    "x = torch.empty(num_elements, device=device, dtype=dtype)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(\"Allocation successful.\")\n",
    "print(\"Check nvidia-smi now. Holding memory for 30 seconds...\")\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "del x\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "print(\"VRAM released.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gbIkvY3Uymit",
   "metadata": {
    "id": "gbIkvY3Uymit"
   },
   "source": [
    "## FP16 raw CUDA memory allocation test (no cuDNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0822f6c6-9d83-45bf-ae57-659d3abc36a5",
   "metadata": {
    "id": "0822f6c6-9d83-45bf-ae57-659d3abc36a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocating ~200 MB on GPU...\n",
      "Allocation successful.\n",
      "Check nvidia-smi now. Holding memory for 3 seconds...\n",
      "VRAM released.\n",
      "Allocating ~400 MB on GPU...\n",
      "Allocation successful.\n",
      "Check nvidia-smi now. Holding memory for 3 seconds...\n",
      "VRAM released.\n",
      "Allocating ~600 MB on GPU...\n",
      "Allocation successful.\n",
      "Check nvidia-smi now. Holding memory for 3 seconds...\n",
      "VRAM released.\n",
      "Allocating ~800 MB on GPU...\n",
      "Allocation successful.\n",
      "Check nvidia-smi now. Holding memory for 3 seconds...\n",
      "VRAM released.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# --- sanity check ---\n",
    "assert torch.cuda.is_available(), \"CUDA is not available\"\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16   # 2 bytes per element\n",
    "\n",
    "# Target allocation: ~300 MB\n",
    "\n",
    "for target_mb in range(200,1000,200):\n",
    "    bytes_per_elem = torch.tensor([], dtype=dtype).element_size()\n",
    "    num_elements = (target_mb * 1024 * 1024) // bytes_per_elem\n",
    "\n",
    "    print(f\"Allocating ~{target_mb} MB on GPU...\")\n",
    "\n",
    "    x = torch.empty(num_elements, device=device, dtype=dtype)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"Allocation successful.\")\n",
    "    print(\"Check nvidia-smi now. Holding memory for 3 seconds...\")\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    del x\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"VRAM released.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ZIVw8_fyTx2",
   "metadata": {
    "id": "6ZIVw8_fyTx2"
   },
   "source": [
    "## cuDNN benchmark enabled: FP16 convolution memory stress test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af582f9-779b-49b1-a892-78732b3ebe12",
   "metadata": {
    "id": "5af582f9-779b-49b1-a892-78732b3ebe12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla K80\n",
      "Starting conv memory test...\n",
      "\n",
      "Allocating ~500 MB with tensor [8,64,704,704]\n",
      "  Conv OK, holding memory for 3s...\n",
      "  Released.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "# --- sanity check ---\n",
    "assert torch.cuda.is_available(), \"CUDA not available\"\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "torch.backends.cudnn.benchmark = True  # allow workspace growth\n",
    "\n",
    "# Fixed conv layer (small weights, large activations)\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=64,\n",
    "    out_channels=64,\n",
    "    kernel_size=3,\n",
    "    padding=1,\n",
    "    bias=False\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "bytes_per = torch.tensor([], dtype=dtype).element_size()\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"Starting conv memory test...\\n\")\n",
    "\n",
    "for target_mb in range(500, 1000, 500):\n",
    "    # We want roughly target_mb used by activations\n",
    "    # activation_bytes â‰ˆ N * C * H * W * bytes_per\n",
    "    target_bytes = target_mb * 1024 * 1024\n",
    "\n",
    "    C = 64\n",
    "    N = 8  # batch size\n",
    "    elems_per_image = target_bytes // (N * C * bytes_per)\n",
    "\n",
    "    side = int(math.sqrt(elems_per_image))\n",
    "    H = W = max((side // 16) * 16, 16)  # align nicely\n",
    "\n",
    "    print(f\"Allocating ~{target_mb} MB with tensor [{N},{C},{H},{W}]\")\n",
    "\n",
    "    x = torch.empty((N, C, H, W), device=device, dtype=dtype)\n",
    "\n",
    "    # Run a few convolutions to force cuDNN workspace allocation\n",
    "    for _ in range(5):\n",
    "        y = conv(x)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"  Conv OK, holding memory for 3s...\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    del x, y\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"  Released.\\n\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25U0yuyJlm",
   "metadata": {
    "id": "af25U0yuyJlm"
   },
   "source": [
    "## cuDNN benchmark enabled: FP32 convolution memory stress test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48705566-7239-426f-89ef-ab6a064b59e1",
   "metadata": {
    "id": "48705566-7239-426f-89ef-ab6a064b59e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla K80\n",
      "dtype=torch.float32, bytes/elem=4\n",
      "\n",
      "Allocating ~500 MB with [4,64,704,704] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Allocating ~600 MB with [4,64,768,768] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Allocating ~700 MB with [4,64,832,832] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Allocating ~800 MB with [4,64,896,896] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Allocating ~900 MB with [4,64,960,960] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float32   # FP32 now\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=64,\n",
    "    out_channels=64,\n",
    "    kernel_size=3,\n",
    "    padding=1,\n",
    "    bias=False\n",
    ").to(device=device, dtype=dtype)\n",
    "\n",
    "bytes_per = torch.tensor([], dtype=dtype).element_size()\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"dtype={dtype}, bytes/elem={bytes_per}\\n\")\n",
    "\n",
    "for target_mb in range(500, 1000, 100):  # lower ceiling for FP32\n",
    "    target_bytes = target_mb * 1024 * 1024\n",
    "\n",
    "    C = 64\n",
    "    N = 4  # smaller batch for FP32\n",
    "    elems_per_image = target_bytes // (N * C * bytes_per)\n",
    "\n",
    "    side = int(math.sqrt(elems_per_image))\n",
    "    H = W = max((side // 16) * 16, 16)\n",
    "\n",
    "    print(f\"Allocating ~{target_mb} MB with [{N},{C},{H},{W}] FP32\")\n",
    "\n",
    "    x = torch.empty((N, C, H, W), device=device, dtype=dtype)\n",
    "\n",
    "    for _ in range(3):\n",
    "        y = conv(x)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"  Conv OK, holding 3s...\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    del x, y\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"  Released.\\n\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ip45hFLzxFDV",
   "metadata": {
    "id": "ip45hFLzxFDV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cIAtk3QxGdJ",
   "metadata": {
    "id": "1cIAtk3QxGdJ"
   },
   "source": [
    "## CuDNN configuration for debugging CUDA memory issues (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea1ab39b-dd7e-4f49-866e-5b75d0ff5193",
   "metadata": {
    "id": "ea1ab39b-dd7e-4f49-866e-5b75d0ff5193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla K80\n",
      "dtype=torch.float32, bytes/elem=4\n",
      "\n",
      "Allocating ~500 MB with [4,64,704,704] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Allocating ~600 MB with [4,64,768,768] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Allocating ~700 MB with [4,64,832,832] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Allocating ~800 MB with [4,64,896,896] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Allocating ~900 MB with [4,64,960,960] FP32\n",
      "  Conv OK, holding 3s...\n",
      "  Released.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "## Disable CuDNN - 32 bit data\n",
    "# IMPORTANT in notebooks: restart kernel before running this if you got illegal access before.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "\n",
    "conv = nn.Conv2d(64, 64, 3, padding=1, bias=False).to(device=device, dtype=dtype)\n",
    "\n",
    "bytes_per = torch.tensor([], dtype=dtype).element_size()\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"dtype={dtype}, bytes/elem={bytes_per}\\n\")\n",
    "\n",
    "for target_mb in range(500, 1000, 100):\n",
    "    target_bytes = target_mb * 1024 * 1024\n",
    "    C = 64\n",
    "    N = 4\n",
    "    elems_per_image = target_bytes // (N * C * bytes_per)\n",
    "    side = int(math.sqrt(elems_per_image))\n",
    "    H = W = max((side // 16) * 16, 16)\n",
    "\n",
    "    print(f\"Allocating ~{target_mb} MB with [{N},{C},{H},{W}] FP32\")\n",
    "    x = torch.empty((N, C, H, W), device=device, dtype=dtype)\n",
    "\n",
    "    for _ in range(3):\n",
    "        y = conv(x)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"  Conv OK, holding 3s...\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    del x, y\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"  Released.\\n\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "osi3rFGYxsGh",
   "metadata": {
    "id": "osi3rFGYxsGh"
   },
   "source": [
    "## cuDNN disabled: native PyTorch conv2d (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5126d2a8-a30c-48a4-994d-ff1b98d2461a",
   "metadata": {
    "id": "5126d2a8-a30c-48a4-994d-ff1b98d2461a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d OK (cuDNN OFF). Holding 3s...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "torch.backends.cudnn.enabled = False  # key line\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "\n",
    "# weight: [out_ch, in_ch, kH, kW]\n",
    "w = torch.empty((64, 64, 3, 3), device=device, dtype=dtype)\n",
    "torch.nn.init.kaiming_uniform_(w, a=math.sqrt(5))\n",
    "\n",
    "N, C, H, W = 4, 64, 704, 704\n",
    "x = torch.empty((N, C, H, W), device=device, dtype=dtype)\n",
    "\n",
    "# warmup\n",
    "for _ in range(3):\n",
    "    y = F.conv2d(x, w, bias=None, stride=1, padding=1)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print(\"conv2d OK (cuDNN OFF). Holding 3s...\")\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8TIWuEPfw5ig",
   "metadata": {
    "id": "8TIWuEPfw5ig"
   },
   "source": [
    "## cuDNN-off FP32 convolution stress test for legacy GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8111d92-5279-4c99-a1f1-699c08ec48e9",
   "metadata": {
    "id": "d8111d92-5279-4c99-a1f1-699c08ec48e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla K80\n",
      "Starting FP32 conv memory test (cuDNN OFF)\n",
      "\n",
      "Alloc ~50 MB with x=[4,64,224,224] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~100 MB with x=[4,64,320,320] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~150 MB with x=[4,64,384,384] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~200 MB with x=[4,64,448,448] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~250 MB with x=[4,64,496,496] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~300 MB with x=[4,64,544,544] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~350 MB with x=[4,64,592,592] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~400 MB with x=[4,64,640,640] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~450 MB with x=[4,64,672,672] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~500 MB with x=[4,64,704,704] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~550 MB with x=[4,64,736,736] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~600 MB with x=[4,64,768,768] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~650 MB with x=[4,64,800,800] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~700 MB with x=[4,64,832,832] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~750 MB with x=[4,64,864,864] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~800 MB with x=[4,64,896,896] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~850 MB with x=[4,64,928,928] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~900 MB with x=[4,64,960,960] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Alloc ~950 MB with x=[4,64,976,976] FP32\n",
      "  OK, holding 2s...\n",
      "  Released.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "torch.backends.cudnn.enabled = False  # key for K80 stability\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "bytes_per = torch.tensor([], dtype=dtype).element_size()\n",
    "\n",
    "# Fixed conv weights\n",
    "w = torch.empty((64, 64, 3, 3), device=device, dtype=dtype)\n",
    "torch.nn.init.kaiming_uniform_(w, a=math.sqrt(5))\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"Starting FP32 conv memory test (cuDNN OFF)\\n\")\n",
    "\n",
    "for target_mb in range(50, 1000, 50):\n",
    "    target_bytes = target_mb * 1024 * 1024\n",
    "\n",
    "    C = 64\n",
    "    N = 4\n",
    "    elems_per_image = target_bytes // (N * C * bytes_per)\n",
    "    side = int(math.sqrt(max(elems_per_image, 1)))\n",
    "    H = W = max((side // 16) * 16, 16)\n",
    "\n",
    "    print(f\"Alloc ~{target_mb} MB with x=[{N},{C},{H},{W}] FP32\")\n",
    "\n",
    "    x = torch.empty((N, C, H, W), device=device, dtype=dtype)\n",
    "\n",
    "    # run a few convs to ensure it really uses the tensors\n",
    "    for _ in range(3):\n",
    "        y = F.conv2d(x, w, bias=None, stride=1, padding=1)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"  OK, holding 2s...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    del x, y\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"  Released.\\n\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wf58MSVxy3P8",
   "metadata": {
    "id": "wf58MSVxy3P8"
   },
   "source": [
    "## Hardware and Driver version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908114b1-a7c9-4b9a-9a04-85ab388ca101",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "908114b1-a7c9-4b9a-9a04-85ab388ca101",
    "outputId": "cb0661ab-4ae1-464a-addc-c04c0e3c038f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu102\n",
      "10.2\n",
      "7605\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "q9qZcuGUzC2R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9qZcuGUzC2R",
    "outputId": "e1f0f346-823e-43eb-cffb-1f2dc9f6fd54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA version (PyTorch): 10.2\n",
      "GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version (PyTorch):\", torch.version.cuda)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c55014-46ca-4d88-a296-909df4875bb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12c55014-46ca-4d88-a296-909df4875bb7",
    "outputId": "7d5c8178-b535-4172-cd3a-8ab1c392c277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470.256.02, Tesla K80\n",
      "470.256.02, Tesla K80\n",
      "470.256.02, Tesla K80\n",
      "470.256.02, Tesla K80\n",
      "470.256.02, Tesla K80\n",
      "470.256.02, Tesla K80\n",
      "470.256.02, Tesla K80\n",
      "470.256.02, Tesla K80\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=driver_version,name --format=csv,noheader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pgOApe3zy-bE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgOApe3zy-bE",
    "outputId": "040d7cf6-0ffd-4a2a-a8f0-5c2fa4e436e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "_imVaFg9zKbN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_imVaFg9zKbN",
    "outputId": "caa78e63-2c34-43ba-bd65-e2e09c65efa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU / CUDA / cuDNN info ===\n",
      "Tesla K80, 470.256.02\n",
      "Tesla K80, 470.256.02\n",
      "Tesla K80, 470.256.02\n",
      "Tesla K80, 470.256.02\n",
      "Tesla K80, 470.256.02\n",
      "Tesla K80, 470.256.02\n",
      "Tesla K80, 470.256.02\n",
      "Tesla K80, 470.256.02\n",
      "\n",
      "PyTorch: 1.12.0+cu102\n",
      "CUDA (PyTorch): 10.2\n",
      "cuDNN: 7605\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=== GPU / CUDA / cuDNN info ===\")\n",
    "!nvidia-smi --query-gpu=name,driver_version --format=csv,noheader\n",
    "\n",
    "print(\"\\nPyTorch:\", torch.__version__)\n",
    "print(\"CUDA (PyTorch):\", torch.version.cuda)\n",
    "print(\"cuDNN:\", torch.backends.cudnn.version())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d7143e-618b-4e4f-9822-860b30b810b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 124998\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"PID: {os.getpid()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fe86b-25e9-47df-9c22-18c7c838f6e1",
   "metadata": {},
   "source": [
    "# Kill PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c44ee1-9883-4846-aa40-48996655c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal\n",
    "os.kill(os.getpid(), signal.SIGKILL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c578e7d3-9798-47e5-b6a5-a413cf3fdc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
