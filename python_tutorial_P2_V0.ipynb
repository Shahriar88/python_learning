{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b3a29485-9a93-4269-9a82-d389aea485a9",
   "metadata": {},
   "source": [
    "https://docs.python.org/3/py-modindex.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62106b03-89f8-4f81-a5d5-184caa5567b0",
   "metadata": {},
   "source": [
    "# 📘 Python Tutorial – Part 2\n",
    "\n",
    "## ✅ Topics Covered\n",
    "\n",
    "* **Efficient workload chunking** for large datasets\n",
    "* **Thread-based parallelism (with chunks)** using `ThreadPoolExecutor`\n",
    "* **Vectorized computation in chunks** with NumPy\n",
    "* **Numba JIT acceleration in chunks**:\n",
    "\n",
    "  * Using `@njit(parallel=True)` with `prange`\n",
    "  * Handling very large arrays by splitting into batches\n",
    "* **CUDA acceleration with Numba**:\n",
    "\n",
    "  * `@cuda.jit` kernels for elementwise operations\n",
    "  * Choosing `threads_per_block` and `blocks_per_grid`\n",
    "  * Chunk-based GPU processing for large arrays\n",
    "  * Optimized CUDA kernel with tuned block sizes\n",
    "* **Device information utilities**:\n",
    "\n",
    "  * `print_cpu_info()` → CPU core/thread details\n",
    "  * `print_cuda_info()` → GPU details (name, SMs, warp size, max threads/block)\n",
    "* **Performance comparison across approaches**:\n",
    "\n",
    "  * Numba (CPU, chunked)\n",
    "  * CUDA (basic & optimized, chunked)\n",
    "  * ThreadPoolExecutor (chunked)\n",
    "  * Pure NumPy vectorization (chunked)\n",
    "\n",
    "## 🛠️ Implemented Functions\n",
    "\n",
    "* **`run_numba_chunked()`** → Numba JIT computation in chunks\n",
    "* **`run_cuda_chunked()`** → CUDA kernel execution in chunks\n",
    "* **`pick_chunk_size()`** → Automatically tune GPU chunk size\n",
    "* **`fill_eqn_threaded_chunked()`** → Thread-based parallel processing with chunks\n",
    "* **`fill_eqn_vectorized_chunked()`** → NumPy vectorized chunked computation\n",
    "* **`print_cpu_info()` / `print_cuda_info()`** → Hardware inspection helpers\n",
    "\n",
    "## ⚡ Key Concepts Practiced\n",
    "\n",
    "* **Chunking strategy** to process massive arrays without exhausting memory\n",
    "* **CPU vs GPU tradeoffs**: when GPU wins, when CPU (NumPy) is more efficient\n",
    "* **Threads vs vectorization vs GPU kernels** performance insights\n",
    "* **CUDA architecture basics**:\n",
    "\n",
    "  * Streaming Multiprocessors (SMs)\n",
    "  * Warp size\n",
    "  * Threads per block tuning\n",
    "* **Correctness validation** → Comparing results across all approaches with `np.allclose`\n",
    "\n",
    "## 📂 Files\n",
    "\n",
    "* **`python_tutorial_P2_V0.ipynb`** → Jupyter Notebook export\n",
    "* **`python_tutorial_P2_V0.html`** → HTML export of tutorial session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2347f0-bfa4-471e-9477-aeba5d066c22",
   "metadata": {},
   "source": [
    "## numba module basics for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae85b1e-f793-4849-960f-159e3368b1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Name: Intel64 Family 6 Model 141 Stepping 1, GenuineIntel\n",
      "Architecture: 64bit\n",
      "Logical CPUs: 16\n",
      "Physical cores: 8\n",
      "Max frequency (MHz): 2611.0\n",
      "Current frequency (MHz): 2611.0\n",
      "Total RAM (GB): 63.2\n",
      "get_num_threads_cpu: 16\n",
      "_________________________\n",
      "[numba_chunked] 2.108 s\n",
      "[cuda_chunked] 23.815 s\n",
      "Name: b'NVIDIA RTX A4000 Laptop GPU'\n",
      "SMs: 40\n",
      "Warp size: 32\n",
      "Max threads/block: 1024\n",
      "_________________________\n",
      "chunk_size : 146716753\n",
      "_________________________\n",
      "[cuda_chunked] 28.060 s\n",
      "[python_ThreadPoolExecutor_chunked] 18.340 s\n",
      "[vectorized_chunked] 43.635 s\n",
      "Equal (Numba vs vectorized, float64)? True\n",
      "Equal (Numba_GPU vs vectorized, float64)? True\n",
      "Equal (Numba_GPU_Opt vs vectorized, float64)? True\n",
      "Equal (python_ThreadPoolExecutor vs vectorized, float64)? False\n"
     ]
    }
   ],
   "source": [
    "# numba module basics for future use ________________________________________\n",
    "import os\n",
    "import numpy as np\n",
    "from functools import wraps\n",
    "from time import perf_counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from numba import njit, prange, set_num_threads, get_num_threads\n",
    "from numba import cuda\n",
    "#////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "\n",
    "# ---------- timing decorator ----------\n",
    "def timed(label: str | None = None):\n",
    "    def deco(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            t0 = perf_counter()\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            finally:\n",
    "                dt = perf_counter() - t0\n",
    "                print(f\"[{label or func.__name__}] {dt:.3f} s\")\n",
    "        return wrapper\n",
    "    return deco\n",
    "#////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "\n",
    "# ---------- Sample data maker for testing code ----------\n",
    "# --- update the equations, as your requirements *************************\n",
    "def make_data(N=100, seed=42,\n",
    "              x_range=(-10, 10), y_range=(-10, 10),\n",
    "              m_range=(-5, 5),  b_range=(-5, 5),\n",
    "              dtype=np.float64) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    data = np.empty((N, 5), dtype=dtype)\n",
    "    data[:, 0] = rng.uniform(*x_range, N)  # x\n",
    "    data[:, 1] = rng.uniform(*y_range, N)  # y\n",
    "    data[:, 2] = rng.uniform(*m_range, N)  # m\n",
    "    data[:, 3] = rng.uniform(*b_range, N)  # b\n",
    "    data[:, 4] = 0.0                       # eqn\n",
    "    return data\n",
    "#////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "\n",
    "# ---------- Numba CPU kernel equation----------\n",
    "# --- update the equations *************************\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def fill_eqn_numba(data, res_col_idx=4):\n",
    "    x = data[:, 0]\n",
    "    y = data[:, 1]\n",
    "    m = data[:, 2]\n",
    "    b = data[:, 3]\n",
    "    out = data[:, res_col_idx]\n",
    "    n = x.shape[0]\n",
    "    for i in prange(n):\n",
    "        x2 = x[i] * x[i]; x4 = x2 * x2\n",
    "        b3 = b[i] * b[i] * b[i]\n",
    "        y2 = y[i] * y[i]; y4 = y2 * y2; y8 = y4 * y4; y10 = y8 * y2\n",
    "        out[i] = y10 + m[i] * x4 + b3\n",
    "\n",
    "\n",
    "# ---------- chunked Numba (CPU) ----------\n",
    "@timed(\"numba_chunked\")\n",
    "def run_numba_chunked(data, res_col_idx=4, chunk_size=5_000_000, threads=None, func = fill_eqn_numba):\n",
    "    \"\"\"\n",
    "    Calls the same @njit(parallel=True) kernel on row-slices of `data`.\n",
    "    Good when temporaries/cache become the bottleneck on huge arrays.\n",
    "    \"\"\"\n",
    "    if threads:\n",
    "        set_num_threads(threads)\n",
    "    N = data.shape[0]\n",
    "    for start in range(0, N, chunk_size):\n",
    "        end = min(start + chunk_size, N)\n",
    "        # Row slices are C-contiguous → zero-copy views\n",
    "        func(data[start:end, :], res_col_idx)\n",
    "#////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "\n",
    "# ---------- Numba CUDA kernel ----------\n",
    "# --- update the equations *************************\n",
    "# Works with float32 or float64; float32 is faster on most GPUs.\n",
    "@cuda.jit(fastmath=True)\n",
    "def fill_eqn_cuda(x, y, m, b, out):\n",
    "    i = cuda.grid(1)\n",
    "    if i < x.size:\n",
    "        xi = x[i]; yi = y[i]; mi = m[i]; bi = b[i]\n",
    "        x2 = xi * xi; x4 = x2 * x2\n",
    "        b3 = bi * bi * bi\n",
    "        y2 = yi * yi; y4 = y2 * y2; y8 = y4 * y4; y10 = y8 * y2\n",
    "        out[i] = y10 + mi * x4 + b3\n",
    "\n",
    "# ---------- chunked Numba (GPU) ----------\n",
    "@timed(\"cuda_chunked\")\n",
    "def run_cuda_chunked(data, res_col_idx=4, threads_per_block=256, chunk_size=5_000_000, use_float32=False,\n",
    "                    func=fill_eqn_cuda):\n",
    "    dtype = np.float32 if use_float32 else np.float64\n",
    "    N = data.shape[0]\n",
    "    for start in range(0, N, chunk_size):\n",
    "        end = min(start + chunk_size, N)\n",
    "\n",
    "        # slice host arrays and make them contiguous for the GPU\n",
    "        x_h = np.ascontiguousarray(data[start:end, 0], dtype=dtype)\n",
    "        y_h = np.ascontiguousarray(data[start:end, 1], dtype=dtype)\n",
    "        m_h = np.ascontiguousarray(data[start:end, 2], dtype=dtype)\n",
    "        b_h = np.ascontiguousarray(data[start:end, 3], dtype=dtype)\n",
    "\n",
    "        \n",
    "        d_x = cuda.to_device(x_h)\n",
    "        d_y = cuda.to_device(y_h)\n",
    "        d_m = cuda.to_device(m_h)\n",
    "        d_b = cuda.to_device(b_h)\n",
    "        d_out = cuda.device_array(end - start, dtype=dtype)\n",
    "        blocks = ((end - start) + threads_per_block - 1) // threads_per_block\n",
    "        func[blocks, threads_per_block](d_x, d_y, d_m, d_b, d_out)\n",
    "        data[start:end, res_col_idx] = d_out.copy_to_host().astype(data.dtype, copy=False)\n",
    "#////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "\n",
    "# ---------- chunked python_ThreadPoolExecutor (GPU) ----------\n",
    "\n",
    "# Your existing block function (kept here for completeness)\n",
    "# --- update the equations *************************\n",
    "def compute_eqn_block(block_xy_mb: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    block_xy_mb: (k, 4) array with columns [x, y, m, b]\n",
    "    returns: (k,) array of y + m*x + b\n",
    "    \"\"\"\n",
    "    x = block_xy_mb[:, 0]\n",
    "    y = block_xy_mb[:, 1]\n",
    "    m = block_xy_mb[:, 2]\n",
    "    b = block_xy_mb[:, 3]\n",
    "    return y + m * x + b\n",
    "\n",
    "@timed(\"python_ThreadPoolExecutor_chunked\")\n",
    "def fill_eqn_threaded_chunked(data: np.ndarray,\n",
    "                              res_col_idx: int = 4,\n",
    "                              func=compute_eqn_block,\n",
    "                              max_workers: int = 4,\n",
    "                              chunk_size: int = 5_000_000) -> None:\n",
    "    \"\"\"\n",
    "    Processes data in chunks to reduce memory pressure, each chunk is threaded internally.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    workers = min(max_workers, os.cpu_count() or 1)\n",
    "\n",
    "    for start in range(0, N, chunk_size):\n",
    "        end = min(start + chunk_size, N)\n",
    "        chunk = data[start:end, :]  # slice view, no copy\n",
    "\n",
    "        # Split this chunk among workers\n",
    "        idx_chunks = np.array_split(np.arange(chunk.shape[0]), workers)\n",
    "\n",
    "        def job(idxs):\n",
    "            block = chunk[idxs, :res_col_idx]  # only x,y,m,b\n",
    "            return func(block)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "            futures = [(idxs, ex.submit(job, idxs)) for idxs in idx_chunks if idxs.size > 0]\n",
    "            for idxs, fut in futures:\n",
    "                chunk[idxs, res_col_idx] = fut.result()\n",
    "\n",
    "        # Writes directly into `data[start:end, res_col_idx]` since `chunk` is a view\n",
    "#////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "\n",
    "# Get CPU and GPU info __________________________\n",
    "def print_cpu_info():\n",
    "\n",
    "    import os\n",
    "    import platform\n",
    "    \n",
    "    try:\n",
    "        import psutil\n",
    "    except ImportError:\n",
    "        psutil = None\n",
    "\n",
    "    print(\"CPU Name:\", platform.processor() or platform.machine())\n",
    "    print(\"Architecture:\", platform.architecture()[0])\n",
    "    print(\"Logical CPUs:\", os.cpu_count())\n",
    "    \n",
    "    if psutil:\n",
    "        print(\"Physical cores:\", psutil.cpu_count(logical=False))\n",
    "        print(\"Max frequency (MHz):\", psutil.cpu_freq().max if psutil.cpu_freq() else \"N/A\")\n",
    "        print(\"Current frequency (MHz):\", psutil.cpu_freq().current if psutil.cpu_freq() else \"N/A\")\n",
    "        print(\"Total RAM (GB):\", round(psutil.virtual_memory().total / (1024**3), 2))\n",
    "        print(f'get_num_threads_cpu: {get_num_threads()}')\n",
    "        print('_'*25)\n",
    "    else:\n",
    "        print(\"Install psutil for more details (pip install psutil)\")\n",
    "        print('_'*25)\n",
    "\n",
    "\n",
    "def print_cuda_info():\n",
    "    dev = cuda.get_current_device()\n",
    "    print(\"Name:\", dev.name)\n",
    "    print(\"SMs:\", dev.MULTIPROCESSOR_COUNT)\n",
    "    print(\"Warp size:\", dev.WARP_SIZE)\n",
    "    print(\"Max threads/block:\", dev.MAX_THREADS_PER_BLOCK)\n",
    "    print('_'*25)\n",
    "\n",
    "def pick_chunk_size(use_float32=False, safety=0.8):\n",
    "    free, _ = cuda.current_context().get_memory_info()\n",
    "    dtype_size = 4 if use_float32 else 8\n",
    "    per_elem = 5 * dtype_size\n",
    "    chunk_size = max(1, int(safety * free // per_elem))\n",
    "    print(f'chunk_size : {chunk_size}')\n",
    "    print('_'*25)\n",
    "    return chunk_size\n",
    "#////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "\n",
    "# ---------- chunked vectorized ----------\n",
    "# --- update the equations *************************\n",
    "@timed(\"vectorized_chunked\")\n",
    "def fill_eqn_vectorized_chunked(data, res_col_idx=4, chunk_size=5_000_000):\n",
    "    \"\"\"\n",
    "    Vectorized per-chunk to limit temporary array sizes.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    for start in range(0, N, chunk_size):\n",
    "        end = min(start + chunk_size, N)\n",
    "        x = data[start:end, 0]\n",
    "        y = data[start:end, 1]\n",
    "        m = data[start:end, 2]\n",
    "        b = data[start:end, 3]\n",
    "        data[start:end, res_col_idx] = y**10 + m*(x**4) + b**3\n",
    "#////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "\n",
    "# --- Execution *************************\n",
    "\n",
    "data_nb = make_data(N=300_000_000, seed=123, dtype=np.float64)\n",
    "data_nb_cuda = data_nb.copy()\n",
    "data_nb_cuda_opt = data_nb.copy()\n",
    "data_thread = data_nb.copy()\n",
    "data_vec = data_nb.copy()\n",
    "\n",
    "# Numba (CPU) in chunks ___________________________________________________________________\n",
    "print_cpu_info()\n",
    "#run_numba_chunked(data_nb, res_col_idx=4, chunk_size=5_000_000, threads=os.cpu_count())\n",
    "run_numba_chunked(data_nb, res_col_idx=4, chunk_size=5_000_000, threads=4)\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Fits big N by processing in batches on the GPU _________________________________________________\n",
    "run_cuda_chunked(data_nb_cuda, res_col_idx=4, threads_per_block=256,\n",
    "                 chunk_size=5_000_000,   # tune this (see below)\n",
    "                 use_float32=False)      # True = faster & less memory, slight numeric diffs\n",
    "\n",
    "#______________CUDA Optimized____________________\n",
    "print_cuda_info()\n",
    "chunk_size = pick_chunk_size()\n",
    "run_cuda_chunked(data_nb_cuda_opt, res_col_idx=4, threads_per_block=512,\n",
    "                 chunk_size=chunk_size,   # tuned this\n",
    "                 use_float32=False)      # True = faster & less memory, slight numeric diffs\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# python_ThreadPoolExecutor in chunks ___________________________________________________________________\n",
    "fill_eqn_threaded_chunked(data_thread, max_workers=4, chunk_size=5_000_000)\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Vectorized in chunks ___________________________________________________________________\n",
    "fill_eqn_vectorized_chunked(data_vec, res_col_idx=4, chunk_size=5_000_000)\n",
    "#///////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "#___________________________________________________________________________________________________________\n",
    "\n",
    "print(\"Equal (Numba vs vectorized, float64)?\", np.allclose(data_nb[:, 4], data_vec[:, 4]))\n",
    "print(\"Equal (Numba_GPU vs vectorized, float64)?\", np.allclose(data_nb_cuda[:, 4], data_vec[:, 4]))\n",
    "print(\"Equal (Numba_GPU_Opt vs vectorized, float64)?\", np.allclose(data_nb_cuda_opt[:, 4], data_vec[:, 4]))\n",
    "print(\"Equal (python_ThreadPoolExecutor vs vectorized, float64)?\", np.allclose(data_thread[:, 4], data_vec[:, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372e270-4b33-4168-bd34-99920ad4a362",
   "metadata": {},
   "source": [
    "## Using Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2369fc4-62c1-4647-81fe-8a60d42f2757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[numba_chunked] 0.455 s\n",
      "[numba_chunked] 0.074 s\n",
      "[vectorized_chunked] 2.479 s\n",
      "Equal (Numba vs vectorized)? True\n",
      "[cuda_chunked] 1.336 s\n",
      "Equal (GPU vs vectorized)? True\n",
      "Name: b'NVIDIA RTX A4000 Laptop GPU'\n",
      "SMs: 40\n",
      "Warp size: 32\n",
      "Max threads/block: 1024\n",
      "_________________________\n",
      "chunk_size: 147555614\n",
      "_________________________\n",
      "[cuda_chunked] 0.639 s\n",
      "Equal (GPU_opt vs vectorized)? True\n",
      "[python_ThreadPoolExecutor_chunked] 0.899 s\n",
      "Equal (ThreadPool vs vectorized)? True\n"
     ]
    }
   ],
   "source": [
    "# ----------------- imports (module level) -----------------\n",
    "import os\n",
    "import platform\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "from functools import wraps\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Optional, Callable\n",
    "\n",
    "from numba import njit, prange, set_num_threads, get_num_threads\n",
    "from numba import cuda\n",
    "\n",
    "# ----------------- timing decorator (module level) -----------------\n",
    "def timed(label: str | None = None):\n",
    "    def deco(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            t0 = perf_counter()\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            finally:\n",
    "                dt = perf_counter() - t0\n",
    "                print(f\"[{label or func.__name__}] {dt:.3f} s\")\n",
    "        return wrapper\n",
    "    return deco\n",
    "\n",
    "# ----------------- JIT kernels (module level) -----------------\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def fill_eqn_numba_kernel(data, res_col_idx=4):\n",
    "    x = data[:, 0]\n",
    "    y = data[:, 1]\n",
    "    m = data[:, 2]\n",
    "    b = data[:, 3]\n",
    "    out = data[:, res_col_idx]\n",
    "    n = x.shape[0]\n",
    "    for i in prange(n):\n",
    "        x2 = x[i] * x[i]; x4 = x2 * x2\n",
    "        b3 = b[i] * b[i] * b[i]\n",
    "        y2 = y[i] * y[i]; y4 = y2 * y2; y8 = y4 * y4; y10 = y8 * y2\n",
    "        out[i] = y10 + m[i] * x4 + b3\n",
    "\n",
    "@cuda.jit(fastmath=True)\n",
    "def fill_eqn_cuda_kernel(x, y, m, b, out):\n",
    "    i = cuda.grid(1)\n",
    "    if i < x.size:\n",
    "        xi = x[i]; yi = y[i]; mi = m[i]; bi = b[i]\n",
    "        x2 = xi * xi; x4 = x2 * x2\n",
    "        b3 = bi * bi * bi\n",
    "        y2 = yi * yi; y4 = y2 * y2; y8 = y4 * y4; y10 = y8 * y2\n",
    "        out[i] = y10 + mi * x4 + b3\n",
    "\n",
    "# Plain Python block function (used by ThreadPoolExecutor path)\n",
    "def compute_eqn_block(block_xy_mb: np.ndarray) -> np.ndarray:\n",
    "    x = block_xy_mb[:, 0]\n",
    "    y = block_xy_mb[:, 1]\n",
    "    m = block_xy_mb[:, 2]\n",
    "    b = block_xy_mb[:, 3]\n",
    "    return y + m * x + b\n",
    "\n",
    "# Make the threaded function use the same polynomial equation:\n",
    "def compute_eqn_block_poly(block_xy_mb: np.ndarray) -> np.ndarray:\n",
    "    x = block_xy_mb[:, 0]\n",
    "    y = block_xy_mb[:, 1]\n",
    "    m = block_xy_mb[:, 2]\n",
    "    b = block_xy_mb[:, 3]\n",
    "    return y**10 + m * (x**4) + b**3\n",
    "\n",
    "# ----------------- class that orchestrates -----------------\n",
    "class ParallelTools:\n",
    "    # ========= sample data =========\n",
    "    @staticmethod\n",
    "    def make_data(N=100, seed=42,\n",
    "                  x_range=(-10, 10), y_range=(-10, 10),\n",
    "                  m_range=(-5, 5), b_range=(-5, 5),\n",
    "                  dtype=np.float64) -> np.ndarray:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        data = np.empty((N, 5), dtype=dtype)\n",
    "        data[:, 0] = rng.uniform(*x_range, N)\n",
    "        data[:, 1] = rng.uniform(*y_range, N)\n",
    "        data[:, 2] = rng.uniform(*m_range, N)\n",
    "        data[:, 3] = rng.uniform(*b_range, N)\n",
    "        data[:, 4] = 0.0\n",
    "        return data\n",
    "\n",
    "    # ========= numba CPU (chunked) =========\n",
    "    @staticmethod\n",
    "    @timed(\"numba_chunked\")\n",
    "    def run_numba_chunked(data: np.ndarray,\n",
    "                          res_col_idx: int = 4,\n",
    "                          chunk_size: int = 5_000_000,\n",
    "                          threads: Optional[int] = None,\n",
    "                          func: Optional[Callable] = None):\n",
    "        if threads:\n",
    "            set_num_threads(threads)\n",
    "        if func is None:\n",
    "            func = fill_eqn_numba_kernel\n",
    "\n",
    "        N = data.shape[0]\n",
    "        for start in range(0, N, chunk_size):\n",
    "            end = min(start + chunk_size, N)\n",
    "            func(data[start:end, :], res_col_idx)\n",
    "\n",
    "    # ========= CUDA (chunked) =========\n",
    "    @staticmethod\n",
    "    @timed(\"cuda_chunked\")\n",
    "    def run_cuda_chunked(data: np.ndarray,\n",
    "                         res_col_idx: int = 4,\n",
    "                         threads_per_block: int = 256,\n",
    "                         chunk_size: int = 5_000_000,\n",
    "                         use_float32: bool = False,\n",
    "                         func = fill_eqn_cuda_kernel):\n",
    "        dtype = np.float32 if use_float32 else np.float64\n",
    "        N = data.shape[0]\n",
    "        for start in range(0, N, chunk_size):\n",
    "            end = min(start + chunk_size, N)\n",
    "\n",
    "            # prepare contiguous host buffers\n",
    "            x_h = np.ascontiguousarray(data[start:end, 0], dtype=dtype)\n",
    "            y_h = np.ascontiguousarray(data[start:end, 1], dtype=dtype)\n",
    "            m_h = np.ascontiguousarray(data[start:end, 2], dtype=dtype)\n",
    "            b_h = np.ascontiguousarray(data[start:end, 3], dtype=dtype)\n",
    "\n",
    "            # device copies\n",
    "            d_x = cuda.to_device(x_h)\n",
    "            d_y = cuda.to_device(y_h)\n",
    "            d_m = cuda.to_device(m_h)\n",
    "            d_b = cuda.to_device(b_h)\n",
    "            d_out = cuda.device_array(end - start, dtype=dtype)\n",
    "\n",
    "            blocks = ((end - start) + threads_per_block - 1) // threads_per_block\n",
    "            func[blocks, threads_per_block](d_x, d_y, d_m, d_b, d_out)\n",
    "\n",
    "            data[start:end, res_col_idx] = d_out.copy_to_host().astype(data.dtype, copy=False)\n",
    "\n",
    "    # ========= Python threads (chunked) =========\n",
    "    @staticmethod\n",
    "    @timed(\"python_ThreadPoolExecutor_chunked\")\n",
    "    def fill_eqn_threaded_chunked(data: np.ndarray,\n",
    "                                  res_col_idx: int = 4,\n",
    "                                  func: Optional[Callable] = None,\n",
    "                                  max_workers: int = 4,\n",
    "                                  chunk_size: int = 5_000_000):\n",
    "        if func is None:\n",
    "            func = compute_eqn_block\n",
    "\n",
    "        N = data.shape[0]\n",
    "        workers = min(max_workers, os.cpu_count() or 1)\n",
    "\n",
    "        for start in range(0, N, chunk_size):\n",
    "            end = min(start + chunk_size, N)\n",
    "            chunk = data[start:end, :]  # view\n",
    "\n",
    "            idx_chunks = np.array_split(np.arange(chunk.shape[0]), workers)\n",
    "\n",
    "            def job(idxs):\n",
    "                block = chunk[idxs, :res_col_idx]\n",
    "                return func(block)\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "                futures = [(idxs, ex.submit(job, idxs)) for idxs in idx_chunks if idxs.size > 0]\n",
    "                for idxs, fut in futures:\n",
    "                    chunk[idxs, res_col_idx] = fut.result()\n",
    "\n",
    "    # ========= vectorized (chunked) =========\n",
    "    @staticmethod\n",
    "    @timed(\"vectorized_chunked\")\n",
    "    def fill_eqn_vectorized_chunked(data: np.ndarray, res_col_idx: int = 4, chunk_size: int = 5_000_000):\n",
    "        N = data.shape[0]\n",
    "        for start in range(0, N, chunk_size):\n",
    "            end = min(start + chunk_size, N)\n",
    "            x = data[start:end, 0]\n",
    "            y = data[start:end, 1]\n",
    "            m = data[start:end, 2]\n",
    "            b = data[start:end, 3]\n",
    "            data[start:end, res_col_idx] = y**10 + m*(x**4) + b**3\n",
    "\n",
    "    # ========= system info =========\n",
    "    @staticmethod\n",
    "    def print_cpu_info():\n",
    "        try:\n",
    "            import psutil\n",
    "        except ImportError:\n",
    "            psutil = None\n",
    "\n",
    "        print(\"CPU Name:\", platform.processor() or platform.machine())\n",
    "        print(\"Architecture:\", platform.architecture()[0])\n",
    "        print(\"Logical CPUs:\", os.cpu_count())\n",
    "        if psutil:\n",
    "            print(\"Physical cores:\", psutil.cpu_count(logical=False))\n",
    "            f = psutil.cpu_freq()\n",
    "            print(\"Max frequency (MHz):\", f.max if f else \"N/A\")\n",
    "            print(\"Current frequency (MHz):\", f.current if f else \"N/A\")\n",
    "            print(\"Total RAM (GB):\", round(psutil.virtual_memory().total / (1024**3), 2))\n",
    "        print(\"Numba threads (current):\", get_num_threads())\n",
    "        print(\"_\"*25)\n",
    "\n",
    "    @staticmethod\n",
    "    def print_cuda_info():\n",
    "        dev = cuda.get_current_device()\n",
    "        print(\"Name:\", dev.name)\n",
    "        print(\"SMs:\", dev.MULTIPROCESSOR_COUNT)\n",
    "        print(\"Warp size:\", dev.WARP_SIZE)\n",
    "        print(\"Max threads/block:\", dev.MAX_THREADS_PER_BLOCK)\n",
    "        print(\"_\"*25)\n",
    "\n",
    "    @staticmethod\n",
    "    def pick_chunk_size(use_float32=False, safety=0.8):\n",
    "        free, _ = cuda.current_context().get_memory_info()\n",
    "        dtype_size = 4 if use_float32 else 8\n",
    "        per_elem = 5 * dtype_size  # x,y,m,b,out\n",
    "        chunk_size = max(1, int(safety * free // per_elem))\n",
    "        print(\"chunk_size:\", chunk_size)\n",
    "        print(\"_\"*25)\n",
    "        return chunk_size\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Sample data maker for testing code ----------\n",
    "# --- update the equations, as your requirements *************************\n",
    "def make_data(N=100, seed=42,\n",
    "              x_range=(-10, 10), y_range=(-10, 10),\n",
    "              m_range=(-5, 5),  b_range=(-5, 5),\n",
    "              dtype=np.float64) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    data = np.empty((N, 5), dtype=dtype)\n",
    "    data[:, 0] = rng.uniform(*x_range, N)  # x\n",
    "    data[:, 1] = rng.uniform(*y_range, N)  # y\n",
    "    data[:, 2] = rng.uniform(*m_range, N)  # m\n",
    "    data[:, 3] = rng.uniform(*b_range, N)  # b\n",
    "    data[:, 4] = 0.0                       # eqn\n",
    "    return data\n",
    "#////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Base once\n",
    "base = make_data(N=30_000_000, seed=123, dtype=np.float64)\n",
    "\n",
    "# (Optional) tiny warmup to avoid counting JIT compile time in first timing\n",
    "ParallelTools.run_numba_chunked(base[:100_000].copy(), chunk_size=50_000, threads=4)\n",
    "\n",
    "# CPU Numba\n",
    "data_nb = base.copy()\n",
    "ParallelTools.run_numba_chunked(data_nb, res_col_idx=4, chunk_size=5_000_000, threads=4)\n",
    "\n",
    "# Vectorized\n",
    "data_vec = base.copy()\n",
    "ParallelTools.fill_eqn_vectorized_chunked(data_vec, res_col_idx=4, chunk_size=5_000_000)\n",
    "print(\"Equal (Numba vs vectorized)?\", np.allclose(data_nb[:,4], data_vec[:,4]))\n",
    "\n",
    "# CUDA (256)\n",
    "data_gpu = base.copy()\n",
    "ParallelTools.run_cuda_chunked(data_gpu, res_col_idx=4, threads_per_block=256,\n",
    "                               chunk_size=5_000_000, use_float32=False)\n",
    "print(\"Equal (GPU vs vectorized)?\", np.allclose(data_gpu[:,4], data_vec[:,4]))\n",
    "\n",
    "# CUDA tuned chunk size (ensure context exists before pick_chunk_size)\n",
    "ParallelTools.print_cuda_info()\n",
    "cs = ParallelTools.pick_chunk_size(use_float32=False)   # or True for speed (then relax tolerances)\n",
    "data_gpu_opt = base.copy()\n",
    "ParallelTools.run_cuda_chunked(data_gpu_opt, res_col_idx=4, threads_per_block=512,\n",
    "                               chunk_size=cs, use_float32=False)\n",
    "print(\"Equal (GPU_opt vs vectorized)?\", np.allclose(data_gpu_opt[:,4], data_vec[:,4]))\n",
    "\n",
    "# ThreadPool\n",
    "data_thr = base.copy()\n",
    "ParallelTools.fill_eqn_threaded_chunked(\n",
    "    data_thr,\n",
    "    res_col_idx=4,\n",
    "    func=compute_eqn_block_poly,   # <-- use the same formula\n",
    "    max_workers=4,\n",
    "    chunk_size=5_000_000\n",
    ")\n",
    "print(\"Equal (ThreadPool vs vectorized)?\", np.allclose(data_thr[:,4], data_vec[:,4]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725f886-7b2f-4b0c-a136-97fb88f4f8b1",
   "metadata": {},
   "source": [
    "# Example: timing methods in a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4efd4001-50de-47ff-ae17-2527a1288569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[instance_method] 3.500 s\n",
      "Alice finished in 3.5 sec\n",
      "[class_method] 3.501 s\n",
      "classwork finished in 3.5 sec\n",
      "[static_method] 3.501 s\n",
      "staticwork finished in 3.5 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "from time import perf_counter\n",
    "\n",
    "# your same decorator\n",
    "def timed(label: str | None = None):\n",
    "    def deco(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            t0 = perf_counter()\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            finally:\n",
    "                dt = perf_counter() - t0\n",
    "                print(f\"[{label or func.__name__}] {dt:.3f} s\")\n",
    "        return wrapper\n",
    "    return deco\n",
    "\n",
    "# ----------------------\n",
    "# Using inside a class\n",
    "# ----------------------\n",
    "class Worker:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    @timed(\"instance_method\") # instance: self\n",
    "    def do_work(self, n=2.5):\n",
    "        time.sleep(n)  # simulate work\n",
    "        return f\"{self.name} finished in {n} sec\"\n",
    "\n",
    "    @classmethod # classmethod: cls\n",
    "    @timed(\"class_method\")\n",
    "    def do_classwork(cls, n=2.5):\n",
    "        time.sleep(n)\n",
    "        return f\"classwork finished in {n} sec\"\n",
    "\n",
    "    @staticmethod # staticmethod: None\n",
    "    @timed(\"static_method\")\n",
    "    def do_staticwork(n=2.5):\n",
    "        time.sleep(n)\n",
    "        return f\"staticwork finished in {n} sec\"\n",
    "\n",
    "# ----------------------\n",
    "# Run\n",
    "# ----------------------\n",
    "w = Worker(\"Alice\")\n",
    "print(w.do_work(3.5))\n",
    "print(Worker.do_classwork(3.5))\n",
    "print(Worker.do_staticwork(3.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa5899e-c8f4-414a-af87-cf581b704310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Celsius:\n",
    "    def __init__(self, temperature=0):\n",
    "        self._temperature = temperature\n",
    "    \n",
    "    @property #getter\n",
    "    def temperature(self):\n",
    "        return self._temperature\n",
    "    \n",
    "    @temperature.setter\n",
    "    def temperature(self, value):\n",
    "        if value < -273.15:\n",
    "            raise ValueError(\"Temperature below -273.15 is not possible\")\n",
    "        self._temperature = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d191f86-b26a-4251-86bf-5209278132ad",
   "metadata": {},
   "source": [
    "## 1️⃣ Instance Methods (default, no decorator)\n",
    "\n",
    "- Defined normally inside a class.  \n",
    "- First argument is always **`self`**, which refers to the instance of the class.  \n",
    "- Used when you need access to **instance attributes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a022e9ab-9f6e-4d49-bc84-a324ec603449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rex says woof!\n"
     ]
    }
   ],
   "source": [
    "class Dog:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def bark(self):  # instance method\n",
    "        return f\"{self.name} says woof!\"\n",
    "\n",
    "d = Dog(\"Rex\")\n",
    "print(d.bark())   # Rex says woof!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1eaf02-3236-4ffb-a593-83bcfe9a4c8f",
   "metadata": {},
   "source": [
    "## 2️⃣ Class Methods (`@classmethod`)\n",
    "\n",
    "- Declared with **`@classmethod`**.  \n",
    "- First argument is always **`cls`** (the class itself, not an instance).  \n",
    "- Often used for **alternative constructors** or **operations that affect the whole class**, not just one instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beffc740-c681-47c1-8ec7-4a3440318f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canis familiaris\n",
      "Canis lupus familiaris\n"
     ]
    }
   ],
   "source": [
    "class Dog:\n",
    "    species = \"Canis familiaris\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    @classmethod\n",
    "    def set_species(cls, species_name):\n",
    "        cls.species = species_name\n",
    "\n",
    "d1 = Dog(\"Buddy\")\n",
    "d2 = Dog(\"Max\")\n",
    "\n",
    "print(d1.species)   # Canis familiaris\n",
    "Dog.set_species(\"Canis lupus familiaris\")  # change at class level\n",
    "print(d2.species)   # Canis lupus familiaris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7485f4-28e5-457e-9393-adaa5e709523",
   "metadata": {},
   "source": [
    "## 3️⃣ Static Methods (`@staticmethod`)\n",
    "\n",
    "- Declared with **`@staticmethod`**.  \n",
    "- Behaves like a plain function placed inside the **class namespace**.  \n",
    "- No `self`, no `cls` is automatically passed.  \n",
    "- Useful for **utility/helper functions** logically related to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1e080d-bc46-4c06-b08e-d63b254f34fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "class MathTools:\n",
    "    @staticmethod\n",
    "    def add(a, b):\n",
    "        return a + b\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_even(n):\n",
    "        return n % 2 == 0\n",
    "\n",
    "print(MathTools.add(3, 4))     # 7\n",
    "print(MathTools.is_even(10))   # True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f0fbc-4578-4468-a4e8-30d819f33d05",
   "metadata": {},
   "source": [
    "## 4️⃣ Special Methods (dunder methods like `__str__`, `__len__`, etc.)\n",
    "\n",
    "- Names start and end with **double underscores** (`__`).  \n",
    "- Define how objects behave with **built-in Python operations**.  \n",
    "- Examples:  \n",
    "  - `__str__` → string representation (`str(obj)`)  \n",
    "  - `__len__` → length of object (`len(obj)`)  \n",
    "  - `__add__` → addition (`obj1 + obj2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09d1e9a2-b552-41e7-a741-0067a687aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: Python Guide\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "class Book:\n",
    "    def __init__(self, title, pages):\n",
    "        self.title = title\n",
    "        self.pages = pages\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Book: {self.title}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.pages\n",
    "\n",
    "b = Book(\"Python Guide\", 500)\n",
    "print(b)         # Book: Python Guide   (uses __str__)\n",
    "print(len(b))    # 500                  (uses __len__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a98419-66cc-46c5-88e4-bc52e41842e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "class MathTools:\n",
    "    @staticmethod\n",
    "    def add(a, b):\n",
    "        return a + b\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_even(n):\n",
    "        return n % 2 == 0\n",
    "\n",
    "    @staticmethod\n",
    "    def add_is_even(a, b):\n",
    "        c = MathTools.add(a, b)   # call via class\n",
    "        return MathTools.is_even(c)\n",
    "\n",
    "print(MathTools.add(3, 4))        # 7\n",
    "print(MathTools.is_even(10))      # True\n",
    "print(MathTools.add_is_even(6, 4))  # True\n",
    "print(MathTools.add_is_even(5, 4))  # False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb549693-a3dd-4603-9698-f3712d867545",
   "metadata": {},
   "source": [
    "## 🔑 Key Point\n",
    "\n",
    "- `@staticmethod`s don’t get **self** or **cls**, so they can’t call other methods without explicitly qualifying them.  \n",
    "- If you want them to easily call each other, consider using **class methods** instead.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Why?\n",
    "\n",
    "- In a **normal method** (no decorator), Python automatically passes the **instance** (`self`) as the first argument.  \n",
    "- In a **`@classmethod`**, Python automatically passes the **class** (`cls`) as the first argument.  \n",
    "- In a **`@staticmethod`**, Python passes **nothing automatically** — you only get what you explicitly put in the argument list.  \n",
    "\n",
    "👉 So `cls` only works because of the `@classmethod` decorator.  \n",
    "Without it, `cls` is just a regular parameter name (like `x`, `y`, or `foo`) and won’t receive the class reference.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "```python\n",
    "class Demo:\n",
    "    def normal(self):\n",
    "        print(\"self is:\", self)\n",
    "\n",
    "    @classmethod\n",
    "    def class_method(cls):\n",
    "        print(\"cls is:\", cls)\n",
    "\n",
    "    @staticmethod\n",
    "    def static_method(x):\n",
    "        print(\"got:\", x)\n",
    "\n",
    "# Usage\n",
    "obj = Demo()\n",
    "obj.normal()           # self is: <__main__.Demo object>\n",
    "Demo.class_method()    # cls is: <class '__main__.Demo'>\n",
    "Demo.static_method(10) # got: 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "750e785d-14e3-40dd-831b-f989e92d8173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "class MathTools:\n",
    "    @classmethod\n",
    "    def add(cls, a, b):\n",
    "        return a + b\n",
    "    \n",
    "    @classmethod\n",
    "    def is_even(cls, n):\n",
    "        return n % 2 == 0\n",
    "\n",
    "    @classmethod\n",
    "    def add_is_even(cls, a, b):\n",
    "        c = cls.add(a, b)      # cleaner call\n",
    "        return cls.is_even(c)\n",
    "\n",
    "print(MathTools.add_is_even(5, 4))  # False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8fd50-e1f7-469c-89a7-5593091e835a",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef740a81-3ae5-4acd-bd90-eb31aa10c971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
