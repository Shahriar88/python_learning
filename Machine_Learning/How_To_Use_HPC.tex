% !TeX program = pdflatex
\documentclass[11pt]{article}
\usepackage{graphicx}


\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{makeidx} % for index
\usepackage{float}



\makeindex % enable index generation

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue
}

% ---------- Listings configuration: more visible code ----------
\definecolor{codebg}{RGB}{245,245,245}
\definecolor{codecomment}{RGB}{120,120,120}
\definecolor{codekeyword}{RGB}{0,0,150}
\definecolor{codestring}{RGB}{0,120,0}

\lstset{
  language=bash,
  basicstyle=\ttfamily\bfseries\small,
  keywordstyle=\color{codekeyword}\bfseries,
  commentstyle=\color{codecomment}\itshape,
  stringstyle=\color{codestring},
  backgroundcolor=\color{codebg},
  breaklines=true,
  columns=fullflexible,
  frame=single,
  rulecolor=\color{black},
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny,
  stepnumber=1,
  numbersep=8pt,
  xleftmargin=2em,
  framexleftmargin=1.5em,
  literate={\$}{{\textdollar}}1,
  float,
  floatplacement=htbp,
}


% New environment: codeblock = listings inside a minipage (no page break)
\lstnewenvironment{codeblock}[1][]%
{%
  \noindent
  \minipage{\linewidth}%
  \lstset{#1}%
}%
{%
  \endminipage\ignorespacesafterend%
}




\title{Getting Started with SLURM\index{SLURM} and Python Environments on an HPC Cluster\index{HPC cluster}}
\author{%
Md Shahriar Forhad\\
\texttt{shahriar.forhad.eee@gmail.com}\\
\url{https://shahriar88.github.io/}%
}
\date{\today}



\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Introduction}

This short tutorial explains how to:
\begin{itemize}
  \item Write a basic SLURM\index{SLURM!batch script} batch script for running a TensorFlow\index{TensorFlow} job.
  \item Understand key \texttt{\#SBATCH}\index{\#SBATCH directives} directives, including nodes, partitions, GPUs, and time limits.
  \item Use and extend a shared TensorFlow environment on the cluster.
  \item Create your own Python virtual environments\index{virtual environment} for CPU and GPU workloads\index{GPU workload}.
\end{itemize}

All examples are designed to be copied directly into a terminal or SLURM job script on a typical HPC system (e.g., a university cluster\index{university cluster}).

\subsection{Logging In to CRADLE}
\label{sec:login-cradle}

Once you have received your user account information, you can log in to the
CRADLE cluster\index{CRADLE} from a terminal or command-line prompt using your
University credentials.

\subsubsection*{Step 1: Connect to the University Network (VPN)}

If you are off campus, you must first connect to the University network using the
approved VPN\index{VPN} software. Install and start the VPN client, then log in
with your University credentials. Once the VPN connection is active, you can reach
the CRADLE login node.

\subsubsection*{Step 2: SSH into the Cluster}

To log in, use \texttt{ssh}\index{ssh} with your assigned username. In the
examples below, replace \texttt{username} with your actual cluster username.

\begin{codeblock}
ssh username@login.cradle.university.edu
\end{codeblock}

You will be prompted for your password:

\begin{codeblock}
username@login.cradle.university.edu's password:
\end{codeblock}

After you enter your password correctly, you should see a prompt indicating
that you are now on the cluster login node, similar to:

\begin{codeblock}
[username@login001 ~]$
\end{codeblock}


At this point you are logged into CRADLE and ready to work with SLURM, Python
environments, and the other tools described in this tutorial.
%______________________________________________________________________________________________

\section{A Minimal SLURM Job Script Example}
\label{sec:basic-script}

Listing~\ref{lst:basic-script} shows a basic SLURM job script that runs a TensorFlow test script in a shared environment\index{environment!shared}.

\begin{codeblock}[caption={Basic SLURM job script with TensorFlow environment},label={lst:basic-script}]
#!/bin/bash

# Job identification
#SBATCH --job-name=myFirstJob

# Standard output and error files (%j = job ID)
#SBATCH --output=myFirstJob.out.%j
#SBATCH --error=myFirstJob.err.%j

# Resource requests
#SBATCH -N 1
#SBATCH -p kimq
#SBATCH --gres=gpu:1
#SBATCH --time=1:00:00

# Display GPU assigned by SLURM
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

# Activate TensorFlow virtual environment
echo "Activating TensorFlow-2.6.2 environment"
source /shared/tensorflow-2.6.2/tf_env/bin/activate

# Run TensorFlow test script
echo "Running testTF.py"
python3 ~/testTFForSlurm/testTF.py

# Deactivate environment
echo "Deactivating TensorFlow-2.6.2 environment"
deactivate
\end{codeblock}

To submit this job to SLURM, save the script as \texttt{myFirstJob.slurm} (for example) and run:
\begin{codeblock}
sbatch myFirstJob.slurm
\end{codeblock}

\section{Understanding Key \texttt{\#SBATCH} Directives}
\label{sec:sbatch-directives}

The \texttt{\#SBATCH} lines at the top of the script tell SLURM what resources you want. They are not shell commands; they are scheduler directives\index{scheduler!directives}.

\subsection{Number of Nodes: \texttt{-N}\index{-N@\texttt{-N}}}

\begin{codeblock}
#SBATCH -N 1
\end{codeblock}

This requests \textbf{1 compute node}\index{node!compute}. A node is a physical machine that may contain many CPU cores, memory, and possibly GPUs.

More examples:\\
\begin{codeblock}
#SBATCH -N 1        # Single-node TensorFlow training
#SBATCH -N 2        # Distributed job across 2 nodes
#SBATCH -N 1-4      # SLURM may allocate anywhere from 1 to 4 nodes
\end{codeblock}

\subsection{Partition (Queue): \texttt{-p}\index{-p@\texttt{-p}}}

\begin{codeblock}
#SBATCH -p kimq
\end{codeblock}

The partition\index{partition} selects the \emph{group of nodes} (queue) your job will run on.

Typical partitions might be:
\begin{itemize}
  \item \texttt{compute} $\rightarrow$ Standard CPU nodes
  \item \texttt{bigmem} $\rightarrow$ High-memory nodes
  \item \texttt{gpu} or \texttt{kimq} $\rightarrow$ GPU-enabled nodes\index{GPU!partition}
\end{itemize}

Examples:\\
\begin{codeblock}
#SBATCH -p kimq     # Run on GPU nodes in the kimq partition
#SBATCH -p compute  # Run on CPU-only nodes
\end{codeblock}

\subsection{GPU Request: \texttt{--gres}\index{--gres@\texttt{--gres}}}

\begin{codeblock}
#SBATCH --gres=gpu:1
\end{codeblock}

This requests \textbf{1 GPU}\index{GPU!request} using the generic resource (GRES)\index{GRES} mechanism.

More examples:\\
\begin{codeblock}
#SBATCH --gres=gpu:1          # One GPU
#SBATCH --gres=gpu:2          # Two GPUs
#SBATCH --gres=gpu:a100:1     # One NVIDIA A100 GPU (if supported)
\end{codeblock}

\paragraph{Important: Partition and GPU must match.}

If you request GPUs but choose a CPU-only partition (e.g., \texttt{-p compute} with \texttt{--gres=gpu:1}), SLURM cannot satisfy the request and the job will stay pending or fail.

\subsection{Time Limit: \texttt{--time}\index{--time@\texttt{--time}}}

\begin{codeblock}
#SBATCH --time=1:00:00
\end{codeblock}

This sets a \textbf{wall-clock time limit}\index{time limit} of 1 hour for your job. When this time is reached, SLURM terminates the job.

More examples:\\
\begin{codeblock}
#SBATCH --time=00:30:00   # 30 minutes
#SBATCH --time=04:00:00   # 4 hours
#SBATCH --time=2-00:00:00 # 2 days
\end{codeblock}

\subsection{Summary Table of Core Directives}

\begin{table}[ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Directive & Controls        & Example      \\
\midrule
\texttt{-N}      & Number of nodes  & \texttt{-N 1}      \\
\texttt{-p}      & Hardware pool    & \texttt{-p kimq}   \\
\texttt{--gres}  & GPU count        & \texttt{gpu:1}     \\
\texttt{--time}  & Runtime limit    & \texttt{1:00:00}   \\
\bottomrule
\end{tabular}
\caption{Core SLURM directives and examples.}
\label{tab:directives}
\end{table}

\section{Why Use Python Environments on an HPC Cluster?}
\label{sec:why-env}

On HPC clusters, shared software environments (for example, a shared TensorFlow installation\index{TensorFlow!shared installation}) are usually:

\begin{itemize}
  \item Optimized for the cluster hardware (correct CUDA\index{CUDA}, cuDNN\index{cuDNN}, drivers).
  \item Read-only for users.
  \item Not modifiable by individual users.
\end{itemize}

You \emph{cannot} safely run:\\
\begin{codeblock}
pip install pandas
\end{codeblock}\\
inside a shared environment such as\\
\begin{codeblock}
 /shared/tensorflow-2.6.2/tf_env/
\end{codeblock}
\\because:

\begin{itemize}
  \item It is shared by everyone.
  \item You do not have write permission.
  \item Changes could break other users' jobs.
\end{itemize}

\textbf{Solution:} Clone the shared environment into your home directory, then customize the copy.


% ======================================================================================================
\section{Transferring Files to and from the CRADLE Cluster}
\label{sec:file-transfer}

When working on the CRADLE HPC cluster, you will often need to move files between
your local computer and the cluster. Common examples include:
\begin{itemize}
  \item Uploading scripts, data files, or configuration files to the cluster.
  \item Downloading results, logs, or trained models back to your local machine.
\end{itemize}

This section presents two recommended and commonly used methods:
\begin{enumerate}
  \item Secure Copy (\texttt{scp})
  \item Cloning repositories from GitHub
\end{enumerate}

\subsection{Using \texttt{scp} (Secure Copy)}
\label{subsec:scp}

The \texttt{scp}\index{scp} command allows you to securely transfer files between
your local machine and the cluster over SSH. It works on Linux, macOS, and Windows
(with PowerShell, Git Bash, or WSL).

\subsubsection{Transferring a Local File to the Cluster}

To copy a file from your local directory to your home directory on the cluster:

\begin{codeblock}
scp localFileInMyDirectory.txt \
    yourUserName@login.cradle.university.edu:~/destinationForYourFile/
\end{codeblock}

Explanation:
\begin{itemize}
  \item \texttt{localFileInMyDirectory.txt} is the file on your local computer.
  \item \texttt{yourUserName@login.cradle.university.edu} is your cluster login.
  \item \texttt{\~{}/destinationForYourFile/} is the target directory on the cluster.
\end{itemize}

If the destination directory does not exist, create it first after logging in:
\begin{codeblock}
mkdir -p ~/destinationForYourFile
\end{codeblock}

\subsubsection{Transferring a File from the Cluster to Your Local Machine}

To copy a file from the cluster back to your local directory:

\begin{codeblock}
scp yourUserName@login.cradle.university.edu:~/destinationForYourFile/localFileInMyDirectory.txt \
    .
\end{codeblock}

Here, the dot (\texttt{.}) represents your current local directory.

\subsection{Working with Directories in the Terminal}
\label{subsec:working-directories}

In both \texttt{scp} examples above, the file name refers to a file located in your
\emph{current working directory}. You can usually identify this directory by
looking at your terminal prompt.

For example:
\begin{codeblock}
[username@mycomputer:~/Desktop/test]$
\end{codeblock}

This indicates that your current directory is:
\begin{codeblock}
~/Desktop/test
\end{codeblock}

You can explicitly check your current directory using:

\paragraph{On Linux or macOS:}
\begin{codeblock}
pwd
\end{codeblock}

This prints something like:
\begin{codeblock}
/home/username/Desktop/test
\end{codeblock}

\paragraph{On Windows (PowerShell):}
\begin{codeblock}
cd
\end{codeblock}

Understanding your current directory is essential to avoid copying the wrong files
or encountering \texttt{file not found} errors.

\subsection{Cloning a GitHub Repository}
\label{subsec:git-clone}

Instead of manually transferring files, it is often easier to store your project
code in a GitHub repository and clone it directly on the cluster.

\subsubsection{Example: Cloning a Repository}

After logging into the cluster, run:

\begin{codeblock}
git clone https://github.com/emenriquez/testTFForSlurm
\end{codeblock}

This creates a new directory called \texttt{testTFForSlurm} containing the full
repository.

Advantages of using GitHub:
\begin{itemize}
  \item Version control and change tracking.
  \item Easy synchronization across multiple machines.
  \item Clean separation between code and large datasets.
\end{itemize}

\subsection{Recommended Best Practices}
\label{subsec:file-transfer-best-practices}

\begin{itemize}
  \item Use \texttt{scp} for small files, configuration files, and quick transfers.
  \item Use GitHub for project code and scripts.
  \item Avoid transferring very large datasets via \texttt{scp}; instead, place them
        directly on shared cluster storage if available.
  \item Keep your home directory organized (e.g., \texttt{projects/}, \texttt{data/}, \texttt{results/}).
\end{itemize}

These practices help ensure efficient workflows and reduce errors when working
between your local system and the CRADLE cluster.
% ======================================================================================================


\section{Cloning a Shared TensorFlow Environment}
\label{sec:clone-env}

\subsection{Step 1: Install the cloning tool (one time)}

\begin{codeblock}
pip3 install --user virtualenv-clone
\end{codeblock}

This:
\begin{itemize}
  \item Installs \texttt{virtualenv-clone}\index{virtualenv-clone} into your user space.
  \item Does not require admin/root access.
  \item Only needs to be run once.
\end{itemize}

\subsection{Step 2: Clone the shared environment}

\begin{codeblock}
virtualenv-clone /shared/tensorflow-2.6.2/tf_env ~/myEnvs/tf_env
\end{codeblock}

Here:
\begin{itemize}
  \item \texttt{/shared/tensorflow-2.6.2/tf\_env/} is the original, optimized, read-only TensorFlow environment.
  \item \texttt{\textasciitilde/myEnvs/tf\_env} is your private, writable copy.
\end{itemize}

What gets copied:
\begin{itemize}
  \item Python version.
  \item TensorFlow build (CUDA, cuDNN matched to cluster GPUs).
  \item All installed dependencies.
  \item Environment configuration tuned for the cluster.
\end{itemize}

Advantages:
\begin{itemize}
  \item Runs as fast as the shared environment.
  \item You can safely add or upgrade packages.
  \item No conflicts with other users.
\end{itemize}

\subsection{Step 3: Activate the cloned environment}

\begin{codeblock}
source ~/myEnvs/tf_env/bin/activate
\end{codeblock}

After activation:
\begin{itemize}
  \item \texttt{python} and \texttt{pip} point to your cloned environment.
  \item Any \texttt{pip install} affects only your copy.
\end{itemize}

The shell prompt typically changes to something like:\\
\begin{codeblock}
(tf_env) username@node \$
\end{codeblock}

\subsection{Step 4: Add new packages (example: \texttt{pandas})}

With the environment active:\\
\begin{codeblock}
pip3 install pandas
\end{codeblock}

What happens:
\begin{itemize}
  \item \texttt{pandas}\index{pandas} is installed only into \texttt{\textasciitilde/myEnvs/tf\_env}.
  \item TensorFlow and CUDA support remain intact.
  \item The customization is isolated and safe.
\end{itemize}

You can also add:\\
\begin{codeblock}
pip install scikit-learn matplotlib seaborn tqdm
\end{codeblock}

\subsection{Step 5: Use the cloned environment in your SLURM job}

Update the activation lines in your job script.

\paragraph{Old (shared environment):\\}
\begin{codeblock}
echo "Activating TensorFlow-2.6.2 environment"
source /shared/tensorflow-2.6.2/tf_env/bin/activate
\end{codeblock}

\paragraph{New (your cloned environment):\\}
\begin{codeblock}
echo "Activating my custom environment"
source ~/myEnvs/tf_env/bin/activate
\end{codeblock}

This ensures:
\begin{itemize}
  \item Your job sees TensorFlow and any extra packages (e.g., \texttt{pandas}).
  \item You avoid dependency errors at runtime.
  \item The behavior of your job is reproducible.
\end{itemize}

\subsection{Mental Model Summary}

\begin{table}[ht]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Piece               & Purpose                             \\
\midrule
Shared environment  & Fast, optimized, read-only          \\
Cloned environment  & Same speed, user-modifiable         \\
\texttt{virtualenv-clone} & Copies entire environment     \\
\texttt{pip install}      & Safe only in cloned environment \\
Job script          & Must activate your chosen environment \\
\bottomrule
\end{tabular}
\caption{Conceptual roles of shared and cloned environments.}
\label{tab:env-model}
\end{table}

\section{Creating Your Own Python Virtual Environment}
\label{sec:create-own-env}

Sometimes you want a completely independent Python environment\index{Python environment}. There are two main options; here we focus on standard \texttt{venv}\index{venv} and the cloned approach.

\subsection{Option A (Recommended for CPU-Only): Create from Scratch}

\subsubsection{Step 1: Choose a location}

Create a directory to hold your environments:\\
\begin{codeblock}
mkdir -p ~/myEnvs
\end{codeblock}

\subsubsection{Step 2: Create the virtual environment}

\begin{codeblock}
python3 -m venv ~/myEnvs/myenv
\end{codeblock}


This creates something like:\\
\begin{codeblock}
~/myEnvs/myenv/
|- bin/
|- lib/
`- pyvenv.cfg
\end{codeblock}


\subsubsection{Step 3: Activate the environment}

\begin{codeblock}
source ~/myEnvs/myenv/bin/activate
\end{codeblock}

Your shell prompt changes to:\\
\begin{codeblock}
(myenv) username@login $
\end{codeblock}

\subsubsection{Step 4: Upgrade \texttt{pip} (recommended)}

\begin{codeblock}
pip install --upgrade pip
\end{codeblock}

\subsubsection{Step 5: Install packages}

Example:\\
\begin{codeblock}
pip install tensorflow pandas numpy matplotlib
\end{codeblock}

Note: On clusters, installing TensorFlow this way may not include GPU support unless CUDA and drivers are correctly matched. For GPU workloads, the cloned shared environment (Option B) is often safer.

\subsubsection{Step 6: Use this environment in a SLURM job}

In your job script:\\
\begin{codeblock}
source ~/myEnvs/myenv/bin/activate
python train.py
\end{codeblock}

\subsection{Option B (Best for GPU): Clone an Optimized Shared Environment}

This is essentially the workflow described in Section~\ref{sec:clone-env}.

\begin{enumerate}[label=Step \arabic*:, leftmargin=*]
  \item Install the clone tool:\\
  \begin{codeblock}
pip install --user virtualenv-clone
  \end{codeblock}

  \item Clone the shared environment:\\
  \begin{codeblock}
virtualenv-clone /shared/tensorflow-2.6.2/tf_env ~/myEnvs/tf_env
  \end{codeblock}

  \item Activate it:\\
  \begin{codeblock}
source ~/myEnvs/tf_env/bin/activate
  \end{codeblock}

  \item Add packages:\\
  \begin{codeblock}
pip install pandas scikit-learn seaborn
  \end{codeblock}

  \item Update your SLURM job to activate:\\
  \begin{codeblock}
source ~/myEnvs/tf_env/bin/activate
  \end{codeblock}
\end{enumerate}

This preserves CUDA/cuDNN compatibility and performance while allowing you to customize the environment.

\section{Verifying Your Environment and Common Pitfalls}
\label{sec:verify-common}

\subsection{Verifying Python and GPU Visibility}

After activating an environment, verify what \texttt{python} and \texttt{pip} you are using:\\
\begin{codeblock}
which python
which pip
\end{codeblock}

For TensorFlow GPU detection:\\
\begin{codeblock}
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
\end{codeblock}

You should see at least one GPU listed when running on a GPU node with correct drivers and environment.

\subsection{Common Mistakes to Avoid}

\begin{itemize}
  \item Installing packages without activating the environment first.
  \item Using \texttt{pip install --user} inside a virtual environment.
  \item Mixing Conda environments and \texttt{venv} in the same workflow.
  \item Using the system Python (no virtual environment) for SLURM jobs.
  \item Requesting GPUs in a CPU-only partition.
\end{itemize}

\subsection{Quick Decision Guide}

\begin{table}[ht]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Situation                & Best choice         \\
\midrule
CPU-only work            & Option A (new \texttt{venv}) \\
GPU + TensorFlow         & Option B (clone shared env)  \\
Cluster-optimized builds & Option B                     \\
Full custom control      & Option A                     \\
\bottomrule
\end{tabular}
\caption{Choosing between creating and cloning environments.}
\label{tab:decision-guide}
\end{table}
%______________________________________________________________________________________________









\section{Using Micromamba and Portable Conda Environments on the HPC Cluster}
\label{sec:micromamba}

On many HPC clusters, including CRADLE, traditional Conda/Anaconda installations
are discouraged or restricted because they are large, modify shell startup
files aggressively, and may conflict with system libraries or CUDA drivers.
A lightweight and HPC-friendly alternative is \emph{Micromamba}\index{Micromamba},
a single static binary that implements the Conda ecosystem without requiring
a global base environment.

Micromamba allows you to:
\begin{itemize}
  \item Recreate a Conda environment from a portable YAML file exported on your laptop.
  \item Install packages into your home directory without administrator rights.
  \item Activate environments explicitly inside SLURM job scripts.
\end{itemize}

The overall workflow is:
\begin{enumerate}
  \item Export a portable Conda environment YAML on your local machine.
  \item Copy the YAML file to CRADLE.
  \item Install Micromamba in your home directory on CRADLE.
  \item Create a Micromamba environment from the YAML file.
  \item Activate the environment in your SLURM job scripts.
\end{enumerate}

\subsection{Exporting a Portable Conda Environment on Your PC}
\label{subsec:export-conda}

On your local machine where you already use Conda, start by creating
a portable YAML description of your environment. Suppose your local
environment is called \texttt{IR}\index{Conda environment!IR}.

First, activate it:\\
\begin{codeblock}
conda activate IR
\end{codeblock}

Then export a \emph{portable} environment file using \texttt{\-\-from\-history}:\\
\begin{codeblock}
conda env export --from-history > IR_portable.yml
\end{codeblock}

This creates a file \texttt{IR\_portable.yml} containing the channels and
high-level dependencies (without build hashes). It is much more likely to
recreate successfully on a different system such as CRADLE.

\subsection{Copying the YAML File to CRADLE}
\label{subsec:copy-yaml}

Use \texttt{scp}\index{scp} or an equivalent file transfer method to copy
\texttt{IR\_portable.yml} to your home directory on CRADLE. For example, from
your local terminal:\\

\begin{codeblock}
scp IR_portable.yml username@login.cradle.university.edu:~
\end{codeblock}

After logging into CRADLE, you should see the file in your home directory:\\
\begin{codeblock}
ls ~/IR_portable.yml
\end{codeblock}

\subsection{Installing Micromamba in Your Home Directory}
\label{subsec:install-micromamba}

Micromamba is distributed as a single binary, which makes it ideal for
installation in a user home directory (no root access required).

Create a directory for the binary:\\
\begin{codeblock}
mkdir -p ~/bin
cd ~/bin
\end{codeblock}

Download and unpack the latest Micromamba binary (Linux 64-bit):\\

\begin{codeblock}
curl -L https://micro.mamba.pm/api/micromamba/linux-64/latest \
  | tar -xvj bin/micromamba
\end{codeblock}

Add \texttt{\$HOME/bin} to your \texttt{PATH} in \texttt{\~/.bashrc}:\\
\begin{codeblock}
echo 'export PATH=$HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc
\end{codeblock}

Verify that \texttt{micromamba} is available:\\
\begin{codeblock}
micromamba --version
\end{codeblock}

\subsection{Creating a Micromamba Environment from the YAML File}
\label{subsec:create-micromamba-env}

Choose a directory where Micromamba will store its environments, for example:\\
\begin{codeblock}
mkdir -p ~/micromamba/envs
\end{codeblock}

Initialize Micromamba for Bash (one time):\\

\begin{codeblock}
micromamba shell init -s bash -p ~/micromamba
source ~/.bashrc
\end{codeblock}

Now create a new environment (for example, \texttt{IR\_NEW}) from your portable
YAML file:\\

\begin{codeblock}
micromamba create \
  -f ~/IR_portable.yml \
  -n IR_NEW \
  -p ~/micromamba/envs/IR_NEW
\end{codeblock}

Here:
\begin{itemize}
  \item \texttt{\~{}/IR\_portable.yml} is the YAML exported on your PC.
  \item \texttt{IR\_NEW} is the environment name.
  \item \texttt{\~{}/micromamba/envs/IR\_NEW} is the full path Micromamba uses.
\end{itemize}

\subsection{Activating the Environment on CRADLE}
\label{subsec:activate-micromamba}

Once created, you can activate the environment on the CRADLE login node:\\

\begin{codeblock}
micromamba activate ~/micromamba/envs/IR_NEW
\end{codeblock}

After activation:
\begin{itemize}
  \item \texttt{python} and \texttt{pip} refer to the Micromamba environment.
  \item Any installs via \texttt{pip} or \texttt{micromamba install} affect only that environment.
\end{itemize}

You can verify:\\
\begin{codeblock}
which python
python --version
pip list
\end{codeblock}

\subsection{Using a Micromamba Environment in a SLURM Job}
\label{subsec:micromamba-slurm}

In a SLURM job script, you should explicitly set the Micromamba root and
activate the environment before running Python code. For example:

\begin{codeblock}
#!/bin/bash
#SBATCH --job-name=ir_gpu_job
#SBATCH --output=ir_gpu_job.out.%j
#SBATCH --error=ir_gpu_job.err.%j
#SBATCH -N 1
#SBATCH -p kimq             # GPU partition
#SBATCH --gres=gpu:1        # 1 GPU
#SBATCH --time=02:00:00     # 2 hours

# Ensure Micromamba root is defined
export MAMBA_ROOT_PREFIX=$HOME/micromamba

# Load shell initialization (if micromamba shell init was used)
source $HOME/.bashrc

echo "Host: $(hostname)"
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

echo "Activating Micromamba environment IR_NEW"
micromamba activate $HOME/micromamba/envs/IR_NEW

echo "Checking TensorFlow GPU visibility"
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"

echo "Running training script"
python ~/projects/train_ir_model.py
\end{codeblock}

This pattern ensures that:
\begin{itemize}
  \item The correct Micromamba environment is active inside the job.
  \item GPU visibility can be checked via TensorFlow (or PyTorch).
  \item The job is reproducible and does not depend on the login-node Conda setup.
\end{itemize}

\subsection{Verifying GPU Support Inside the Environment}
\label{subsec:verify-gpu}

To confirm that your Micromamba environment has working GPU support for
TensorFlow (or another framework), you can run:\\

\begin{codeblock}
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
\end{codeblock}

On a GPU node with correct drivers and CUDA libraries, this should list one or
more GPU devices. You can also check the low-level GPU visibility with:\\
\begin{codeblock}
nvidia-smi
\end{codeblock}

\subsection{Best Practices and Common Pitfalls for Micromamba on HPC}
\label{subsec:micromamba-best-practices}

\textbf{Best practices:}
\begin{itemize}
  \item Export environments from Conda using \texttt{--from-history} or \texttt{--no-builds}
        to avoid hardware-specific build strings.
  \item Use Micromamba to recreate the environment on the cluster, not full Anaconda.
  \item Keep all Micromamba files under \texttt{\~{}/micromamba} or a dedicated directory
        in your home space.
  \item Activate environments explicitly inside SLURM job scripts.
\end{itemize}

\textbf{Common pitfalls:}
\begin{itemize}
  \item Installing full Anaconda on an HPC login node (disk usage and conflicts).
  \item Relying on Conda's \texttt{base} environment in batch jobs.
  \item Mixing Conda and Micromamba environments in the same workflow.
  \item Installing GPU frameworks with incompatible \texttt{cudatoolkit} versions;
        always check which CUDA versions the cluster supports.
\end{itemize}

Using Micromamba with a portable Conda YAML exported from your local machine
gives you a reproducible, HPC-safe way to manage Python environments on
CRADLE while respecting cluster policies and maximizing compatibility with
the installed NVIDIA drivers and CUDA libraries.
%______________________________________________________________________________________________

















\section{Worked Examples: Environments and SLURM Job Scripts}
\label{sec:examples}

This section provides concrete, copy-pasteable examples covering:
\begin{enumerate}
  \item Creating a new environment.
  \item Cloning a shared TensorFlow environment.
  \item Selecting (activating) an environment.
  \item Installing extra libraries.
  \item Checking and verifying resources.
  \item Writing SLURM scripts for different resource requirements.
\end{enumerate}

% -------------------- Environments --------------------

\subsection{Creating a New CPU-Only Environment}
\label{subsec:create-env-example}

\begin{codeblock}
# 1. Create a directory to store your environments
mkdir -p ~/myEnvs

# 2. Create a new CPU-only Python virtual environment
python3 -m venv ~/myEnvs/cpu_env

# 3. Activate the environment
source ~/myEnvs/cpu_env/bin/activate

# 4. Upgrade pip
pip install --upgrade pip

# 5. Install some common libraries
pip install numpy scipy pandas matplotlib scikit-learn

# 6. When finished, deactivate
deactivate
\end{codeblock}

\subsection{Cloning a Shared GPU-Optimized TensorFlow Environment}
\label{subsec:clone-env-example}

\begin{codeblock}
# 1. Install the clone tool (only once per user)
pip3 install --user virtualenv-clone

# 2. Create a directory to store your environments (if not already created)
mkdir -p ~/myEnvs

# 3. Clone the shared TensorFlow environment into your home directory
virtualenv-clone /shared/tensorflow-2.6.2/tf_env ~/myEnvs/tf_env

# 4. Activate the cloned environment
source ~/myEnvs/tf_env/bin/activate

# 5. Install extra libraries needed for your project
pip install pandas scikit-learn matplotlib seaborn tqdm

# 6. Deactivate when done
deactivate
\end{codeblock}

\subsection{Selecting an Environment in a Job Script}
\label{subsec:select-env-example}

\paragraph{Activate the CPU-only environment:\\}
\begin{codeblock}
# Inside your SLURM job script
echo "Activating CPU-only environment"
source ~/myEnvs/cpu_env/bin/activate
\end{codeblock}

\paragraph{Activate the cloned GPU TensorFlow environment:\\}
\begin{codeblock}
# Inside your SLURM job script
echo "Activating GPU TensorFlow environment"
source ~/myEnvs/tf_env/bin/activate
\end{codeblock}

\subsection{Installing Libraries After Environment Activation}
\label{subsec:install-libs-example}

\paragraph{Inside the CPU-only environment:\\}
\begin{codeblock}
source ~/myEnvs/cpu_env/bin/activate
pip install jupyter notebook ipykernel
deactivate
\end{codeblock}

\paragraph{Inside the cloned GPU environment:\\}
\begin{codeblock}
source ~/myEnvs/tf_env/bin/activate
pip install opencv-python pillow tensorboard
deactivate
\end{codeblock}

% -------------------- Checking resources --------------------

\subsection{Checking and Verifying Resources}
\label{subsec:check-resources}

This subsection shows how to verify what resources are available on the cluster
and what resources SLURM\index{SLURM!resources} has actually allocated to your jobs.

\subsubsection{Check Available Partitions and Nodes}

To see which partitions and nodes exist and their current status:

\begin{codeblock}
# Show all partitions and their status
sinfo
\end{codeblock}

For more detailed information about each node:

\begin{codeblock}
# Detailed node information
scontrol show nodes
\end{codeblock}

\subsubsection{Check Your Jobs}

To see all of your jobs in the queue:

\begin{codeblock}
# Show jobs for your user
squeue -u $USER
\end{codeblock}

To inspect a specific job by its job ID:

\begin{codeblock}
# Show a specific job
squeue -j <JOBID>
\end{codeblock}

\subsubsection{Inspect Resources Allocated by SLURM}

To see precisely what resources were allocated to a job:

\begin{codeblock}
# Show full job allocation details
scontrol show job <JOBID>
\end{codeblock}

Look for fields such as \texttt{NumNodes}, \texttt{NumCPUs}, \texttt{Gres},
and \texttt{NodeList}.

\subsubsection{Check Resources Inside a Job Script}

Inside a running job, you can query SLURM environment variables\index{SLURM!environment variables}
to see what was allocated:

\begin{codeblock}
echo "Job ID:            $SLURM_JOB_ID"
echo "Nodes allocated:   $SLURM_NNODES"
echo "Node list:         $SLURM_NODELIST"
echo "CPUs per node:     $SLURM_CPUS_ON_NODE"
echo "Tasks per node:    $SLURM_TASKS_PER_NODE"
echo "Job partition:     $SLURM_JOB_PARTITION"
\end{codeblock}

\subsubsection{Check GPU Allocation}

For GPU jobs, it is important to verify which GPUs are visible:

\begin{codeblock}
# GPUs assigned by SLURM (logical IDs)
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

# List visible GPUs and utilization
nvidia-smi
\end{codeblock}

\subsubsection{Check CPU and Memory Usage}

You can inspect CPU and memory characteristics on the node:

\begin{codeblock}
# CPU info
lscpu

# Memory info
free -h

# Per-node memory summary (first lines of /proc/meminfo)
cat /proc/meminfo | head
\end{codeblock}

\subsubsection{Check Resources Used by Completed Jobs}

After a job finishes, accounting information can be retrieved with
\texttt{sacct}\index{sacct}:

\begin{codeblock}
# Accounting info for a finished job
sacct -j <JOBID> --format=JobID,JobName,Partition,AllocCPUS,Elapsed,MaxRSS,State
\end{codeblock}

This shows how many CPUs were allocated, how long the job ran, peak memory
usage (\texttt{MaxRSS}), and the final state (e.g., COMPLETED, FAILED, TIMEOUT).

\subsubsection{Minimal Resource Debug Job}

The following SLURM script does nothing except report which resources it
received. It is useful when testing new partitions or debugging allocation
issues:

\begin{codeblock}
#!/bin/bash
#SBATCH --job-name=resource_debug
#SBATCH -N 1
#SBATCH -p kimq
#SBATCH --gres=gpu:1
#SBATCH --time=00:10:00

echo "Job ID:        $SLURM_JOB_ID"
echo "Node list:     $SLURM_NODELIST"
echo "CPUs/node:     $SLURM_CPUS_ON_NODE"
echo "GPUs visible:  $CUDA_VISIBLE_DEVICES"

echo "==== nvidia-smi ===="
nvidia-smi

echo "==== lscpu ===="
lscpu

echo "==== memory ===="
free -h
\end{codeblock}






\begin{table}[H]
\centering
\small
\renewcommand{\arraystretch}{1.15}
\setlength{\tabcolsep}{5pt}

\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Aspect} 
& \textbf{CPU} 
& \textbf{Single GPU} 
& \textbf{DataParallel (DP)} 
& \textbf{DistributedDataParallel (DDP)} \\
\midrule
Hardware & CPU cores only & 1 GPU & Multiple GPUs (1 node) & Multiple GPUs (1+ nodes) \\
Processes & 1 & 1 & 1 & 1 per GPU \\
GPU usage & None & Full single GPU & Automatic batch split & Explicit per-rank GPU \\
Multi-node support & No & No & No & Yes \\
Performance & Slowest & Fast & Medium & Fastest \\
Scalability & Poor & Limited & Limited & Excellent \\
Communication & None & None & CPU-based gather/scatter & NCCL all-reduce \\
Gradient sync & N/A & N/A & Implicit (inefficient) & Explicit (efficient) \\
Memory efficiency & High & Medium & Low (replication overhead) & High \\
Fault isolation & N/A & N/A & Poor & Good \\
Code complexity & Very low & Low & Low & Medium \\
Launch method & \texttt{python} & \texttt{python} & \texttt{python} & \texttt{torchrun} / \texttt{srun} \\
Best use case & Debugging, tests & Small models & Prototyping & Large-scale training \\
Recommended for HPC & No & Limited & No & Yes \\
\bottomrule
\end{tabular}%
}

\caption{Comparison of CPU, single-GPU, DataParallel (DP), and DistributedDataParallel (DDP) execution modes in PyTorch.}
\label{tab:cpu-gpu-dp-ddp}
\end{table}





% ======================================================================================================
\section{PyTorch DataParallel (DP) on SLURM}
\label{sec:dp}

This section describes PyTorch \textbf{DataParallel (DP)}, a simpler multi-GPU
approach compared to Distributed Data Parallel (DDP).
DP uses a \emph{single Python process} and automatically splits each batch across
multiple GPUs on the same node.

\textbf{Important:} DP is suitable for quick experiments on a \emph{single node}
only. For serious training, large models, or multi-node jobs, prefer DDP
(Section~\ref{sec:ddp}).

\subsection{When to Use DataParallel}
\begin{itemize}[leftmargin=*]
  \item Single node with multiple GPUs.
  \item Prototyping or debugging multi-GPU behavior.
  \item Minimal code changes from single-GPU training.
\end{itemize}

\subsection{When \emph{Not} to Use DataParallel}
\begin{itemize}[leftmargin=*]
  \item Multi-node training (DP does not scale across nodes).
  \item Large models or long training runs (DDP is faster and more stable).
  \item When precise performance benchmarking matters.
\end{itemize}

\subsection{Minimal DataParallel Training Skeleton}
Save as \texttt{train\_dp.py}. This example assumes a single node with multiple GPUs.

\begin{codeblock}[language=python,caption={Minimal PyTorch DataParallel training skeleton (\texttt{train\_dp.py})}]
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

def build_model():
    # Replace with your FCN / VAE model
    return nn.Identity()

def build_dataset(split="train"):
    # Replace with your dataset
    raise NotImplementedError

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = build_model()

    if torch.cuda.device_count() > 1:
        print(f"Using DataParallel on {torch.cuda.device_count()} GPUs")
        model = nn.DataParallel(model)

    model = model.to(device)

    train_ds = build_dataset("train")
    train_loader = DataLoader(
        train_ds,
        batch_size=16,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        drop_last=True,
    )

    optim = torch.optim.Adam(model.parameters(), lr=1e-3)

    model.train()
    for epoch in range(5):
        for batch in train_loader:
            X, Y = batch
            X = X.to(device, non_blocking=True)
            Y = Y.to(device, non_blocking=True)

            optim.zero_grad(set_to_none=True)
            Yhat = model(X)
            loss = torch.mean((Yhat - Y) ** 2)
            loss.backward()
            optim.step()

        print(f"Epoch {epoch} done. loss={loss.item():.6f}")

if __name__ == "__main__":
    main()
\end{codeblock}

\subsection{SLURM Script for DataParallel (single node)}
DP requires \textbf{one Python process only}. Do \emph{not} use \texttt{torchrun}
or multiple tasks.

\begin{codeblock}[caption={SLURM script for DataParallel (\texttt{dp\_1node.slurm})}]
#!/bin/bash
#SBATCH --job-name=dp_1node
#SBATCH --output=dp_1node.out.%j
#SBATCH --error=dp_1node.err.%j
#SBATCH -N 1
#SBATCH -p kimq
#SBATCH --gres=gpu:4
#SBATCH --cpus-per-task=8
#SBATCH --time=04:00:00

echo "Host: \$(hostname)"
echo "CUDA_VISIBLE_DEVICES: \$CUDA_VISIBLE_DEVICES"

source ~/myEnvs/tf_env/bin/activate

export OMP_NUM_THREADS=\$SLURM_CPUS_PER_TASK

# Single process only
python train_dp.py

deactivate
\end{codeblock}

\subsection{DP vs DDP: Practical Comparison}

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Feature & DataParallel (DP) & DistributedDataParallel (DDP) \\
\midrule
Processes & 1 total & 1 per GPU \\
Multi-node support & No & Yes \\
Performance & Slower & Faster \\
Scalability & Limited & Excellent \\
Failure isolation & Poor & Good \\
Recommended for & Prototyping & Production / research \\
\bottomrule
\end{tabular}
\caption{Comparison between DataParallel and DistributedDataParallel.}
\end{table}

\subsection{Common DP Pitfalls}
\begin{itemize}[leftmargin=*]
  \item Do not combine DP with \texttt{torchrun} or \texttt{srun -n > 1}.
  \item Expect slower scaling beyond 2â€“4 GPUs.
  \item Avoid DP for multi-node jobs.
  \item Always move the model to GPU \emph{after} wrapping with \texttt{DataParallel}.
\end{itemize}

\subsection{Rule of Thumb}
\begin{itemize}[leftmargin=*]
  \item \textbf{1 node, quick test} $\rightarrow$ DataParallel
  \item \textbf{Anything serious or multi-node} $\rightarrow$ DDP
\end{itemize}
% ======================================================================================================



% ======================================================================================================
\section{PyTorch Distributed Data Parallel (DDP) on SLURM}
\label{sec:ddp}

This section shows a minimal, practical recipe for PyTorch DDP on an HPC cluster.
DDP runs one Python process per GPU and synchronizes gradients efficiently.

\subsection{What You Need (Checklist)}
\begin{itemize}[leftmargin=*]
  \item Request \textbf{multiple GPUs} in SLURM (e.g., \texttt{--gres=gpu:4}).
  \item Launch with \textbf{\texttt{torchrun}} (recommended) or \texttt{srun} + environment variables.
  \item In Python: call \texttt{init\_process\_group}, set the per-rank GPU, wrap model with \texttt{DistributedDataParallel}.
  \item Use \texttt{DistributedSampler} for training (and validation if you want correct global metrics quickly).
\end{itemize}

\subsection{Minimal DDP Training Skeleton (single file)}
Save as \texttt{train\_ddp.py}. Replace \texttt{build\_model()}, \texttt{build\_dataset()}, and the loss/optimizer with your own code.

\begin{codeblock}[language=python,caption={Minimal PyTorch DDP training skeleton (\texttt{train\_ddp.py})}]
import os
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler

# -------------------------
# 1) DDP setup / teardown
# -------------------------
def ddp_setup():
    """
    Works with: torchrun (sets RANK, LOCAL_RANK, WORLD_SIZE, MASTER_ADDR, MASTER_PORT).
    """
    if dist.is_available() and not dist.is_initialized():
        dist.init_process_group(backend="nccl")  # GPUs -> nccl

    rank = int(os.environ.get("RANK", "0"))
    local_rank = int(os.environ.get("LOCAL_RANK", "0"))
    world_size = int(os.environ.get("WORLD_SIZE", "1"))

    if torch.cuda.is_available():
        torch.cuda.set_device(local_rank)
        device = torch.device(f"cuda:{local_rank}")
    else:
        device = torch.device("cpu")

    return device, rank, local_rank, world_size

def ddp_cleanup():
    if dist.is_available() and dist.is_initialized():
        dist.destroy_process_group()

# -------------------------
# 2) Your builders (replace)
# -------------------------
def build_model():
    # Replace with your FCN/VAE model builder
    return nn.Identity()

def build_dataset(split="train"):
    # Replace with your dataset (e.g., CleanImageFolder / PairedImageFolder)
    # Must return a torch.utils.data.Dataset
    raise NotImplementedError

# -------------------------
# 3) Main training loop
# -------------------------
def main():
    device, rank, local_rank, world = ddp_setup()
    is_ddp = (world > 1)

    model = build_model().to(device)
    if is_ddp:
        model = DDP(model, device_ids=[local_rank], output_device=local_rank)

    train_ds = build_dataset("train")
    train_sampler = DistributedSampler(
        train_ds, num_replicas=world, rank=rank, shuffle=True
    ) if is_ddp else None

    train_loader = DataLoader(
        train_ds,
        batch_size=16,
        shuffle=(train_sampler is None),
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True,
        drop_last=True,
        persistent_workers=True,
    )

    optim = torch.optim.Adam(model.parameters(), lr=1e-3)

    for epoch in range(5):
        if is_ddp:
            train_sampler.set_epoch(epoch)  # IMPORTANT: different shuffle each epoch

        model.train()
        for batch in train_loader:
            # Example batch: (X, Y) for denoising
            X, Y = batch
            X = X.to(device, non_blocking=True)
            Y = Y.to(device, non_blocking=True)

            optim.zero_grad(set_to_none=True)
            Yhat = model(X)
            loss = torch.mean((Yhat - Y) ** 2)  # replace with your loss
            loss.backward()
            optim.step()

        # Print only from rank 0 (avoids duplicated logs)
        if rank == 0:
            print(f"Epoch {epoch} done. loss={loss.item():.6f}")

    ddp_cleanup()

if __name__ == "__main__":
    main()
\end{codeblock}

\subsection{SLURM Script for DDP (single node, 4 GPUs)}
This launches \textbf{one process per GPU} on the same node.

\begin{codeblock}[caption={SLURM script for DDP on 1 node (\texttt{ddp\_1node.slurm})}]
#!/bin/bash
#SBATCH --job-name=ddp_1node
#SBATCH --output=ddp_1node.out.%j
#SBATCH --error=ddp_1node.err.%j
#SBATCH -N 1
#SBATCH -p kimq
#SBATCH --gres=gpu:4
#SBATCH --cpus-per-task=4
#SBATCH --time=04:00:00

echo "Host: \$(hostname)"
echo "Node list: \$SLURM_NODELIST"
echo "CUDA_VISIBLE_DEVICES: \$CUDA_VISIBLE_DEVICES"

source ~/myEnvs/tf_env/bin/activate

# One process per GPU:
export OMP_NUM_THREADS=\$SLURM_CPUS_PER_TASK

# torchrun is recommended.
# --standalone works for single-node. For multi-node, use rendezvous settings shown below.
srun torchrun \
  --standalone \
  --nproc_per_node=4 \
  train_ddp.py

deactivate
\end{codeblock}

\subsection{SLURM Script for DDP (2 nodes, 4 GPUs per node)}
For multi-node DDP, you need a rendezvous endpoint (master address/port).
A common pattern is to use the first node in \texttt{\$SLURM\_NODELIST} as the master.

\begin{codeblock}[caption={SLURM script for DDP on 2 nodes (\texttt{ddp\_2node.slurm})}]
#!/bin/bash
#SBATCH --job-name=ddp_2node
#SBATCH --output=ddp_2node.out.%j
#SBATCH --error=ddp_2node.err.%j
#SBATCH -N 2
#SBATCH -p kimq
#SBATCH --gres=gpu:4
#SBATCH --cpus-per-task=4
#SBATCH --time=06:00:00

source ~/myEnvs/tf_env/bin/activate

# Pick master node and port
MASTER_ADDR=\$(scontrol show hostnames "\$SLURM_NODELIST" | head -n 1)
MASTER_PORT=29500

echo "MASTER_ADDR=\$MASTER_ADDR"
echo "MASTER_PORT=\$MASTER_PORT"

export OMP_NUM_THREADS=\$SLURM_CPUS_PER_TASK

# Total processes = nodes * gpus_per_node = 2 * 4 = 8
srun torchrun \
  --nnodes=2 \
  --nproc_per_node=4 \
  --rdzv_backend=c10d \
  --rdzv_endpoint=\$MASTER_ADDR:\$MASTER_PORT \
  train_ddp.py

deactivate
\end{codeblock}

\subsection{Common DDP Pitfalls (Fast Debug List)}
\begin{itemize}[leftmargin=*]
  \item \textbf{Do not use \texttt{DataParallel} with DDP.} Use one or the other.
  \item \textbf{Call \texttt{DistributedSampler(...)} for training} and \textbf{call \texttt{set\_epoch(epoch)}} each epoch.
  \item \textbf{Set the per-process GPU} via \texttt{LOCAL\_RANK} and \texttt{torch.cuda.set\_device(local\_rank)}.
  \item \textbf{Print/checkpoint only on rank 0} to avoid duplicates and file corruption.
  \item If validation metrics must be global, either:
    \begin{itemize}
      \item use \texttt{DistributedSampler} for validation and \texttt{all\_reduce} metric sums, or
      \item run validation only on rank 0 (simpler but slower).
    \end{itemize}
\end{itemize}
% ======================================================================================================




% -------------------- Job templates --------------------

\subsection{SLURM Job Templates for Different Resource Requirements}
\label{subsec:slurm-templates}

This subsection shows full job scripts for common use cases.


% ======================================================================================================
\subsection{Example Commands for File Transfer}
\label{subsec:file-transfer-examples}

This subsection provides practical, copy-pasteable examples for transferring
files and directories between your local machine and the CRADLE cluster.
These commands are typically run from a local terminal, not inside a SLURM job.

\subsubsection{Copying a Single File from Local Machine to CRADLE}

\begin{codeblock}
scp my_script.py \
    yourUserName@login.cradle.university.edu:~/projects/
\end{codeblock}

This copies \texttt{my\_script.py} from your current local directory into
\texttt{\~{}/projects/} on the cluster.

\subsubsection{Copying a Single File from CRADLE to Local Machine}

\begin{codeblock}
scp yourUserName@login.cradle.university.edu:~/projects/results.txt \
    .
\end{codeblock}

The dot (\texttt{.}) represents your current local directory.

\subsubsection{Copying an Entire Directory to CRADLE}

To recursively copy a directory (for example, a project folder):

\begin{codeblock}
scp -r myProjectFolder/ \
    yourUserName@login.cradle.university.edu:~/projects/
\end{codeblock}

This creates \texttt{\~{}/projects/myProjectFolder/} on the cluster.

\subsubsection{Copying an Entire Directory from CRADLE to Local Machine}

\begin{codeblock}
scp -r yourUserName@login.cradle.university.edu:~/projects/myProjectFolder/ \
    ~/Desktop/
\end{codeblock}

This downloads the entire project directory to your local desktop.

\subsubsection{Transferring Job Output After Completion}

After a SLURM job finishes, you often want to retrieve output files such as logs
or trained models.

\begin{codeblock}
scp yourUserName@login.cradle.university.edu:~/slurm_outputs/myJob.out.123456 \
    .
\end{codeblock}

Here, \texttt{123456} is the SLURM job ID.

\subsubsection{Copying Files Using Wildcards}

You can use wildcards to transfer multiple files at once:

\begin{codeblock}
scp yourUserName@login.cradle.university.edu:~/slurm_outputs/*.out.* \
    .
\end{codeblock}

This copies all SLURM output files in the directory.

\subsubsection{Recommended Workflow with SLURM Jobs}

A typical workflow looks like this:
\begin{enumerate}
  \item Copy scripts to CRADLE using \texttt{scp} or \texttt{git clone}.
  \item Submit the job using \texttt{sbatch}.
  \item Monitor the job with \texttt{squeue}.
  \item Copy results back to your local machine using \texttt{scp}.
\end{enumerate}

\paragraph{Important Note:}
File transfers should generally be performed from the \emph{login node},
not from inside running compute jobs, unless explicitly required.
% ======================================================================================================


\subsubsection{Single Node, CPU-Only Job}
\label{subsubsec:cpu-only-job}

\begin{codeblock}
#!/bin/bash
#SBATCH --job-name=cpu_only_job
#SBATCH --output=cpu_only_job.out.%j
#SBATCH --error=cpu_only_job.err.%j
#SBATCH -N 1
#SBATCH -p compute        # CPU-only partition
#SBATCH --time=01:00:00   # 1 hour

echo "Host: $(hostname)"
echo "Using CPU-only environment"
source ~/myEnvs/cpu_env/bin/activate

echo "Running CPU-only Python script"
python ~/projects/my_cpu_script.py

echo "Deactivating environment"
deactivate
\end{codeblock}

\subsubsection{Single Node, Single GPU Job}
\label{subsubsec:single-gpu-job}

\begin{codeblock}
#!/bin/bash
#SBATCH --job-name=single_gpu_job
#SBATCH --output=single_gpu_job.out.%j
#SBATCH --error=single_gpu_job.err.%j
#SBATCH -N 1
#SBATCH -p kimq           # GPU partition
#SBATCH --gres=gpu:1      # 1 GPU
#SBATCH --time=02:00:00   # 2 hours

echo "Host: $(hostname)"
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

echo "Activating GPU TensorFlow environment"
source ~/myEnvs/tf_env/bin/activate

echo "Running single-GPU training script"
python ~/projects/train_single_gpu.py

echo "Deactivating environment"
deactivate
\end{codeblock}

\subsubsection{Single Node, Multiple GPUs (e.g., 4 GPUs)}
\label{subsubsec:multi-gpu-single-node}

\begin{codeblock}
#!/bin/bash
#SBATCH --job-name=multi_gpu_job
#SBATCH --output=multi_gpu_job.out.%j
#SBATCH --error=multi_gpu_job.err.%j
#SBATCH -N 1
#SBATCH -p kimq            # GPU partition
#SBATCH --gres=gpu:4       # 4 GPUs on the same node
#SBATCH --time=04:00:00    # 4 hours

echo "Host: $(hostname)"
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

echo "Activating GPU TensorFlow environment"
source ~/myEnvs/tf_env/bin/activate

echo "Running multi-GPU training script (e.g., data parallel)"
python ~/projects/train_multi_gpu.py --num-gpus 4

echo "Deactivating environment"
deactivate
\end{codeblock}

\subsubsection{Multiple Nodes, CPU-Only (e.g., MPI or Distributed CPU Job)}
\label{subsubsec:multi-node-cpu}

\begin{codeblock}
#!/bin/bash
#SBATCH --job-name=cpu_multi_node_job
#SBATCH --output=cpu_multi_node_job.out.%j
#SBATCH --error=cpu_multi_node_job.err.%j
#SBATCH -N 2               # 2 nodes
#SBATCH -p compute         # CPU-only partition
#SBATCH --time=03:00:00    # 3 hours

echo "SLURM_NODELIST: $SLURM_NODELIST"
echo "SLURM_NNODES:   $SLURM_NNODES"

echo "Activating CPU-only environment"
source ~/myEnvs/cpu_env/bin/activate

# Example: launch an MPI or distributed job
# (replace with your actual launcher, e.g., srun mpirun, torchrun, etc.)
echo "Running distributed CPU job"
srun python ~/projects/distributed_cpu_job.py

echo "Deactivating environment"
deactivate
\end{codeblock}

\subsubsection{Multiple Nodes, Multiple GPUs (Distributed GPU Training)}
\label{subsubsec:multi-node-multi-gpu}

\begin{codeblock}
#!/bin/bash
#SBATCH --job-name=multi_node_multi_gpu
#SBATCH --output=multi_node_multi_gpu.out.%j
#SBATCH --error=multi_node_multi_gpu.err.%j
#SBATCH -N 2                # 2 nodes
#SBATCH -p kimq             # GPU partition
#SBATCH --gres=gpu:4        # 4 GPUs per node (total 8 GPUs)
#SBATCH --time=06:00:00     # 6 hours

echo "SLURM_NODELIST: $SLURM_NODELIST"
echo "SLURM_NNODES:   $SLURM_NNODES"
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

echo "Activating GPU TensorFlow environment"
source ~/myEnvs/tf_env/bin/activate

# Example using srun to launch a distributed training job
# Replace with your actual distributed launcher (e.g., torchrun, horovodrun)
echo "Running multi-node, multi-GPU training"
srun python ~/projects/train_distributed_gpu.py \
     --nodes $SLURM_NNODES \
     --gpus-per-node 4

echo "Deactivating environment"
deactivate
\end{codeblock}

% -------------------- Quick mapping --------------------

\subsection{Quick Mapping: Which Example to Use?}
\label{subsec:which-example}

\begin{itemize}
  \item \textbf{Create new CPU environment}: Listing in Section~\ref{subsec:create-env-example}.
  \item \textbf{Clone shared GPU TF environment}: Listing in Section~\ref{subsec:clone-env-example}.
  \item \textbf{CPU-only job}: Section~\ref{subsubsec:cpu-only-job}.
  \item \textbf{Single-GPU job}: Section~\ref{subsubsec:single-gpu-job}.
  \item \textbf{Single node, multiple GPUs}: Section~\ref{subsubsec:multi-gpu-single-node}.
  \item \textbf{Multi-node CPU job}: Section~\ref{subsubsec:multi-node-cpu}.
  \item \textbf{Multi-node, multi-GPU job}: Section~\ref{subsubsec:multi-node-multi-gpu}.
\end{itemize}
%______________________________________________________________________________________________

\section{Summary}
\label{sec:summary}

In this tutorial, you have seen:

\begin{itemize}
  \item How to write and submit a basic SLURM job script that runs a TensorFlow program.
  \item How to interpret key \texttt{\#SBATCH} directives: number of nodes, partition, GPU requests, and time limits.
  \item Why shared environments on HPC clusters are read-only and how to safely extend them by cloning to your home directory.
  \item How to create your own Python virtual environments from scratch and how to connect them to SLURM job scripts.
\end{itemize}

This workflow lets you run fast, GPU-enabled jobs using cluster-optimized software while keeping your custom Python packages isolated and safe.

\printindex
\end{document}
