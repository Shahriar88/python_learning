\documentclass[11pt]{article}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=1.2em}
\usepackage{bm}
\usepackage{tabularx}  % Add this in your preamble

\usepackage{array}
\usepackage{booktabs}
\usepackage{makecell} % better cell wrapping
\usepackage{multirow}
\usepackage{setspace}
\usepackage{xcolor} % for more color names like olive
\definecolor{salmon}{RGB}{250,128,114}

\usepackage{makeidx}   % For creating index
\makeindex             % Initialize index



\usepackage{hyperref}  % clickable ToC/LoF/LoT/index
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}



\begin{document}

\title{Summary of Activation Functions, Loss Functions, and Optimizers}
\author{}
\date{}
\maketitle


% ---- Front matter (at beginning) ----
\pagenumbering{roman}
\tableofcontents
\listoffigures
\listoftables
%\printindex
\cleardoublepage
\pagenumbering{arabic}
% -------------------------------------



\section{Activation Functions}\index{Activation Functions}

\subsection{Sigmoid}\index{Activation Functions!Sigmoid}
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
\textbf{Applications:} Binary classification outputs, logistic regression, simple neural networks.  
\textbf{Advantages:}
\begin{itemize}
\item Smooth, differentiable.
\item Maps input to $(0,1)$ range for probability interpretation.
\end{itemize}
\textbf{Disadvantages:}
\begin{itemize}
\item Vanishing gradients for large $|x|$.
\item Non-zero-centered outputs slow convergence.
\end{itemize}

\subsection{Tanh}\index{Activation Functions!Tanh}
\[
\tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
\]
\textbf{Applications:} Hidden layers of RNNs, MLPs.  
\textbf{Advantages:}
\begin{itemize}
\item Zero-centered outputs.
\item Stronger gradients than Sigmoid.
\end{itemize}
\textbf{Disadvantages:}
\begin{itemize}
\item Still suffers from vanishing gradients.
\item Costlier than ReLU.
\end{itemize}

\subsection{ReLU}\index{Activation Functions!ReLU}
\[
\mathrm{ReLU}(x) = \max(0, x)
\]
\textbf{Applications:} CNNs, fully connected layers in deep networks.  
\textbf{Advantages:}
\begin{itemize}
\item Simple and efficient.
\item Sparse activations reduce computation.
\item Mitigates vanishing gradient problem.
\end{itemize}
\textbf{Disadvantages:}
\begin{itemize}
\item ``Dying ReLU'' problem for negative inputs.
\item Unbounded positive outputs.
\end{itemize}

\subsection{Leaky ReLU}\index{Activation Functions!Leaky ReLU}
\[
f(x) = \begin{cases} x, & x > 0 \\ \alpha x, & x \le 0 \end{cases}
\]
\textbf{Applications:} CNNs where ReLU causes dead neurons.  
\textbf{Advantages:} Avoids dying ReLU; small slope for negatives.  
\textbf{Disadvantages:} Requires $\alpha$ tuning.

\subsection{Softmax}\index{Activation Functions!Softmax}
\[
\mathrm{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}
\]
\textbf{Applications:} Multiclass classification output layers.  
\textbf{Advantages:} Converts logits to probabilities.  
\textbf{Disadvantages:} Sensitive to large logits; saturation slows learning.

\subsection{GELU}\index{Activation Functions!GELU}
\[
\mathrm{GELU}(x) = x \Phi(x)
\]
\textbf{Applications:} Transformers, BERT, Vision Transformers.  
\textbf{Advantages:} Smooth and probabilistic activation.  
\textbf{Disadvantages:} More computation-heavy.

% ------------------------------------------------------

\section{Loss Functions}\index{Loss Functions}

\subsection{Mean Squared Error (MSE)}\index{Loss Functions!MSE}
\[
\mathrm{MSE} = \frac{1}{N}\sum_{i=1}^{N}(y_i - \hat{y}_i)^2
\]
\textbf{Applications:} Regression problems, autoencoders.  
\textbf{Advantages:} Penalizes large errors heavily; convex.  
\textbf{Disadvantages:} Sensitive to outliers.

\subsection{Mean Absolute Error (MAE)}\index{Loss Functions!MAE}
\[
\mathrm{MAE} = \frac{1}{N}\sum_{i=1}^{N} |y_i - \hat{y}_i|
\]
\textbf{Applications:} Robust regression.  
\textbf{Advantages:} Robust to outliers.  
\textbf{Disadvantages:} Non-differentiable at 0.

\subsection{Huber Loss}\index{Loss Functions!Huber Loss}
\[
L_\delta(e) =
\begin{cases}
\frac{1}{2}e^2, & |e| \le \delta \\
\delta(|e| - \frac{1}{2}\delta), & |e| > \delta
\end{cases}
\]
\textbf{Applications:} Regression with moderate outliers.  
\textbf{Advantages:} Combines smoothness (MSE) and robustness (MAE).  
\textbf{Disadvantages:} Requires tuning $\delta$.

\subsection{Cross Entropy Loss}\index{Loss Functions!Cross Entropy}
\[
L = -\frac{1}{N}\sum_{i=1}^N \sum_{k=1}^K y_{ik}\log p_{ik}
\]
\textbf{Applications:} Classification (binary, multiclass, multi-label).  
\textbf{Advantages:} Probabilistic; aligns with maximum likelihood.  
\textbf{Disadvantages:} Sensitive to noisy labels; overconfident predictions.

\subsection{Focal Loss}\index{Loss Functions!Focal Loss}
\[
L = -\alpha (1 - p_t)^\gamma \log(p_t)
\]
\textbf{Applications:} Object detection (Mask R-CNN, RetinaNet).  
\textbf{Advantages:} Handles class imbalance.  
\textbf{Disadvantages:} Hyperparameters $\alpha$, $\gamma$ need tuning.

\subsection{Dice / IoU Loss}\index{Loss Functions!Dice Loss}\index{Loss Functions!IoU Loss}
\[
\mathrm{Dice} = \frac{2|A \cap B|}{|A| + |B|}, \qquad
L = 1 - \mathrm{Dice}
\]
\textbf{Applications:} Image segmentation (Mask R-CNN, U-Net).  
\textbf{Advantages:} Works well with imbalanced masks.  
\textbf{Disadvantages:} Non-linear, harder optimization.

\subsection{KL Divergence}\index{Loss Functions!KL Divergence}
\[
D_{KL}(P\Vert Q) = \sum_i P(i)\log\frac{P(i)}{Q(i)}
\]
\textbf{Applications:} Variational Autoencoders, distillation.  
\textbf{Advantages:} Measures information difference.  
\textbf{Disadvantages:} Asymmetric; can be unstable.

\setlength{\tabcolsep}{4pt} % tighter columns

\subsection*{Loss Functions}
\renewcommand{\arraystretch}{1.25} % row spacing
\begin{tabular}{|p{2.8cm}|p{2.8cm}|p{5cm}|p{5cm}|}
\hline
\textbf{Loss} & \textbf{Typical Use} & \textbf{Advantages} & \textbf{Disadvantages} \\ \hline

MSE ($\frac{1}{N}\sum (y-\hat y)^2$) & Regression &
Convex, smooth, strong penalty on large errors &
Sensitive to outliers \\ \hline

MAE ($\frac{1}{N}\sum |y-\hat y|$) & Robust regression &
Robust to outliers &
Non-differentiable at 0; slower convergence \\ \hline

Huber / Smooth-$L_1$ & Regression, box regression &
Combines MSE and MAE; robust and smooth &
Requires tuning of $\delta$ \\ \hline

Binary CE & Binary classification &
Probabilistic; aligns with MLE &
Sensitive to noisy labels \\ \hline

Multiclass CE & Multiclass classification &
Standard loss with softmax; probabilistic interpretation &
Overconfident predictions are heavily penalized \\ \hline

BCEWithLogits & Multi-label classification &
Numerically stable (sigmoid + CE combined) &
Ignores label dependencies \\ \hline

Focal Loss & Imbalanced classification/detection &
Focuses learning on hard examples &
Requires tuning of $\alpha$ and $\gamma$ \\ \hline

Dice Loss & Image segmentation (masks) &
Handles class imbalance; measures overlap directly &
Unstable for very small targets \\ \hline

IoU (Jaccard) Loss & Segmentation / bounding boxes &
Directly optimizes IoU metric &
Non-smooth; slower early convergence \\ \hline

Smooth-$L_1$ (boxes) & Object detection (R-CNNs) &
Robust to outliers; standard for box regression &
Scale-sensitive \\ \hline

KL Divergence & VAEs, knowledge distillation &
Measures divergence between distributions &
Asymmetric; unstable when $Q\approx0$ \\ \hline

Triplet Loss & Metric learning &
Learns discriminative embedding spaces &
Needs triplet mining; margin tuning \\ \hline

Contrastive Loss & Siamese networks / similarity tasks &
Learns pairwise distance relationships &
Requires balanced positive/negative pairs \\ \hline

Cosine Embedding & Text/vision embeddings &
Scale-invariant and simple &
Loses magnitude information \\ \hline

Perceptual Loss & Super-resolution / style transfer &
Encourages perceptual similarity using deep features &
Computationally heavy; needs pretrained $\phi$ \\ \hline

Total Variation & Image smoothing / denoising &
Removes noise, encourages smoothness &
Can over-smooth fine details \\ \hline
\end{tabular}

% ------------------------------------------------------

\section{Optimizers}\index{Optimizers}

\subsection{Stochastic Gradient Descent (SGD)}\index{Optimizers!SGD}
\[
\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta_t)
\]
\textbf{Applications:} Classical ML, CNNs.  
\textbf{Advantages:} Simple, effective.  
\textbf{Disadvantages:} Sensitive to learning rate; oscillates.

\subsection{Momentum}\index{Optimizers!Momentum}
\[
v_t = \beta v_{t-1} + (1 - \beta)\nabla_\theta J(\theta_t), \quad
\theta_{t+1} = \theta_t - \eta v_t
\]
\textbf{Applications:} Deep CNNs.  
\textbf{Advantages:} Faster convergence; less noise.  
\textbf{Disadvantages:} Needs tuning of $\beta$.

\subsection{Adagrad}\index{Optimizers!Adagrad}
\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot \nabla_\theta J(\theta_t)
\]
\textbf{Applications:} Sparse features (e.g. NLP embeddings).  
\textbf{Advantages:} Adaptive learning rate.  
\textbf{Disadvantages:} Learning rate decays too fast.

\subsection{RMSProp}\index{Optimizers!RMSProp}
\[
E[g^2]_t = \beta E[g^2]_{t-1} + (1 - \beta)g_t^2, \quad
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}}g_t
\]
\textbf{Applications:} RNNs, time series, non-stationary data.  
\textbf{Advantages:} Stable, adaptive.  
\textbf{Disadvantages:} Sensitive to $\beta$.

\subsection{Adam}\index{Optimizers!Adam}
\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t,\quad
v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t^2
\]
\[
\hat{m}_t = \frac{m_t}{1 - \beta_1^t},\quad
\hat{v}_t = \frac{v_t}{1 - \beta_2^t},\quad
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}\hat{m}_t
\]
\textbf{Applications:} Deep learning, NLP, Transformers.  
\textbf{Advantages:} Combines momentum + adaptive rate.  
\textbf{Disadvantages:} May generalize poorly.

\subsection{AdamW}\index{Optimizers!AdamW}
\[
\theta_{t+1} = \theta_t - \eta\left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t\right)
\]
\textbf{Applications:} Transformers, BERT, ViTs.  
\textbf{Advantages:} Decoupled weight decay → better generalization.  
\textbf{Disadvantages:} Slightly more computation.

\subsection{LAMB}\index{Optimizers!LAMB}
\[
r_t = \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}, \quad
\theta_{t+1} = \theta_t - \eta \frac{\|\theta_t\|}{\|r_t\|} r_t
\]
\textbf{Applications:} Large-batch Transformer training.  
\textbf{Advantages:} Enables distributed training.  
\textbf{Disadvantages:} Complex; more tuning.

% ------------------------------------------------------

\section{Summary Tables}

\subsection*{Activation Functions}
\begin{tabular}{|l|l|l|}
\hline
Function & Pros & Cons \\ \hline
Sigmoid & Probabilistic, smooth & Vanishing gradient \\
Tanh & Zero-centered & Saturation at extremes \\
ReLU & Sparse, fast & Dead neurons \\
Leaky ReLU & Fixes dead ReLU & Hyperparam $\alpha$ \\
Softmax & Probabilities & Saturation \\
GELU & Smooth & Slower compute \\ \hline
\end{tabular}

\subsection*{Optimizers}
\begin{tabular}{|l|l|l|}
\hline
Optimizer & Pros & Cons \\ \hline
SGD & Simple, reliable & Slow, oscillates \\
Momentum & Smooth convergence & Needs tuning \\
RMSProp & Stable & Sensitive $\beta$ \\
Adam & Fast, adaptive & May overfit \\
AdamW & Best generalization & Slightly slower \\ \hline
\end{tabular}

\newpage







\section{Forward Pass}\index{Forward Pass}

\noindent
The forward pass computes activations layer by layer using learned weights and biases. Each layer applies linear transformations followed by nonlinear activation functions (e.g., sigmoid, ReLU, etc.).

\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{4pt}

\begin{table}[h!]
\centering
\caption{Forward Pass Computations and Data Structures}
\begin{tabular}{|p{3.5cm}|p{5cm}|p{4cm}|p{3cm}|}
\hline
\textbf{Step} & \textbf{Calculation} & \textbf{Data Structure} & \textbf{Multiplication Type} \\ \hline

\multicolumn{4}{|l|}{\textbf{Forward Pass}} \\ \hline

Input to 1st hidden layer (HI\_1) &
$HI_1 = W_1 \cdot X + B_1$ &
$HI_1$: Matrix, $W_1$: Matrix, $X$: Matrix, $B_1$: Vector &
\textbf{Dot Product} (between $W_1$ and $X$) \\ \hline

Output of 1st hidden layer (HO\_1) &
$HO_1 = \sigma(HI_1)$ &
$HO_1$: Vector &
\textbf{Element-wise} (Sigmoid applied element-wise) \\ \hline

Input to 2nd hidden layer (HI\_2) &
$HI_2 = W_2 \cdot HO_1 + B_2$ &
$HI_2$: Vector, $W_2$: Matrix, $HO_1$: Vector, $B_2$: Vector &
\textbf{Dot Product} (between $W_2$ and $HO_1$) \\ \hline

Output of 2nd hidden layer (HO\_2) &
$HO_2 = \sigma(HI_2)$ &
$HO_2$: Vector &
\textbf{Element-wise} (Sigmoid applied element-wise) \\ \hline

Input to output layer (HO\_final) &
$HO_{final} = W_3 \cdot HO_2 + B_3$ &
$HO_{final}$: Vector, $W_3$: Matrix, $HO_2$: Vector, $B_3$: Vector &
\textbf{Dot Product} (between $W_3$ and $HO_2$) \\ \hline

Final output $\hat{Y}$ &
$\hat{Y} = \sigma(HO_{final})$ &
$\hat{Y}$: Vector &
\textbf{Element-wise} (Sigmoid applied element-wise) \\ \hline

Error Calculation &
$E = Y - \hat{Y}$ &
$E$: Vector, $Y$: Vector, $\hat{Y}$: Vector &
\textbf{Element-wise} (Subtraction) \\ \hline
\end{tabular}
\end{table}






\section{Backpropagation}\index{Backpropagation}
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{4pt}

\begin{table}[h!]
\centering
\caption{Comprehensive Backpropagation Steps}
\begin{tabular}{|p{3.8cm}|p{5.2cm}|p{4.2cm}|p{3cm}|}
\hline
\textbf{Step} & \textbf{Calculation} & \textbf{Data Structure} & \textbf{Multiplication Type} \\ \hline

\multicolumn{4}{|l|}{\textbf{Backpropagation}} \\ \hline

Error at output layer &
$err_{HO_{final}} = E \cdot \sigma'(HO_{final})$ &
$err_{HO_{final}}$: Vector, $E$: Vector, $\sigma'(HO_{final})$: Vector &
\textbf{Element-wise} (Multiplication) \\ \hline

Error in 2nd hidden layer (err\_HO\_2) &
$err_{HO_2} = err_{HO_{final}} \cdot W_3^{T} \cdot \sigma'(HI_2)$ &
$err_{HO_2}$: Vector, $W_3^{T}$: Matrix (transpose), $\sigma'(HI_2)$: Vector &
\textbf{Dot Product} (between $err_{HO_{final}}$ and $W_3^{T}$) followed by \textbf{Element-wise} multiplication with $\sigma'(HI_2)$ \\ \hline

Error in 1st hidden layer (err\_HO\_1) &
$err_{HO_1} = err_{HO_2} \cdot W_2^{T} \cdot \sigma'(HI_1)$ &
$err_{HO_1}$: Vector, $W_2^{T}$: Matrix (transpose), $\sigma'(HI_1)$: Vector &
\textbf{Dot Product} (between $err_{HO_2}$ and $W_2^{T}$) followed by \textbf{Element-wise} multiplication with $\sigma'(HI_1)$ \\ \hline

Error Calculation &
$E = Y - \hat{Y}$ &
$E$: Vector, $Y$: Vector, $\hat{Y}$: Vector &
\textbf{Element-wise} (Subtraction) \\ \hline
\end{tabular}
\end{table}


\begin{center}
\fbox{%
\parbox{0.92\linewidth}{%
\[
\textcolor{blue}{Error\_Layer\_N} = 
\textcolor{olive}{\sigma'_d(Input\_N)} 
\Bigl(
\textcolor{salmon}{Error\_Layer\_{N+1}} 
(\textcolor{orange}{W\_{N+1}})^{T}
\Bigr),
\qquad
\textcolor{green}{Input\_N = W X + B.}
\]
}}
\end{center}





\section{Weight and Bias Updates}\index{Weight and Bias Updates}
\renewcommand{\arraystretch}{1.25}
\setlength{\tabcolsep}{4pt}

\begin{table}[h!]
\centering
\caption{Weight and Bias Update Steps}
\begin{tabular}{|p{3.8cm}|p{5.2cm}|p{4.2cm}|p{3cm}|}
\hline
\textbf{Step} & \textbf{Calculation} & \textbf{Data Structure} & \textbf{Multiplication Type} \\ \hline

\multicolumn{4}{|l|}{\textbf{Weight and Bias Updates}} \\ \hline

Update $W_1$ and $B_1$ &
$W_1 = W_1 + lr \cdot X^{T} \cdot err_{HO_1}, \quad B_1 = B_1 + lr \cdot err_{HO_1}$ &
$W_1$: Matrix, $X^{T}$: Matrix (transpose), $err_{HO_1}$: Vector &
\textbf{Dot Product} (between $X^{T}$ and $err_{HO_1}$) \\ \hline

Update $W_2$ and $B_2$ &
$W_2 = W_2 + lr \cdot HO_1^{T} \cdot err_{HO_2}, \quad B_2 = B_2 + lr \cdot err_{HO_2}$ &
$W_2$: Matrix, $HO_1^{T}$: Matrix (transpose), $err_{HO_2}$: Vector &
\textbf{Dot Product} (between $HO_1^{T}$ and $err_{HO_2}$) \\ \hline

Update $W_3$ and $B_3$ &
$W_3 = W_3 + lr \cdot HO_2^{T} \cdot err_{HO_{final}}, \quad B_3 = B_3 + lr \cdot err_{HO_{final}}$ &
$W_3$: Matrix, $HO_2^{T}$: Matrix (transpose), $err_{HO_{final}}$: Vector &
\textbf{Dot Product} (between $HO_2^{T}$ and $err_{HO_{final}}$) \\ \hline
\end{tabular}
\end{table}

\begin{center}
\fbox{%
\parbox{0.92\linewidth}{%
\[
\textcolor{blue}{W_{new}} = 
\textcolor{green!60!black}{W_{old}} + 
\textcolor{olive}{lr} \cdot 
\textcolor{purple}{Own\_Error} \cdot 
\textcolor{orange}{Own\_Input^{T}}
\]

\[
\textcolor{orange}{Own\_Input} = \textcolor{purple}{X}, \quad
\textcolor{orange}{Own\_Input\_N} = \textcolor{purple}{HO_{N-1}} = 
\textcolor{purple}{\sigma(HI_{N-1})}
\]
}}
\end{center}






\section{General Structure of Forward Pass and Backpropagation}
The general structure of the forward pass and backpropagation is the same for all basic neural networks, regardless of how many hidden layers or neurons are used. The following principles hold true for most feedforward neural networks (also known as \textbf{multilayer perceptrons (MLPs)}).

\subsection{Forward Pass}
\subsubsection*{Basic Structure}
\begin{itemize}
    \item The network consists of an input layer, one or more hidden layers, and an output layer.
    \item Each layer computes a weighted sum of its inputs (or the outputs of the previous layer), adds a bias, and applies an activation function (such as sigmoid, ReLU, or tanh) to produce its output.
\end{itemize}

\subsubsection*{Computation Flow}
\begin{itemize}
    \item Data flows forward through the network from the input layer to the output layer.
    \item The output of one layer becomes the input to the next layer.
\end{itemize}

\subsection{Backpropagation}
\subsubsection*{Error Propagation}
\begin{itemize}
    \item After computing the final output during the forward pass, the network compares the predicted output with the expected (target) output.
    \item The error is calculated using a loss function (for example, mean squared error or cross-entropy).
    \item This error is propagated backward from the output layer to the hidden layers, adjusting the neuron weights to minimize the error.
\end{itemize}

\subsubsection*{Weight Update}
\begin{itemize}
    \item The gradients (rate of change of error with respect to weights) are computed using the chain rule of calculus.
    \item The weights and biases are updated using gradient descent or its variants (e.g., stochastic gradient descent).
\end{itemize}

\subsubsection*{General Steps}
\begin{enumerate}
    \item Compute the error at the output layer.
    \item Backpropagate the error through each preceding layer.
    \item Update the weights and biases to reduce the overall error.
\end{enumerate}

\subsection{Universality of the Process}
\noindent The process of forward pass and backpropagation described above holds for all basic neural networks, regardless of:
\begin{itemize}
    \item The number of hidden layers.
    \item The number of neurons per layer.
    \item The activation function used.
\end{itemize}

\subsection{Variations}
While the core steps remain the same, there are variations among different network types and techniques:
\begin{itemize}
    \item \textbf{Activation Functions:} Modern networks often use ReLU (Rectified Linear Unit) or tanh instead of sigmoid in hidden layers.
    \item \textbf{Loss Functions:} Classification problems typically use cross-entropy loss, while regression problems often use mean squared error (MSE).
    \item \textbf{Learning Algorithms:} Standard gradient descent can be replaced by advanced optimizers like Adam, RMSProp, or Adagrad, which adaptively adjust the learning rate.
\end{itemize}

\subsection{Summary}
\begin{itemize}
    \item The concept of forward pass and backpropagation is \textbf{universal} to most feedforward neural networks.
    \item Differences arise mainly in the architecture (number of layers or neurons) and the type of activation, loss function, and optimization algorithm used.
    \item As networks become deeper and more complex, the same principles apply, but across more layers.
\end{itemize}

\subsubsection*{Examples}
\begin{itemize}
    \item \textbf{Deep Neural Networks (DNNs):} Contain more hidden layers, but the core forward and backward propagation remain identical.
    \item \textbf{Convolutional Neural Networks (CNNs)} and \textbf{Recurrent Neural Networks (RNNs):} Both employ forward and backward propagation with specialized modifications.
\end{itemize}

\noindent In essence, for any basic feedforward neural network, the process of forward pass and backpropagation remains fundamentally the same.




\printindex

\end{document}
